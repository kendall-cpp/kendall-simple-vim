
- [- FFmpeg 数据结构 AVBuffer](#--ffmpeg-数据结构-avbuffer)
- [H264 编码](#h264-编码)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
- [遇到难题](#遇到难题)
- [为什么要有YUV这种数据出来（YUV相比RGB来说的优点）](#为什么要有yuv这种数据出来yuv相比rgb来说的优点)
- [H264/H265有什么区别](#h264h265有什么区别)
- [H264 解码过程](#h264-解码过程)
  - [帧划分类型](#帧划分类型)
- [播放器的基本原理](#播放器的基本原理)
- [使用边缓存边播放对视频播放进行优化](#使用边缓存边播放对视频播放进行优化)
  - [实现原理](#实现原理)
- [优化卡顿带来的积累迟延](#优化卡顿带来的积累迟延)
- [视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](#视频或者音频传输你会选择tcp协议还是udp协议为什么)
- [音视频直播清晰度、画面质量，降低迟延(TCP改成UDP)](#音视频直播清晰度画面质量降低迟延tcp改成udp)
- [UDP 实现可靠传输](#udp-实现可靠传输)
  - [窗口与拥塞控制](#窗口与拥塞控制)
  - [RTT 与 RTO 的计算](#rtt-与-rto-的计算)
- [RTMP 协议与 HLS 协议](#rtmp-协议与-hls-协议)
- [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)
- [FFmpeg 数据结构 AVBuffer](#ffmpeg-数据结构-avbuffer)
---- 


## H264 编码

原始视频如果不经过压缩的话是会占用很大内存的，由于网络带宽和硬盘存储空间都非常有限，需要先试用视频编码技术对原始视频进行压缩，然后再进行存储和分发。H264 的压缩比率大概是 100 ：1

- 编码器采用的是 X264，在 ffmpeg 中的名称是 libx264

- 解码器采用的是 ffmpeg 默认内置的 H264 解码器。名称就是 h264


### 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是  `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。

## 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率 1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。

一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。

后来怀疑是 ABR 模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。 

后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！



## 为什么要有YUV这种数据出来（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全 0时为黑色，全255时为白色。RGB 是一种依赖于设备的颜色空间：

不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。

Y'UV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV 和 RGB 是可以相互转换的，基本上所有图像算法都是基于 YUV 的，所有显示面板都是接收 RGB 数据。

## H264/H265有什么区别

同样的画质和同样的码率，H.265 比 H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下 H.265 会比 H.264 画质要高一些理论值是30%~40%。

比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，最小的 8x8 到最大的 64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265 标准主要是围绕着现有的视频编码标准 H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265 研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。


## H264 解码过程

大体可以分为几个主要的步骤

- 划分帧类型
- 帧内/帧间编码
- 变换 + 向量
- 滤波
- 熵编码

### 帧划分类型

在连续的几帧的图像中，一般只有 10% 以内的像素有差别，亮度的差值变化不超过 2%，色度的差值变化只在 1% 以内

于是可以将一串连续的相似的帧轨道一个图像群组，也就是 GOP（Group of Pictures）

GOP 中的帧可以分为 3 中类型

- I 帧，也就是 帧内编码图像，也叫关键帧，
  - 是视频的第一帧，也是 GOP 的第一帧，一个 GOP 只有一个帧
  - 编码；对图像数据进行编码
  - 解码：只是用当前帧的编码数据就可以解码出完整的图像
  - I 帧是一种自带全部信息的独立帧，不需要参考其他图像就可以独立进行解码，可以简单理解为一张静态图像

- P 帧，也就是 预测编码图像
  - 编码：并不会对整个帧的编码数据就可以解码出完整的图像，需要以前面的帧或者 P 帧作为参考，只能编码当前与参考帧的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据届澳门出当前 P 帧完整的图像

- B 帧：也就是 前后预测编码图像
  - 编码：并不会对整个帧图形进行编码
    - 同时以前面、后面的帧或 P 帧作为参考帧，只编码当前 B 帧与前后参考的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据解码出当前 B 帧完整的图像

所以编码后的数据大小： I 帧 > P 帧 > B 帧


## 播放器的基本原理

> https://mp.weixin.qq.com/s/vxrpMmf-UsVdGFvwWRLYKw

- **解协议**

在播放视频前，我们一般会拿到一个视频的播放地址，如果是本地视频，就是一个文件路径；如果是一个在线视频，那么可能有多种流媒体协议，常见的如HTTP、RTMP、HLS、DASH等。解协议的过程就是通过拿到的播放地址判断出当前视频的流媒体协议，然后用对应的协议去获取媒体文件数据。

FFmpeg中内置了常见的流媒体格式协议的解析，对于一个视频 `url http:www.qq.com/test.mp4`, 常见的解析的过程如下：

- 首先取出 url 中的协议头如 "http"

- 和初始化好的协议列表中的协议名进行对比，如果匹配上则使用该协议解析器；这里会匹配上 http 协议，*http 协议的 name 就是:http，实现在 http.c*

- 如果是一个本地文件的地址，会解析为 file 协议，*file 协议的 name 就是:file，实现在 file.c*

解析完成后就会使用对应协议实现的获取媒体数据的方式来读取媒体流。就比如说 File 协议的实现就是读取本地文件；Http 协议的实现就是通过 http 请求的方式向服务器请求数据

- **解封装**

视频有多种格式，如常见的有MP4、3GP、AVI、FLV、RMVB。一般来说视频的格式名就对应着他的封装协议名称。**封装协议的主要作用就是将已经编码好的视频数据和音频数据按照协议规则放在一个文件中**。

一个完整的视频文件中，除了有已经编码后的音视频信息外，一般还会有描述媒体数据的组织结构的信息。如MP4封装格式如下所示，就是一个一个的box及其嵌套，不同的box里面存储了不同的信息，MP4的所有信息都以box的方式进行组织，box可以相互嵌套。其中最重要的就是moov box和mdat box，在moov box中存储了描述音视频格式如视频宽高、分辨率、码率等相关的格式信息，也有如moov box其中嵌套的stbl包含了所有音视频sample的时间戳pts和在文件中的偏移位置offset的信息；而mdat box则完全是存储的压缩后的音视频数据。

解封装的过程就是通过 moov box中的媒体结构信息，从mdta box中分离出Audio Track和Video Track，再把一份一份的视频数据或音频数据取出来的过程。

- **解码**

目前视频常用的压缩格式为H264和H265，音频常用的压缩格式为AAC。解码的过程就是将这些按照压缩算法解码为可直接送给播放器播放的原始数据类型。通常视频是解码YUV或RGB格式，音频是解码为PCM格式。

使用FFmpeg自带的软解解码器大致的解码流程如下：

  - 通过find_probe_decoder找到合适的解码器，详细寻找过程和寻找demuxer类似：

  - 解码器也会在初始化的时候初始化好放在codec_list中；寻找的过程就是找到解码器的AVCodecID相等的即可；AVCodecID存放在track box中，是在解析视频header的时候初始化的，如果该视频是HEVC编码格式的，找到的就是hevc的decoder；

  - 找到匹配的decoder为hevc，hevc格式的name就是:hevc,，实现在hevcdec.c；

  - 找到对应的decoder后，先通过hevc_decode_init初始化；

  - 解码的时候会单独在一个解码线程，通过读取解封装数据缓存区的数据来进行解码，然后将解码后的数据放入缓存池中。

- **音视频同步**

在视频数据解码完成后，不会立即渲染到View上，还需要通过音视频同步机制，等到合适的渲染时机。

音视频同步主要分为三种：

- 音频时钟为基准：以当前正在播放的音频时钟基准，比较视频和音频的pts差值，如果视频过慢，则通过丢帧的方式进行追赶；如果视频播放过快，则一直渲染当前帧，直到音频跟上；

- 视频时钟为基准：以当前正在播放的视频时钟为基准，比较视频和音频的pts差值，这里和音频时钟为基准不同的是，这里音频是通过重采样的方式适当缩减或添加audio sample来达到同步的目的。

- 以外部时钟为基准：音频和视频在输出时，都需要和外部时钟进行对比，然后音视频按照各自同步的方法进行同步(视频丢帧或等待、音频重采样)，外部时钟的更新依赖于最近同步过的音频时钟或视频时钟。

![](https://mmbiz.qpic.cn/mmbiz_png/csvJ6rH9Mcsic1Nv6MialKaMsbg9KNYCKykpiaqqf3zd7DVbbBBvZibhcaEBMGhYnavCKjHic97YRZRSasrdJxMZCmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 使用边缓存边播放对视频播放进行优化

一般情况下，播放音视频时，播放器数据的请求是由播放器内部发起的，我们只是提供了一个url，而不能控制数据的请求过程，都是要先进行下载，下载到一定量之后播放器再开始播放，当下载进度减去播放进度小于一定阀值，进入缓冲状态。一般最小缓存大小是4M，最大20M。

但是这样有两个弊端

- 卡顿恢复时长比较高，影响用户体验。
- 每次都要重新跟进url重新下载视频，造成了严重的流量浪费。

> 遇到这个问题之后，我去知乎上请教了一些人，他们建议我去参考一下 AndroidVideoCache 这个开源项目。然后实现机制就参考率这个开源项目

[参考](https://mp.weixin.qq.com/s/LWTAAXgjojoQF6vFio7MCg)

### 实现原理

加入本地代理，通过Socket的的方式，首先建立本地的socketServer，监听local host和指定端口的请求，也就是 bind的时候指定让系统来分配一个可用的端口。每次数据的请求都发给local host，socketSrever 监听到有 Socket 连接时，由 socketServer 来代理视频数据的请求，请求到的数据不返回给播放器，而是直接写入到文件缓存中,再从这个文件缓存中读取 buffer 数据给到播放器。

- 首先生成本地代理服务器  「在 AndroidVideoCache 中是通过构造器来实现」
  - 根据host生成本地代理服务器的地址
  - 然后创建ServerSocket，最大可于8个client进行连接
  - 接着由系统自动分配一个端口
  - 然后开启一个线程，在线程中轮训，检测是否有新的socket连接，并接收socket连接

- 处理这个socket连接
  
- 然后就到 边缓存边播放 的实现

  - 把数据先以流的方式 写入到缓存，在通过socket的outStream给到播放器
  - 也就是每次从网络流中读取8192个字节，先写入到缓存文件，再从缓存文件中取出给到播放器
  - 在先往文件中写入数据时，直到写完（整个文件写完或者8192个写完）或者中断。

> 在请求远程url时将文件写到本地缓存中，然后从这个本地缓存中读数据，写入到客户端socket里面。服务器Socket主要还是一个代理的作用，从中间拦截掉网络请求，然后实现对socket的读取和写入。

> 不过这种方法在 Seek的场景 是有缺陷的。    
> https://mp.weixin.qq.com/s/kSBu_wY9pHJhg2euCuZWvg


## 优化卡顿带来的积累迟延

在网络状态良好的情况下，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

> 在拉流时，音频流、视频流是单独保存到缓冲队列的。如果发生网络抖动，就会引起缓冲抖动，可以总结为网络卡顿导致音视频缓冲队列增大，从而导致解码滞后、播放滞后。此时，我们需要主动丢包来跟进当前时间戳。因为音视频同步一般以音频时钟为基准，人们对音频更加敏感，所以我们优先丢掉视频队列的包。但是，丢视频数据包时，需要丢掉整个GOP的数据包，因为B帧、P帧依赖I帧来解码，否则会引起花屏


[参考](https://blog.csdn.net/u011686167/article/details/85256101?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)

## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。

## 音视频直播清晰度、画面质量，降低迟延(TCP改成UDP)

> 但是为了保证参数的可靠性，又实现了 UDP 的可靠传输

> https://zhuanlan.zhihu.com/p/82854047

有一个项目采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如果想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频推送到云服务端的流媒体，再进行大规模的分发和转发。但是我们摄像头以前只支持 TCP 长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将 TCP 推流改成 UDP 推流，

> https://www.sohu.com/a/305914050_134613

首先基本上直播的推流和拉流一般都是在基于 RTMP 协议的，在网络状态良好的情况下 RTMP 协议无论是推流还是拉流都是可以很好工作的，至少在网络不丢包的情况下是很流畅的，但是因为 RTMP 是基于 TCP 协议的，因为 TCP 协议需要通过 3次 握手建立链接，还需要 TLS 2 次握手，还用拥塞不可控等情况，这些情况都会影响音视频播放的流畅度，在弱网环境下延迟会增大。

因此我们选择了使用 UDP，因为 UDP 不需要建立连接，而且速度快，占用资源少。

> 摄像头，并发少，RTMP，TCP --> UDP QUIC协议

我当时采用的是 QUIC 协议 ，QUIC 就是快速 UDP 互联网连接，QUIC 位于应用层(比如 Http)和网络层(UDP)之间，是在 **UDP 上实现的一个多路复用的协议**。

QUIC 替换掉了 TCP 在应用层和传输层中间的角色

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/QUIC.6egow7866pw0.png)


## UDP 实现可靠传输

> [参考](https://www.toutiao.com/a6589194755962307080/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1534206661&app=news_article&utm_source=mobile_qq&iid=40708017633&utm_medium=toutiao_ios&group_id=6589194755962307080&wid=1638594135814)

> 因为 RTMP 底层是基于 TCP 的

> 在弱网环境下，需要做低延迟可靠通信，如果用 TCP 通信延迟可能会非常大，还有一种情况，有时候客户端数据上传需要突破本身 TCP 公平性的限制来达到高速低延时和稳定，也就是说要用特殊的流控算法来压住客户端上传带宽，这种情况就需要实现一种机制，实现不仅能压榨带宽，也能更好地增加通信的稳定性，避免类似 TCP 的频繁断开重连。     
> 当然还有避免 TCP 的三次握手和四次挥手的过程。

然后实现 UDP 的可靠最主要的就是：**重传**

我们都知道 IP 协议在设计的时候就不是为了数据可靠到达而设计的，所以 UDP 要保证可靠，就依赖于重传。

我们实现的重传是发送端通过接收端 ACK 的丢包信息反馈来进行数据重传，发送端会根据场景来设计自己的重传方式，设计的重传方式分为三类：定时重传、请求重传和 FEC 选择重传。

- 定时重传（固定时间）

就是发送端如果在发出数据包（T1）时刻经过一个 RTO（重传时间) 之后还未收到这个数据包的 ACK 消息，那么发送端就重传这个数据包。这种方式依赖于接收端的 ACK 和 RTO，但是这种重传机制容易产生误判，比如：

- 对方收到了数据包，但是 ACK 发送途中丢失。

- ACK 在途中，但是发送端的时间已经超过了一个 RTO。

所以超时重传的方式主要集中在 RTO 的计算上，如果应对的（比如书写同步）场景是一个对延迟敏感但对流量成本要求不高的场景，就可以将 RTO 的计算设计得比较小，这样能尽较大可能保证你的延时足够小。比如 125ms

- 请求重传

请求重传就是接收端在发送 ACK 的时候携带自己丢失报文的信息反馈，发送端接收到 ACK 信息时根据丢包反馈进行报文重传

这个反馈过程最关键的步骤就是回送 ACK 的时候应该携带哪些丢失报文的信息，因为 UDP 在网络传输过程中会乱序会抖动，接收端在通信的过程中要评估网络的 `jitter_time`（抖动时间），也就是 `rtt_var`（RTT 方差值），当发现丢包的时候记录一个时刻 t1，当 `t1 + rtt_var < curr_t`(当前时刻)，我们就认为它丢失了。

这个时候后续的 ACK 就需要携带这个丢包信息并更新丢包时刻 t2，后续持续扫描丢包队列，如果 `t2 + RTO < curr_t`，则再次在 ACK 携带这个丢包信息，以此类推，直到收到报文为止。

这种方式是由丢包请求引起的重发，如果网络很不好，接收端会不断发起重传请求，造成发送端不停的重传，引起网络风暴，通信质量会下降，所以我们还需要在发送端设计一个**拥塞控制模块**来限流，

> 整个请求重传机制依赖于 jitter time 和 RTO 这个两个时间参数，评估和调整这两个参数和对应的传输场景也息息相关。请求重传这种方式比定时重传方式的延迟会大，一般适合于带宽较大的传输场景，例如：视频、文件传输、数据同步等。

- FEC 选择重传

除了定时重传和请求重传模式以外，还有一种方式就是以 FEC 分组方式选择重传，这也是在设计UDP可靠中最重要的重传机制。FEC（Forward Error Correction）是一种前向纠错技术，一般通过 XOR 类似的算法来实现，也有多层的 EC 算法和 raptor 涌泉码技术，其实是一个解方程的过程。

在发送方发送报文的时候，会根据 FEC 方式把几个报文进行 FEC 分组，通过 XOR 的方式得到若干个冗余包，然后一起发往接收端，如果接收端发现丢包但能通过 FEC 分组算法还原，就不向发送端请求重传，如果分组内包是不能进行 FEC 恢复的，就向发送端请求原始的数据包

### 窗口与拥塞控制

> 因为我们主要指通过重传来解决可靠问题的，但是重传会引来两个问题，一个是延时，一个是重传的带宽，尤其是带宽，如果控制不好会引来网络风暴，所以在发送端会设计一个窗口拥塞机制了避免并发带宽占用过高的问题。

这里需要一个收发的滑动窗口系统来配合对应的拥塞算法做流量控制，如果涉及到可靠有序的 UDP 重传，接收端就要做窗口排序和缓冲，如果是无序可靠或者尽力可靠的场景，接收端一般就不做窗口缓冲，只做位置滑动。

> 画个图说说

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/滑动窗口.2uin6jnicyo0.png)

比如发送端从发送窗口中发了 6 个数据报文给接收端，接收端收到 101，102，103，106 时会先判断报文的连续性并滑动窗口开始位置到 103，接着每个包都回应 ACK，发送端在接收到 ACK 的时候，会确认报文的连续性（通过seq序号），并滑动窗口到 103，发送端会再判断窗口的空余，然后填补新的发送数据，这就是整个窗口滑动的流程。

在接收端收到 106 时的处理，如果是有序可靠，那么 106 不会通知上层业务进行处理，而是等待 104、105。如果是尽力可靠和无序可靠场景，会将 106 通知给上层业务先进行处理。在收到 ACK 后，发送端的窗口要滑动多少是由自己的拥塞机制定的，也就是说窗口的滑动速度受拥塞机制控制，拥塞控制实现要么基于丢包率来实现，要么基于双方的通信时延来实现，

**使用经典的拥塞控制算法**

- 慢启动（slow start）

当连接链路刚刚建立后，不可能一开始将 cwnd 设置得很大，这样容易造成大量重传，经典拥塞里面会在开始将 cwnd = 1，然后根据通信过程的丢包情况来逐步扩大 cwnd 来适应当前的网络状态，直到达到慢启动的门限阈值 (ssthresh)，步骤如下：

1、初始化设置 cwnd = 1，并开始传输数据。

2、收到回馈的 ACK，会将 cwnd 加 1。

3、当发送端一个 RTT 后且未发现有丢包重传，就会将 cwnd = cwnd * 2。

4、当 cwnd >= ssthresh 或发生丢包重传时慢启动结束，进入拥塞避免状态。

- 拥塞避免

当通信连接结束慢启动后，有可能还未到网络传输速度的上线，这个时候需要进一步通过一个缓慢的调节过程来进行适配。一般是一个 RTT 后如果未发现丢包，就将 cwnd = cwnd + 1。一但发现丢包和超时重传，就进入拥塞处理状态。

- 拥塞处理

拥塞处理在 TCP 里面实现很暴力，如果发生丢包重传，直接将 cwnd = cwnd / 2，然后进入快速恢复状态。

- 快速恢复

通过确认丢包只发生在窗口一个位置的包来确定是否进行快速恢复，如图 6 中描述，如果只是 104 发生了丢失，而 105 和 106 是收到了的，那么 ACK 总是会将 ACK 的 base = 103，如果连续 3 次收到 base 为 103 的 ACK，就进行快速恢复，也就是立即重传 104，而后如果收到新的 ACK 且 base > 103，将 cwnd = cwnd + 1，并进入拥塞避免状态。

### RTT 与 RTO 的计算

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/RTO计算.55cj031ga5s0.png)

RTT = T2 - T1

这个计算方式只是计算了某一个报文时刻的 RTT，但网络是会波动的，这难免会有噪声现象，所以在计算的过程中引入了加权平均收敛的方法(参考 RFC793)

`SRTT = (α * SRTT) + (1-α)RTT`

这样可以求得逼近的 SRTT，在公式中一般`α=0.8`，确定了 SRTT，下一步就是计算 RTT_VAR(方差)，我们设 `RTT_VAR = |SRTT – RTT|`，那么 `SRTT_VAR =(α * SRTT_VAR) + (1-α) RTT_VAR`，这样可以得到 RTT_VAR 的值。

但最终我们是需要 RTO，因为涉及到报文重传，RTO 就是一个报文的重传周期，从网络的通信流程我们很容易知道，重传一个包以后，如果一个 `RTT+RTT_VAR` 之后的时间还没收到确定，那我们就可以再次重传，则可知：`RTO = SRTT + SRTT_VAR`。

但一般网络在严重抖动的情况下还是会有较大的重复率问题，所以：`RTO = β*(SRTT + RTT_VAR)`，`1.2 <β<2.0`，当然可以根据不同的传输场景来选择 `β` 的值。

-----

## RTMP 协议与 HLS 协议

- RTMP 协议是实时消息协议。但它实际上并不能做到真正的实时，一般情况最少都会有几秒到几十秒的延迟，底层是基于 TCP 协议的

- HLS，全称 HTTP Live Streaming，是苹果公司实现的基于 HTTP 的流媒体传输协议。它可以支持流媒体的直播和点播，主要应用在 iOS 系统和 HTML5 网页播放器中。   
HLS 协议的本质就是通过 HTTP 下载文件，然后将下载的切片缓存起来。由于切片文件都非常小，所以可以实现边下载边播的效果。HLS 规范规定，播放器至少下载一个 ts 切片才能播放，所以 HLS 理论上至少会有一个切片的延迟。

HLS 最主要的问题就是实时性差。由于 HLS 往往采用 10s 的切片，所以最小也要有 10s 的延迟，一般是 20～30s 的延迟，有时甚至更差。HLS 之所以能达到 20～30s 的延迟，主要是由于 HLS 的实现机制造成的。HLS 使用的是 HTTP 短连接，且 HTTP 是基于 TCP 的，所以这就意味着 HLS 需要不断地与服务器建立连接。TCP 每次建立连接时都要进行三次握手，而断开连接时，也要进行四次挥手，基于以上这些复杂的原因，就造成了 HLS 延迟比较久的局面。

在 PC 上，我们使用 RTMP 协议，因为 PC 基本都安装了 Flash 播放器，直播效果要好很多。

点播系统使用 HLS 协议。因为点播没有实时互动需求，延迟大一些是可以接受的，并且可以在浏览器上直接观看。

> 在拉取 HLS 媒体流时，客户端首先通过 HLS 协议将 m3u8 索引文件下载下来，然后按索引文件中的顺序，将 .ts 文件一片一片下载下来，然后一边播放一边缓冲。此时，你就可以在 PC、手机、平板等设备上观看直播节目了。

**对于使用 HLS 协议的直播系统来说，最重要的一步就是切片**。源节点服务器收到音视频流后，先要数据缓冲起来，保证到达帧的所有分片都已收到之后，才会将它们切片成 TS 流。

> PC 上使用 RTMP 协议，点播使用 HLS 协议，点播没有实时互动需求    
> RTMP 基于 TCP，实时性较好，但还是存在   
> HLS 通过 HTTP 下载文件 --> 切片缓存，切片小，可边下载边播放，切片 10s,迟延 10 s，而且也是基于 TCP     
> 切割使用 ffmpeg



## AVFormatContext 和 AVInputFormat之间的关系

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/ffmpeg数据结构04.fnx3k76bak0.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.

## FFmpeg 数据结构 AVBuffer

[参考](https://www.cnblogs.com/leisure_chn/p/10399048.html)

AVBuffer 是 FFmpeg中很常用的一种缓冲区，缓冲区使用引用计数(reference-counted)机制。

AVBufferRef 则对 AVBuffer 缓冲区提供了一层封装，最主要的是作引用计数处理，实现了一种安全机制。

用户不应直接访问 AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

**数据结构定义**

- struct AVBuffer

struct AVBuffer 定义于 “`libavutil/buffer_internal.h`”，`buffer_internal.h` 位于 FFmpeg 工程源码中，而 FFmpeg 提供的开发库头文件中并无此文件，因此这是一个内部数据结构，不向用户开放，用户不应直接访问AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

内部通过 `av_buffer_ref()` 来增加引用计数，通过 `av_buffer_unref()` 来减少引用计数

销毁一个 AVBufferRef 时，将其AVBuffer缓冲区引用计数减1，若缓冲区引用计数变为0，则将缓冲区也回收，
