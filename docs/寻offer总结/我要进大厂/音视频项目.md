
- [- FFmpeg 数据结构 AVBuffer](#--ffmpeg-数据结构-avbuffer)
- [H264 编码](#h264-编码)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
- [遇到难题](#遇到难题)
- [为什么要有YUV这种数据出来（YUV相比RGB来说的优点）](#为什么要有yuv这种数据出来yuv相比rgb来说的优点)
- [H264/H265有什么区别](#h264h265有什么区别)
- [H264 解码过程](#h264-解码过程)
  - [帧划分类型](#帧划分类型)
- [视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](#视频或者音频传输你会选择tcp协议还是udp协议为什么)
- [优化卡顿带来的积累迟延](#优化卡顿带来的积累迟延)
- [音视频直播清晰度、画面质量、流畅度如何保障，降低迟延](#音视频直播清晰度画面质量流畅度如何保障降低迟延)
- [UDP 实现可靠](#udp-实现可靠)
  - [你设计的可靠的 UDP 和 TCP 有什么不一样](#你设计的可靠的-udp-和-tcp-有什么不一样)
  - [设计的 UDP 的建立和连接](#设计的-udp-的建立和连接)
- [RTMP 协议与 HLS 协议](#rtmp-协议与-hls-协议)
- [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)
- [FFmpeg 数据结构 AVBuffer](#ffmpeg-数据结构-avbuffer)
---- 


## H264 编码

原始视频如果不经过压缩的话是会占用很大内存的，由于网络带宽和硬盘存储空间都非常有限，需要先试用视频编码技术对原始视频进行压缩，然后再进行存储和分发。H264 的压缩比率大概是 100 ：1

- 编码器采用的是 X264，在 ffmpeg 中的名称是 libx264

- 解码器采用的是 ffmpeg 默认内置的 H264 解码器。名称就是 h264


### 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是  `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。

## 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率 1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。

一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。

后来怀疑是 ABR 模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。 

后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！



## 为什么要有YUV这种数据出来（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全 0时为黑色，全255时为白色。RGB 是一种依赖于设备的颜色空间：

不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。

Y'UV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV 和 RGB 是可以相互转换的，基本上所有图像算法都是基于 YUV 的，所有显示面板都是接收 RGB 数据。

## H264/H265有什么区别

同样的画质和同样的码率，H.265 比 H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下 H.265 会比 H.264 画质要高一些理论值是30%~40%。

比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，最小的 8x8 到最大的 64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265 标准主要是围绕着现有的视频编码标准 H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265 研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。


## H264 解码过程

大体可以分为几个主要的步骤

- 划分帧类型
- 帧内/帧间编码
- 变换 + 向量
- 滤波
- 熵编码

### 帧划分类型

在连续的几帧的图像中，一般只有 10% 以内的像素有差别，亮度的差值变化不超过 2%，色度的差值变化只在 1% 以内

于是可以将一串连续的相似的帧轨道一个图像群组，也就是 GOP（Group of Pictures）

GOP 中的帧可以分为 3 中类型

- I 帧，也就是 帧内编码图像，也叫关键帧，
  - 是视频的第一帧，也是 GOP 的第一帧，一个 GOP 只有一个帧
  - 编码；对图像数据进行编码
  - 解码：只是用当前帧的编码数据就可以解码出完整的图像
  - I 帧是一种自带全部信息的独立帧，不需要参考其他图像就可以独立进行解码，可以简单理解为一张静态图像

- P 帧，也就是 预测编码图像
  - 编码：并不会对整个帧的编码数据就可以解码出完整的图像，需要以前面的帧或者 P 帧作为参考，只能编码当前与参考帧的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据届澳门出当前 P 帧完整的图像

- B 帧：也就是 前后预测编码图像
  - 编码：并不会对整个帧图形进行编码
    - 同时以前面、后面的帧或 P 帧作为参考帧，只编码当前 B 帧与前后参考的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据解码出当前 B 帧完整的图像

所以编码后的数据大小： I 帧 > P 帧 > B 帧



## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。

## 优化卡顿带来的积累迟延

在网络状态良好的情况下，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

> 在拉流时，音频流、视频流是单独保存到缓冲队列的。如果发生网络抖动，就会引起缓冲抖动，可以总结为网络卡顿导致音视频缓冲队列增大，从而导致解码滞后、播放滞后。此时，我们需要主动丢包来跟进当前时间戳。因为音视频同步一般以音频时钟为基准，人们对音频更加敏感，所以我们优先丢掉视频队列的包。但是，丢视频数据包时，需要丢掉整个GOP的数据包，因为B帧、P帧依赖I帧来解码，否则会引起花屏


[参考](https://blog.csdn.net/u011686167/article/details/85256101?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)

## 音视频直播清晰度、画面质量、流畅度如何保障，降低迟延

> https://zhuanlan.zhihu.com/p/82854047

有一个项目采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如果想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频推送到云服务端的流媒体，再进行大规模的分发和转发。但是我们摄像头以前只支持 TCP 长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将 TCP 推流改成 UDP 推流，

> https://www.sohu.com/a/305914050_134613

首先基本上直播的推流和拉流一般都是在基于 RTMP 协议的，在网络状态良好的情况下 RTMP 协议无论是推流还是拉流都是可以很好工作的，至少在网络不丢包的情况下是很流畅的，但是因为 RTMP 是基于 TCP 协议的，因为 TCP 协议需要通过 3次 握手建立链接，还需要 TLS 2 次握手，还用拥塞不可控等情况，这些情况都会影响音视频播放的流畅度，在弱网环境下延迟会增大。

因此我们选择了使用 UDP，因为 UDP 不需要建立连接，而且速度快，占用资源少。

> 摄像头，并发少，RTMP，TCP --> UDP QUIC协议

我当时采用的是 QUIC 协议 ，QUIC 就是快速 UDP 互联网连接，QUIC 位于应用层(比如 Http)和网络层(UDP)之间，是在 **UDP 上实现的一个多路复用的协议**。

QUIC 替换掉了 TCP 在应用层和传输层中间的角色

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/QUIC.6egow7866pw0.png)

**QUIC 的总体特点**

- 一是可靠性改进，QUIC为了保持TCP的可靠性，几乎继承了TCP所有的特性，比如说序列号改进、拥塞控制、重传机制优化，安全保密性

- 还有一点就是，无队头阻塞的多路复用。QUIC 的多路复用，在一条 QUIC 连接上可以发送多个请求 (stream)，一个连接上的多个请求( stream )之间没有依赖。比如说这个 packet 丢失了，不会影响其他的 stream。这个特性对于直播来说，弱网下推流更流畅。

- ORTT 连接。QUIC 的连接将版本协商、加密、和传输握手交织在一起以减少连接建立延迟。这个特性对于直播来说，使首幀更快，延迟更小。

- 改进的拥塞控制。这个是 QUIC 最重要的一个特性，TCP 的拥塞控制包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复。QUIC 协议当前默认使用了 TCP 协议的 Cubic `[ˈkjuːbɪk]` 拥塞控制算法，同时 QUIC 拥有完善的数据包同步机制，在应用层做了很多网络拥塞控制层面的优化，能有效降低数据丢包率，这有助降低复杂网络下的直播卡顿率，提升传输效率，可是使推流更流畅。

> 在 UDP 上实现多路复用协议，继承 TCP 的序列号，拥塞控制，重传等特性    
> 无队头阻塞的多路复用，一个连接多个请求，packet 丢失互不干扰   
> 版本协商，加密，握手交织在一起，减少建立迟延      
> 改进拥塞控制,TCP 4 个算法，默认使用了 TCP 的 Cubic 算法，完善同步机制

## UDP 实现可靠

> [参考](https://zhuanlan.zhihu.com/p/129218784)

传输层无法保证数据的可靠传输，只能通过应用层来实现了。实现的方式可以参照 tcp 可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。

最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。

- 首先，为了保证可靠性，我们需要在发送数据的时候添加**重传定时器**，来保证丢失的数据会被重传。重传的定时器可以定时回调发送重传的数据，也支持将接收到ACK的数据从定时器中取出。

- 现在有了重传定时器，那每次发送数据的时候，还需要考虑定时器设置多长的超时时间，最简单的可以设定一个固定的重传时间，但是最好是针对每条传输链路的不同设置每个连接的合理时间 -- RTO。

- 为了找到rto时间，我们需要获取到每个数据包发送确认时间，也就是 RTT 时间，即数据从发送到接收到 ACK 确认之间的时间间隔。

- 我们参照 TCP 的实现策略，可以给每个消息记录一个发送时间，当接收到 ACK 确认时，将此时的时间减去记录的发送时间就获取到了 RTT 时间。但这样有一个问题，当发生数据重传时接收到 ACK，无法判断这个 ACK 是对初次发送数据的确认还是对重传数据的确认，此时只能将发生重传数据测量到的 RTT 时间丢弃。

- 所以又有第二种 RTT 计算策略，我们可以将发送时间记录在数据头中发送出去，接受端在发送确认 ACK 时，将这个时间戳抄下来顺着 ACK 返回，这样发送端接收到 ACK 确认时，就能准确的知道要确认数据的发送时间，由此来计算 RTT 时间。有了 RTT 时间，我们按照 TCP 的标准方法《CP/TP详解卷一, P465》，计算 RTO  时间。 

- 当接受到ACK确认时，我们需要将确认的数据从定时器中移除。

> **为了提高网络链路利用率，接收端不能每次接收到数据时都立即发送ACK确认**，为什么呢？

传输的数据量越小，控制头占比越高，而且网络中到处都是只携带一个 ACK 的包在传输，会造成路由器排队。这里可以接着参考 TCP 的实现策略。一种是延时 ACK，即接收端接收到消息时定制一个 pending time，当超时时将这段时间内所有要发送的 ACK 组合在一起发送，还有一种是捎带 ACK，即 pending time 还没到，但恰好也有数据要发送给对端，那么就将 ACK 捎带在这个数据包中一起发送出去。由于接收端 ACK 发送都不是瞬时的，所以在刚刚说到的 RTT 计算时也需要考虑引起的计算误差。


### 你设计的可靠的 UDP 和 TCP 有什么不一样

我们之前都只说了一个数据包的发送接收策略，当大量数据到来时如何发送呢？不可能一下子将所有数据都发送出去。所以我们需要一个发送窗体来控制发送数据的个数，当允许发送时就拿出下一个数据包发送，这发生在接收到新的 ACK 确认或者发送窗体大小调整时。这里和 TCP 的实现不同，TCP 将所有的数据平铺在一个 buffer 里，然后通过移动滑动窗体来控制发送数据流动。我在这里**没有用到滑动窗体**，而是将所有的数据包都放到一个**权限队列**中，按照发送两个高一级权限数据包一个低一级数据包的规则来调整发送顺序，发送窗体中只有 inflight 的数据包，当可以发送下一个数据包时，再从权限队列中获取。发送窗体负责对发送后的数据缓存，确认，权限队列负责给发送的数据按优先级排序。

对接收端而言，也需要一个**接收队列**对接收到的数据包进行整理，这里我们可以根据需求的不同实现多种排队策略。如果是想得到 TCP 的效果，数据即有序，又可靠，那我们需要给所有到达的数据包发送 ACK 确认且排队，只有前一个数据包排好队，无乱序时，才能将数据反回给上层；如果只实现可靠性，不需要有序，那可以接收到一个数据包时，直接反回给上层，但是要发送 ACK 确认。如果只需要有序性，不需要可靠性，那可以记录目前收到最大的数据包序号，比这个序号大的数据包返回给上层，比这个序号小的直接丢弃，也不需要发送 ACK，因为发送端也不会重传数据。

### 设计的 UDP 的建立和连接

传输建立时并没有参考 TCP 的三次握手，依照 UDP 的简单粗暴，发送端只管发送数据，接收端能收到算建立了连接，没有接收到则发送端超时。因为我们的协议实现在应用层，没有进程启动的时候也无法发送 RST 给对端。连接断开时基本参考了 TCP 的四次挥手实现，继续保留了 TIME_WAIT 状态来保证网络中上一个连接的数据包不会发送到现任连接上。

## RTMP 协议与 HLS 协议

- RTMP 协议是实时消息协议。但它实际上并不能做到真正的实时，一般情况最少都会有几秒到几十秒的延迟，底层是基于 TCP 协议的

- HLS，全称 HTTP Live Streaming，是苹果公司实现的基于 HTTP 的流媒体传输协议。它可以支持流媒体的直播和点播，主要应用在 iOS 系统和 HTML5 网页播放器中。   
HLS 协议的本质就是通过 HTTP 下载文件，然后将下载的切片缓存起来。由于切片文件都非常小，所以可以实现边下载边播的效果。HLS 规范规定，播放器至少下载一个 ts 切片才能播放，所以 HLS 理论上至少会有一个切片的延迟。

HLS 最主要的问题就是实时性差。由于 HLS 往往采用 10s 的切片，所以最小也要有 10s 的延迟，一般是 20～30s 的延迟，有时甚至更差。HLS 之所以能达到 20～30s 的延迟，主要是由于 HLS 的实现机制造成的。HLS 使用的是 HTTP 短连接，且 HTTP 是基于 TCP 的，所以这就意味着 HLS 需要不断地与服务器建立连接。TCP 每次建立连接时都要进行三次握手，而断开连接时，也要进行四次挥手，基于以上这些复杂的原因，就造成了 HLS 延迟比较久的局面。

在 PC 上，我们使用 RTMP 协议，因为 PC 基本都安装了 Flash 播放器，直播效果要好很多。

点播系统使用 HLS 协议。因为点播没有实时互动需求，延迟大一些是可以接受的，并且可以在浏览器上直接观看。

> 在拉取 HLS 媒体流时，客户端首先通过 HLS 协议将 m3u8 索引文件下载下来，然后按索引文件中的顺序，将 .ts 文件一片一片下载下来，然后一边播放一边缓冲。此时，你就可以在 PC、手机、平板等设备上观看直播节目了。

**对于使用 HLS 协议的直播系统来说，最重要的一步就是切片**。源节点服务器收到音视频流后，先要数据缓冲起来，保证到达帧的所有分片都已收到之后，才会将它们切片成 TS 流。

> PC 上使用 RTMP 协议，点播使用 HLS 协议，点播没有实时互动需求    
> RTMP 基于 TCP，实时性较好，但还是存在   
> HLS 通过 HTTP 下载文件 --> 切片缓存，切片小，可边下载边播放，切片 10s,迟延 10 s，而且也是基于 TCP     
> 切割使用 ffmpeg



## AVFormatContext 和 AVInputFormat之间的关系

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/ffmpeg数据结构04.fnx3k76bak0.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.

## FFmpeg 数据结构 AVBuffer

[参考](https://www.cnblogs.com/leisure_chn/p/10399048.html)

AVBuffer 是 FFmpeg中很常用的一种缓冲区，缓冲区使用引用计数(reference-counted)机制。

AVBufferRef 则对 AVBuffer 缓冲区提供了一层封装，最主要的是作引用计数处理，实现了一种安全机制。

用户不应直接访问 AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

**数据结构定义**

- struct AVBuffer

struct AVBuffer 定义于 “`libavutil/buffer_internal.h`”，`buffer_internal.h` 位于 FFmpeg 工程源码中，而 FFmpeg 提供的开发库头文件中并无此文件，因此这是一个内部数据结构，不向用户开放，用户不应直接访问AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

内部通过 `av_buffer_ref()` 来增加引用计数，通过 `av_buffer_unref()` 来减少引用计数

销毁一个 AVBufferRef 时，将其AVBuffer缓冲区引用计数减1，若缓冲区引用计数变为0，则将缓冲区也回收，
