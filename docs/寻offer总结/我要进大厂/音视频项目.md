
- [怎么使用多线程机实现录制和播放](#怎么使用多线程机实现录制和播放)
- [播放 PCM](#播放-pcm)
- [H264 编码](#h264-编码)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
- [基于 VideoToolbox 来实现 H264 硬编码和硬解码](#基于-videotoolbox-来实现-h264-硬编码和硬解码)
  - [H264 硬编码](#h264-硬编码)
    - [遇到难题](#遇到难题)
  - [H264 硬解码](#h264-硬解码)
- [ffmpeg 部分源码](#ffmpeg-部分源码)
  - [ffmpeg 源码中内存管理](#ffmpeg-源码中内存管理)
  - [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)
  - [FFmpeg 数据结构 AVBuffer](#ffmpeg-数据结构-avbuffer)
- [为什么要有YUV这种数据出来（YUV相比RGB来说的优点）](#为什么要有yuv这种数据出来yuv相比rgb来说的优点)
- [H264/H265有什么区别](#h264h265有什么区别)
- [H264 解码过程](#h264-解码过程)
  - [帧划分类型](#帧划分类型)
- [视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](#视频或者音频传输你会选择tcp协议还是udp协议为什么)
- [项目中怎么使用 flv.js 和 video.js](#项目中怎么使用-flvjs-和-videojs)
- [缓冲区控制，优化卡顿带来的积累迟延](#缓冲区控制优化卡顿带来的积累迟延)

---- 

## 怎么使用多线程机实现录制和播放

如果录制和播放都放在主线程中执行的话，就会造成界面一直处于阻塞状态，所以就将播放和录制的逻辑放在子线程中执行，当线程启动的时候就会自动调用 run 函数。这样就可以用多个线程分别处理不同的任务。

## 播放 PCM

播放 PCM 使用的是多媒体开发库 SDL 来实现的，播放的步骤是

- 先初始化子系统，就是调用 SDL_Init 函数，传入的参数决定播放音频还是视频

- 接着是调用 SDL_OpenAudio 开发音频设备，

查看音频设备可以通过 ffmpeg 命令查看

```
./ffmpeg.exe -f dshow -list_devices true -i dunmy
```

SDL_OpenAudio 这个函数插进去的是一个 SDL_AudioSpec 的结构体，这个结构体中包含音频的各种数据，比如采样率，频道等等。

- 打开文件开始播放 PCM 数据

然后播放 PCM 数据在 SDL 有两种方式

1.一种是 push, 程序主动推送数据给音频设备

2.另一种是 pull，音频设备主动从程序中拉取数据

> 这里选择第二种

- 最后调用 SDL_Quit 清楚子系统释放资源

## H264 编码

原始视频如果不经过压缩的话是会占用很大内存的，由于网络带宽和硬盘存储空间都非常有限，需要先试用视频编码技术对原始视频进行压缩，然后再进行存储和分发。H264 的压缩比率大概是 100 ：1

- 编码器采用的是 X264，在 ffmpeg 中的名称是 libx264

- 解码器采用的是 ffmpeg 默认内置的 H264 解码器。名称就是 h264


### 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是  `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。



## 基于 VideoToolbox 来实现 H264 硬编码和硬解码

### H264 硬编码

> https://zfpp25.blog.csdn.net/article/details/108219421

iOS8.0及以上我们可以通过 VideoToolbox 实现视频数据的硬编解码。

基本步骤

- 1、通过 VTCompressionSessionCreate 创建编码器
- 2、通过 VTSessionSetProperty 设置编码器属性
- 3、设置完属性调用 VTCompressionSessionPrepareToEncodeFrames 准备编码
- 4、输入采集到的视频数据，调用 VTCompressionSessionEncodeFrame 进行编码
- 5、获取到编码后的数据并进行处理
- 6、调用 VTCompressionSessionCompleteFrames 停止编码器
- 7、调用 VTCompressionSessionInvalidate 销毁编码器

> 创建编码器 --> 设置编码器的属性 --> 然后准备进行编码 --> 接着是对编码后的数据进行处理 --> 最后还需要 停止和销毁编码器     
> 主要是用 `VTCompressionSession` 下的几个函数

#### 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率 1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。

一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。

后来怀疑是 ABR 模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。 

后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！

### H264 硬解码

> https://zfpp25.blog.csdn.net/article/details/108219440

硬解码流程很简单：
- 1、解析H264数据
- 2、初始化解码器（VTDecompressionSessionCreate）
- 3、将解析后的 H264 数据送入解码器（VTDecompressionSessionDecodeFrame）
- 4、解码器回调输出解码后的数据（CVImageBufferRef）


## ffmpeg 部分源码


ffmpeg博客：https://www.cnblogs.com/leisure_chn/category/1351812.html

### ffmpeg 源码中内存管理

> https://blog.csdn.net/King1425/article/details/70613310

`av_malloc()`: 是内存分配函数，`av_malloc() `就是简单的封装了系统函数malloc()，并做了一些错误检查工作。

`av_realloc()`用于对申请的内存的大小进行调整. `av_realloc()` 简单封装了系统的`realloc()`函数。


`av_free()`用于释放申请的内存,`av_free()` 简单的封装了`free`

`av_freep()`简单封装了`av_free()`。并且在释放内存之后将目标指针设置为 NULL

### AVFormatContext 和 AVInputFormat之间的关系

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/ffmpeg数据结构04.fnx3k76bak0.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.

### FFmpeg 数据结构 AVBuffer

[参考](https://www.cnblogs.com/leisure_chn/p/10399048.html)

AVBuffer 是 FFmpeg中很常用的一种缓冲区，缓冲区使用引用计数(reference-counted)机制。

AVBufferRef 则对 AVBuffer 缓冲区提供了一层封装，最主要的是作引用计数处理，实现了一种安全机制。

用户不应直接访问 AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

**数据结构定义**

- struct AVBuffer

struct AVBuffer 定义于 “`libavutil/buffer_internal.h`”，`buffer_internal.h` 位于 FFmpeg 工程源码中，而 FFmpeg 提供的开发库头文件中并无此文件，因此这是一个内部数据结构，不向用户开放，用户不应直接访问AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

内部通过 `av_buffer_ref()` 来增加引用计数，通过 `av_buffer_unref()` 来减少引用计数

销毁一个 AVBufferRef 时，将其AVBuffer缓冲区引用计数减1，若缓冲区引用计数变为0，则将缓冲区也回收，


## 为什么要有YUV这种数据出来（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全 0时为黑色，全255时为白色。RGB 是一种依赖于设备的颜色空间：

不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。

Y'UV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV 和 RGB 是可以相互转换的，基本上所有图像算法都是基于 YUV 的，所有显示面板都是接收 RGB 数据。

## H264/H265有什么区别

同样的画质和同样的码率，H.265 比 H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下 H.265 会比 H.264 画质要高一些理论值是30%~40%。

比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，最小的 8x8 到最大的 64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265 标准主要是围绕着现有的视频编码标准 H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265 研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。


## H264 解码过程

大体可以分为几个主要的步骤

- 划分帧类型
- 帧内/帧间编码
- 变换 + 向量
- 滤波
- 熵编码

### 帧划分类型

在连续的几帧的图像中，一般只有 10% 以内的像素有差别，亮度的差值变化不超过 2%，色度的差值变化只在 1% 以内

于是可以将一串连续的相似的帧轨道一个图像群组，也就是 GOP（Group of Pictures）

GOP 中的帧可以分为 3 中类型

- I 帧，也就是 帧内编码图像，也叫关键帧，
  - 是视频的第一帧，也是 GOP 的第一帧，一个 GOP 只有一个帧
  - 编码；对图像数据进行编码
  - 解码：只是用当前帧的编码数据就可以解码出完整的图像
  - I 帧是一种自带全部信息的独立帧，不需要参考其他图像就可以独立进行解码，可以简单理解为一张静态图像

- P 帧，也就是 预测编码图像
  - 编码：并不会对整个帧的编码数据就可以解码出完整的图像，需要以前面的帧或者 P 帧作为参考，只能编码当前与参考帧的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据届澳门出当前 P 帧完整的图像

- B 帧：也就是 前后预测编码图像
  - 编码：并不会对整个帧图形进行编码
    - 同时以前面、后面的帧或 P 帧作为参考帧，只编码当前 B 帧与前后参考的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据解码出当前 B 帧完整的图像

所以编码后的数据大小： I 帧 > P 帧 > B 帧



## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。


## 项目中怎么使用 flv.js 和 video.js

flv.js 是B站开源的项目，它可以解析 FLV 文件，提取出音视频数据并转成一种 MP4 格式，然后交给 HTML5 的`<video>`标签进行播放。通过这种方式，使得浏览器在不借助 Flash 的情况下也可以播放 FLV 文件了。

`flv.js` 更聚焦在多媒体格式方面，其主要是将 FLV 格式转换为 MP4 格式，而对于播放器的音量控制、进度条、菜单等 UI 展示部分没有做特别的处理。而 `video.js` 对音量控制、进度条、菜单等 UI 相关逻辑做了统一处理，对媒体播放部分设计了一个插件框架，可以集成不同媒体格式的播放器进去。所以相比较而言，`video.js` 更像是一款完整的播放器。

> flv.js 将 FLV 文件，提出去音频转成 BMFF ，一种 MP4 格式，交给 `<video>` 播放。没有做 播放控制，进度条 等处理    
> `video.js` 对音量控制，进度统一处理

## 缓冲区控制，优化卡顿带来的积累迟延

在网络状态良好的情况下，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

[参考](https://blog.csdn.net/u011686167/article/details/85256101?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)