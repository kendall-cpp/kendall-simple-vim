
- [怎么使用多线程机实现录制和播放](#怎么使用多线程机实现录制和播放)
<<<<<<< HEAD
- [H264 编码](#h264-编码)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
    - [遇到难题](#遇到难题)
- [ffmpeg 部分源码](#ffmpeg-部分源码)
=======
- [播放 PCM](#播放-pcm)
- [H264 编码](#h264-编码)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
- [基于 VideoToolbox 来实现 H264 硬编码和硬解码](#基于-videotoolbox-来实现-h264-硬编码和硬解码)
  - [H264 硬编码](#h264-硬编码)
    - [遇到难题](#遇到难题)
  - [H264 硬解码](#h264-硬解码)
- [ffmpeg 部分源码](#ffmpeg-部分源码)
  - [ffmpeg 源码中内存管理](#ffmpeg-源码中内存管理)
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4
  - [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)
  - [FFmpeg 数据结构 AVBuffer](#ffmpeg-数据结构-avbuffer)
- [为什么要有YUV这种数据出来（YUV相比RGB来说的优点）](#为什么要有yuv这种数据出来yuv相比rgb来说的优点)
- [H264/H265有什么区别](#h264h265有什么区别)
- [H264 解码过程](#h264-解码过程)
  - [帧划分类型](#帧划分类型)
- [视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](#视频或者音频传输你会选择tcp协议还是udp协议为什么)
<<<<<<< HEAD
- [缓冲区控制，优化卡顿带来的积累迟延](#缓冲区控制优化卡顿带来的积累迟延)
- [音视频直播清晰度、画面质量、流畅度如何保障，降低迟延](#音视频直播清晰度画面质量流畅度如何保障降低迟延)
- [RTMP 协议与 HLS 协议](#rtmp-协议与-hls-协议)
=======
- [项目中怎么使用 flv.js 和 video.js](#项目中怎么使用-flvjs-和-videojs)
- [缓冲区控制，优化卡顿带来的积累迟延](#缓冲区控制优化卡顿带来的积累迟延)
- [音视频直播清晰度、画面质量、流畅度如何保障，降低迟延](#音视频直播清晰度画面质量流畅度如何保障降低迟延)
- [RTMP 协议与 HLS 协议](#rtmp-协议与-hls-协议)
- [FFmpeg 将 MP4 文件切割成 HLS 格式](#ffmpeg-将-mp4-文件切割成-hls-格式)
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4

---- 

## 怎么使用多线程机实现录制和播放

如果录制和播放都放在主线程中执行的话，就会造成界面一直处于阻塞状态，所以就将播放和录制的逻辑放在子线程中执行，当线程启动的时候就会自动调用 run 函数。这样就可以用多个线程分别处理不同的任务。

<<<<<<< HEAD

=======
## 播放 PCM

播放 PCM 使用的是多媒体开发库 SDL 来实现的，播放的步骤是

- 先初始化子系统，就是调用 SDL_Init 函数，传入的参数决定播放音频还是视频

- 接着是调用 SDL_OpenAudio 开发音频设备，

查看音频设备可以通过 ffmpeg 命令查看

```
./ffmpeg.exe -f dshow -list_devices true -i dunmy
```

SDL_OpenAudio 这个函数插进去的是一个 SDL_AudioSpec 的结构体，这个结构体中包含音频的各种数据，比如采样率，频道等等。

- 打开文件开始播放 PCM 数据

然后播放 PCM 数据在 SDL 有两种方式

1.一种是 push, 程序主动推送数据给音频设备

2.另一种是 pull，音频设备主动从程序中拉取数据

> 这里选择第二种

- 最后调用 SDL_Quit 清楚子系统释放资源
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4

## H264 编码

原始视频如果不经过压缩的话是会占用很大内存的，由于网络带宽和硬盘存储空间都非常有限，需要先试用视频编码技术对原始视频进行压缩，然后再进行存储和分发。H264 的压缩比率大概是 100 ：1

- 编码器采用的是 X264，在 ffmpeg 中的名称是 libx264

- 解码器采用的是 ffmpeg 默认内置的 H264 解码器。名称就是 h264


### 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是  `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。



<<<<<<< HEAD
=======
## 基于 VideoToolbox 来实现 H264 硬编码和硬解码

### H264 硬编码

> https://zfpp25.blog.csdn.net/article/details/108219421

iOS8.0及以上我们可以通过 VideoToolbox 实现视频数据的硬编解码。

基本步骤

- 1、通过 VTCompressionSessionCreate 创建编码器
- 2、通过 VTSessionSetProperty 设置编码器属性
- 3、设置完属性调用 VTCompressionSessionPrepareToEncodeFrames 准备编码
- 4、输入采集到的视频数据，调用 VTCompressionSessionEncodeFrame 进行编码
- 5、获取到编码后的数据并进行处理
- 6、调用 VTCompressionSessionCompleteFrames 停止编码器
- 7、调用 VTCompressionSessionInvalidate 销毁编码器

> 创建编码器 --> 设置编码器的属性 --> 然后准备进行编码 --> 接着是对编码后的数据进行处理 --> 最后还需要 停止和销毁编码器     
> 主要是用 `VTCompressionSession` 下的几个函数

>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4
#### 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率 1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。

一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。

后来怀疑是 ABR 模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。 

后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！

<<<<<<< HEAD
=======
### H264 硬解码

> https://zfpp25.blog.csdn.net/article/details/108219440

硬解码流程很简单：
- 1、解析H264数据
- 2、初始化解码器（VTDecompressionSessionCreate）
- 3、将解析后的 H264 数据送入解码器（VTDecompressionSessionDecodeFrame）
- 4、解码器回调输出解码后的数据（CVImageBufferRef）
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4


## ffmpeg 部分源码


<<<<<<< HEAD
=======
ffmpeg博客：https://www.cnblogs.com/leisure_chn/category/1351812.html

### ffmpeg 源码中内存管理

> https://blog.csdn.net/King1425/article/details/70613310

`av_malloc()`: 是内存分配函数，`av_malloc() `就是简单的封装了系统函数malloc()，并做了一些错误检查工作。

`av_realloc()`用于对申请的内存的大小进行调整. `av_realloc()` 简单封装了系统的`realloc()`函数。


`av_free()`用于释放申请的内存,`av_free()` 简单的封装了`free`

`av_freep()`简单封装了`av_free()`。并且在释放内存之后将目标指针设置为 NULL
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4

### AVFormatContext 和 AVInputFormat之间的关系

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/ffmpeg数据结构04.fnx3k76bak0.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.

### FFmpeg 数据结构 AVBuffer

[参考](https://www.cnblogs.com/leisure_chn/p/10399048.html)

AVBuffer 是 FFmpeg中很常用的一种缓冲区，缓冲区使用引用计数(reference-counted)机制。

AVBufferRef 则对 AVBuffer 缓冲区提供了一层封装，最主要的是作引用计数处理，实现了一种安全机制。

用户不应直接访问 AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

**数据结构定义**

- struct AVBuffer

struct AVBuffer 定义于 “`libavutil/buffer_internal.h`”，`buffer_internal.h` 位于 FFmpeg 工程源码中，而 FFmpeg 提供的开发库头文件中并无此文件，因此这是一个内部数据结构，不向用户开放，用户不应直接访问AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

内部通过 `av_buffer_ref()` 来增加引用计数，通过 `av_buffer_unref()` 来减少引用计数

销毁一个 AVBufferRef 时，将其AVBuffer缓冲区引用计数减1，若缓冲区引用计数变为0，则将缓冲区也回收，


## 为什么要有YUV这种数据出来（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全 0时为黑色，全255时为白色。RGB 是一种依赖于设备的颜色空间：

不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。

Y'UV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV 和 RGB 是可以相互转换的，基本上所有图像算法都是基于 YUV 的，所有显示面板都是接收 RGB 数据。

## H264/H265有什么区别

同样的画质和同样的码率，H.265 比 H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下 H.265 会比 H.264 画质要高一些理论值是30%~40%。

比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，最小的 8x8 到最大的 64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265 标准主要是围绕着现有的视频编码标准 H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265 研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。


## H264 解码过程

大体可以分为几个主要的步骤

- 划分帧类型
- 帧内/帧间编码
- 变换 + 向量
- 滤波
- 熵编码

### 帧划分类型

在连续的几帧的图像中，一般只有 10% 以内的像素有差别，亮度的差值变化不超过 2%，色度的差值变化只在 1% 以内

于是可以将一串连续的相似的帧轨道一个图像群组，也就是 GOP（Group of Pictures）

GOP 中的帧可以分为 3 中类型

- I 帧，也就是 帧内编码图像，也叫关键帧，
  - 是视频的第一帧，也是 GOP 的第一帧，一个 GOP 只有一个帧
  - 编码；对图像数据进行编码
  - 解码：只是用当前帧的编码数据就可以解码出完整的图像
  - I 帧是一种自带全部信息的独立帧，不需要参考其他图像就可以独立进行解码，可以简单理解为一张静态图像

- P 帧，也就是 预测编码图像
  - 编码：并不会对整个帧的编码数据就可以解码出完整的图像，需要以前面的帧或者 P 帧作为参考，只能编码当前与参考帧的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据届澳门出当前 P 帧完整的图像

- B 帧：也就是 前后预测编码图像
  - 编码：并不会对整个帧图形进行编码
    - 同时以前面、后面的帧或 P 帧作为参考帧，只编码当前 B 帧与前后参考的差异数据
  - 解码：需要先解码出前面的参考帧，再结合差异数据解码出当前 B 帧完整的图像

所以编码后的数据大小： I 帧 > P 帧 > B 帧



## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。


<<<<<<< HEAD
=======
## 项目中怎么使用 flv.js 和 video.js

flv.js 是B站开源的项目，它可以解析 FLV 文件，提取出音视频数据并转成一种 MP4 格式，然后交给 HTML5 的`<video>`标签进行播放。通过这种方式，使得浏览器在不借助 Flash 的情况下也可以播放 FLV 文件了。

`flv.js` 更聚焦在多媒体格式方面，其主要是将 FLV 格式转换为 MP4 格式，而对于播放器的音量控制、进度条、菜单等 UI 展示部分没有做特别的处理。而 `video.js` 对音量控制、进度条、菜单等 UI 相关逻辑做了统一处理，对媒体播放部分设计了一个插件框架，可以集成不同媒体格式的播放器进去。所以相比较而言，`video.js` 更像是一款完整的播放器。

> flv.js 将 FLV 文件，提出去音频转成 BMFF ，一种 MP4 格式，交给 `<video>` 播放。没有做 播放控制，进度条 等处理    
> `video.js` 对音量控制，进度统一处理
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4

## 缓冲区控制，优化卡顿带来的积累迟延

在网络状态良好的情况下，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

[参考](https://blog.csdn.net/u011686167/article/details/85256101?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)

## 音视频直播清晰度、画面质量、流畅度如何保障，降低迟延

> https://zhuanlan.zhihu.com/p/82854047

有一个项目采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如果想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频推送到云服务端的流媒体，再进行大规模的分发和转发。但是我们摄像头以前只支持 TCP 长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将 TCP 推流改成 UDP 推流，

> https://www.sohu.com/a/305914050_134613

首先基本上直播的推流和拉流一般都是在基于 RTMP 协议的，在网络状态良好的情况下 RTMP 协议无论是推流还是拉流都是可以很好工作的，至少在网络不丢包的情况下是很流畅的，但是因为 RTMP 是基于 TCP 协议的，因为 TCP 协议需要通过 3次 握手建立链接，还需要 TLS 2 次握手，还用拥塞不可控等情况，这些情况都会影响音视频播放的流畅度，在弱网环境下延迟会增大。

因此我们选择了使用 UDP，因为 UDP 不需要建立连接，而且速度快，占用资源少。

> 摄像头，并发少，RTMP，TCP --> UDP QUIC协议

我当时采用的是 QUIC 协议 ，QUIC 就是快速 UDP 互联网连接，QUIC 位于应用层(比如 Http)和网络层(UDP)之间，是在 **UDP 上实现的一个多路复用的协议**。

QUIC 替换掉了 TCP 在应用层和传输层中间的角色

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/QUIC.6egow7866pw0.png)

**QUIC 的总体特点**

- 一是可靠性改进，QUIC为了保持TCP的可靠性，几乎继承了TCP所有的特性，比如说序列号改进、拥塞控制、重传机制优化，安全保密性

- 还有一点就是，无队头阻塞的多路复用。QUIC 的多路复用，在一条 QUIC 连接上可以发送多个请求 (stream)，一个连接上的多个请求( stream )之间没有依赖。比如说这个 packet 丢失了，不会影响其他的 stream。这个特性对于直播来说，弱网下推流更流畅。

- ORTT 连接。QUIC 的连接将版本协商、加密、和传输握手交织在一起以减少连接建立延迟。这个特性对于直播来说，使首幀更快，延迟更小。

- 改进的拥塞控制。这个是 QUIC 最重要的一个特性，TCP 的拥塞控制包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复。QUIC 协议当前默认使用了 TCP 协议的 Cubic `[ˈkjuːbɪk]` 拥塞控制算法，同时 QUIC 拥有完善的数据包同步机制，在应用层做了很多网络拥塞控制层面的优化，能有效降低数据丢包率，这有助降低复杂网络下的直播卡顿率，提升传输效率，可是使推流更流畅。

> 在 UDP 上实现多路复用协议，继承 TCP 的序列号，拥塞控制，重传等特性    
> 无队头阻塞的多路复用，一个连接多个请求，packet 丢失互不干扰   
> 版本协商，加密，握手交织在一起，减少建立迟延      
> 改进拥塞控制,TCP 4 个算法，默认使用了 TCP 的 Cubic 算法，完善同步机制


## RTMP 协议与 HLS 协议

- RTMP 协议是实时消息协议。但它实际上并不能做到真正的实时，一般情况最少都会有几秒到几十秒的延迟，底层是基于 TCP 协议的

- HLS，全称 HTTP Live Streaming，是苹果公司实现的基于 HTTP 的流媒体传输协议。它可以支持流媒体的直播和点播，主要应用在 iOS 系统和 HTML5 网页播放器中。   
HLS 协议的本质就是通过 HTTP 下载文件，然后将下载的切片缓存起来。由于切片文件都非常小，所以可以实现边下载边播的效果。HLS 规范规定，播放器至少下载一个 ts 切片才能播放，所以 HLS 理论上至少会有一个切片的延迟。

HLS 最主要的问题就是实时性差。由于 HLS 往往采用 10s 的切片，所以最小也要有 10s 的延迟，一般是 20～30s 的延迟，有时甚至更差。HLS 之所以能达到 20～30s 的延迟，主要是由于 HLS 的实现机制造成的。HLS 使用的是 HTTP 短连接，且 HTTP 是基于 TCP 的，所以这就意味着 HLS 需要不断地与服务器建立连接。TCP 每次建立连接时都要进行三次握手，而断开连接时，也要进行四次挥手，基于以上这些复杂的原因，就造成了 HLS 延迟比较久的局面。

在 PC 上，我们使用 RTMP 协议，因为 PC 基本都安装了 Flash 播放器，直播效果要好很多。

点播系统使用 HLS 协议。因为点播没有实时互动需求，延迟大一些是可以接受的，并且可以在浏览器上直接观看。

> 在拉取 HLS 媒体流时，客户端首先通过 HLS 协议将 m3u8 索引文件下载下来，然后按索引文件中的顺序，将 .ts 文件一片一片下载下来，然后一边播放一边缓冲。此时，你就可以在 PC、手机、平板等设备上观看直播节目了。

**对于使用 HLS 协议的直播系统来说，最重要的一步就是切片**。源节点服务器收到音视频流后，先要数据缓冲起来，保证到达帧的所有分片都已收到之后，才会将它们切片成 TS 流。

> PC 上使用 RTMP 协议，点播使用 HLS 协议，点播没有实时互动需求    
> RTMP 基于 TCP，实时性较好，但还是存在   
> HLS 通过 HTTP 下载文件 --> 切片缓存，切片小，可边下载边播放，切片 10s,迟延 10 s，而且也是基于 TCP     
> 切割使用 ffmpeg

<<<<<<< HEAD
=======
## FFmpeg 将 MP4 文件切割成 HLS 格式

我们是通过 FFmpeg 将 MP4 文件切割成 HLS 格式

```
ffmpeg -i test.mp4 -c copy -start_number 0 -hls_time 10 -hls_list_size 0 -hls_segment_filename test%03d.ts index.m3u8
```

- -i ，输入文件选项，可以是磁盘文件，也可以是媒体设备。
- -c copy，表示只是进行封装格式的转换。不需要将多媒体文件中的音视频数据重新进行编码。
- -start_number，表示 .ts 文件的起始编号，这里设置从 0 开始。当然，你也可以设置其他数字。
- -hls_time，表示每个 .ts 文件的最大时长，单位是秒。这里设置的是 10s，表示每个切片文件的时长，为 10 秒。当然，由于没有进行重新编码，所以这个时长并不准确。
- -hls_list_size，表示播放列表文件的长度，0 表示不对播放列表文件的大小进行限制。
- -hls_segment_filename，表示指定 TS 文件的名称。index.m3u8，表示索引文件名称。
>>>>>>> 51aa1e8b5901fe36b629fd1cd7202aa75899dfe4
