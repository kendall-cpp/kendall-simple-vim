
- [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
- [为什么要有YUV这种数据出来（YUV相比RGB来说的优点）](#为什么要有yuv这种数据出来yuv相比rgb来说的优点)
- [H264/H265有什么区别](#h264h265有什么区别)
- [H264](#h264)
  - [H264 码流分层结构](#h264-码流分层结构)
  - [VCL & NAL](#vcl--nal)
  - [片（slice）](#片slice)
    - [slice 组成](#slice-组成)
  - [宏块（Macroblock）](#宏块macroblock)
  - [帧（frame）和场（filed）](#帧frame和场filed)
  - [帧划分类型](#帧划分类型)
  - [GOP](#gop)
  - [IDR](#idr)
- [播放器的基本原理](#播放器的基本原理)
- [使用边缓存边播放对视频播放进行优化](#使用边缓存边播放对视频播放进行优化)
  - [实现原理](#实现原理)
- [优化卡顿带来的积累迟延](#优化卡顿带来的积累迟延)
- [视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](#视频或者音频传输你会选择tcp协议还是udp协议为什么)
- [音视频直播清晰度、画面质量，降低迟延(TCP改成UDP)](#音视频直播清晰度画面质量降低迟延tcp改成udp)
- [UDP 实现可靠传输](#udp-实现可靠传输)
  - [窗口与拥塞控制](#窗口与拥塞控制)
  - [RTT 与 RTO 的计算](#rtt-与-rto-的计算)
- [RTMP 协议](#rtmp-协议)
  - [RTMP 的握手过程](#rtmp-的握手过程)
  - [大概流程](#大概流程)
  - [RTMP 的传输：消息块 & 消息封包传输](#rtmp-的传输消息块--消息封包传输)
  - [RTMP 的传输：消息块的组成](#rtmp-的传输消息块的组成)
    - [基础数据头](#基础数据头)
    - [消息数据头](#消息数据头)
- [FFmpeg 数据结构 AVBuffer](#ffmpeg-数据结构-avbuffer)
- [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)
- [遇到难题](#遇到难题)


---- 


## 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是  `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。

---



## 为什么要有YUV这种数据出来（YUV相比RGB来说的优点）

RGB是指光学三原色红、绿和蓝，通过这3种的数值（0-255）改变可以组成其他颜色，全 0时为黑色，全255时为白色。RGB 是一种依赖于设备的颜色空间：

不同设备对特定RGB值的检测和重现都不一样，因为颜色物质（荧光剂或者染料）和它们对红、绿和蓝的单独响应水平随着制造商的不同而不同，甚至是同样的设备不同的时间也不同。

YUV，是一种颜色编码方法。常使用在各个视频处理组件中。三个字母分别表示亮度信号Y和两个色差信号R－Y（即U）、B－Y（即V），作用是描述影像色彩及饱和度，用于指定像素的颜色。

YUV的发明是由于彩色电视与黑白电视的过渡时期。黑白视频只有Y视频，也就是灰阶值。与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV 和 RGB 是可以相互转换的，基本上所有图像算法都是基于 YUV 的，所有显示面板都是接收 RGB 数据。



## H264/H265有什么区别

同样的画质和同样的码率，H.265 比 H2.64 占用的存储空间要少理论50%。如果存储空间一样大，那么意味着，在一样的码率下 H.265 会比 H.264 画质要高一些理论值是30%~40%。

比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，最小的 8x8 到最大的 64x64。信息量不多的区域(颜色变化不明显)划分的宏块较大，编码后的码字较少，而细节多的地方划分的宏块就相应的小和多一些，编码后的码字较多，这样就相当于对图像进行了有重点的编码，从而降低了整体的码率，编码效率就相应提高了。

H.265 标准主要是围绕着现有的视频编码标准 H.264，在保留了原有的某些技术外，增加了能够改善码流、编码质量、延时及算法复杂度之间的关系等相关的技术。H.265 研究的主要内容包括，提高压缩效率、提高鲁棒性和错误恢复能力、减少实时的时延、减少信道获取时间和随机接入时延、降低复杂度。

## H264

[参考](https://blog.csdn.net/qq_19923217/article/details/83348095)

![](https://gitee.com/kevin1993175/image_resource/raw/master/nalu_layout.png)

H264 是 MPEG-4 标准所定义的最新编码格式。

### H264 码流分层结构

在 H264 中，句法元素共被组织成：序列、图像（帧）、片、宏块、子宏块五个层次。

### VCL & NAL

H264 原始码流是由一个接一个 NALU（NAL Unit） 组成，它的功能分为两层，VCL（Video Coding Layer）视频编码层和 NAL（Network Abstraction Layer）网络提取层。

- VCL：包括核心压缩引擎和块、宏块和片的语法级别定义，设计目标是尽可能地独立于网络进行高效的编码；
- NAL：负责将 VCL 产生的比特字符串适配到各种各样的网络和多元环境中，覆盖了所有片级以上的语法级别；

NAL是 H.264 为适应网络传输应用而制定的一层数据打包操作。传统的视频编码算法编完的视频码流在任何应用领域下（无论用于存储、传输等）都是统一的码流模式，视频码流仅有视频编码层 VCL（Video Coding Layer）。而 H.264 可根据不同应用增加不同的 NAL 片头，以适应不同的网络应用环境，减少码流的传输差错。

在 VCL 进行数据传输或存储之前，这些编码的 VCL 数据，被映射或封装进NAL单元（NALU）。

```
一个 NALU = 一组对应于视频编码的 NALU 头部信息 + 一个原始字节序列负荷（RBSP，Raw Byte Sequence Payload）
```

![](https://gitee.com/kevin1993175/image_resource/raw/master/nalu_set.png)

一个原始的 H.264 NALU 单元常由 [StartCode] [NALU Header] [NALU Payload] 三部分组成，其中 Start Code 用于标示这是一个NALU 单元的开始，必须是 “00 00 00 01”

> 实际原始视频图像数据保存在 VCL 分层的 NAL Units 中

### 片（slice）

> 一个片 = Slice Header + Slice Data

片是 H.264 提出的新概念，实际原始视频图像数据保存在 VCL 层级的 NAL Unit 中，这部分数据在码流中被称作是片（slice）。一个 slice 包含一帧图像的部分或全部数据，换言之，一帧视频图像可以编码为一个或若干个 slice。一个 slice 最少包含一个宏块，最多包含整帧图像的数据。在不同的编码实现中，同一帧图像中所构成的 slice 数目不一定相同。

> 一个 slice 编码之后被打包进一个 NALU，所以 slice = NALU

> 那么为什么要设置片呢?

设置片的目的是为了限制误码的扩散和传输，应使编码片相互间是独立的。某片的预测不能以其他片中的宏块为参考图像，这样某一片中的预测误差才不会传播到其他片中。

在上图中，可以看到每个图像中，若干宏块（Macroblock）被排列成片。一个视频图像可编成一个或更多个片，每片包含整数个宏块 (MB)，每片至少包含一个宏块。

|  slice   | 意义  |
|  ----  | ----  |
| I slice  | 只包含 I 宏块 |
| P slice  | 包含 P 和 I 宏块 |
| B slice  | 包含 B 和 I 宏块 |
| SP slice  | 包含 P 或 I 宏块,用于不同码流之间的切换 |
| SI slice  | 一种特殊类型的编码宏块 |

#### slice 组成

每一个 slice 总体来看都由两部分组成，一部分作为 slice header，用于保存 slice 的总体信息（如当前 slice 的类型等），另一部分为 slice body，通常是一组连续的宏块结构（或者宏块跳过信息）

![](https://gitee.com/kevin1993175/image_resource/raw/master/slice.png)

### 宏块（Macroblock）

刚才在片中提到了宏块，那么什么是宏块呢？

宏块是视频信息的主要承载者。一个编码图像通常划分为多个宏块组成.包含着每一个像素的亮度和色度信息。视频解码最主要的工作则是提供高效的方式从码流中获得宏块中像素阵列。

一个宏块由一个 16×16 亮度像素和附加的一个 8×8 Cb 和一个 8×8 Cr 彩色像素块组成。

|  宏块分类   | 意义  |
|  ----  | ----  |
| I 宏块  | 利用从当前片中已解码的像素作为参考进行帧内预测 |
| P 宏块  | 利用前面已编码图像作为参考进行帧内预测 |
| B slice  | 利用双向的参考图像（当前和未来的已编码图像帧）进行帧内预测 |


### 帧（frame）和场（filed）

视频的一场和一帧用来产生一个编码图像，一帧通常是一个完整的图像，当采集视频信号时，如果采用隔行扫描（奇、偶数行），则扫描下来的一帧图像就被分成了两个部分，这每一部分都被称为 [场]，根据次序，分为 [顶场] 和 [底场]。

> 为什么会产生场的概念？      
> 人眼可察觉到的电视视频图像刷新中的闪烁为 0.02 秒，即当电视系统的帧率低于 50 帧/秒，人眼可感觉得出画面的闪烁。常规如 PAL 制式电视系统帧率为 25 帧/秒、NTSC 制式的则为 30 帧/秒，如果采用逐行扫描将不可避免地在视频刷新时产生闪烁现象。而另一方面如果单纯的提高帧率达到避免闪烁刷新效果，则会增加系统的频带宽度。

在隔行扫描中，每一帧包含两个场（top field）和（bottom field），其中每个 field 包含一帧中一半数量的水平线，top field 包含所有奇数线，bottom field 则包含所有偶数线。则在电视显示过程中，电子枪每发射一行隔一行—先发射奇数行13579…（top field）回头再发射2468…（bottom field）利用两次扫描来完成一幅图像，因为视觉的滞留性，我们看到的效果是差不多的。如在 NTSC 视频中 frame 的频率为30次/秒-àfield的频率则为 60 次/秒，大于了人眼可察觉闪烁的频率。


### 帧划分类型

在连续的几帧的图像中，一般只有 10% 以内的像素有差别，亮度的差值变化不超过 2%，色度的差值变化只在 1% 以内

于是可以将一串连续的相似的帧轨道一个图像群组，也就是 GOP（Group of Pictures）

GOP 中的帧可以分为 3 中类型

- I 帧，也就是 帧内编码图像，也叫关键帧，
  - 是视频的第一帧，也是 GOP 的第一帧，一个 GOP 只有一个帧
  - 自身可以通过视频解压算法解压成一张单独的完整的图片；

- P 帧，也就是 预测编码图像
  - 需要参考其前面的一个 I frame 或者 B frame 来生成一张完整的图片；

- B 帧：也就是 前后预测编码图像
  - 要参考其前一个 I 或者 P 帧 及其后面的一个 P 帧来生成一张完整的图片；

所以编码后的数据大小： I 帧 > P 帧 > B 帧

### GOP

GOP 是画面组，一个 GOP 是一组连续的画面。

GOP 一般有两个数字，如 M = 3，N = 12，M 制定 I 帧与 P 帧之间的距离，N 指定两个 I 帧之间的距离。那么现在的 GOP 结构是

```
I BBP BBP BBP BB I
```

增大图片组能有效的减少编码后的视频体积，但是也会降低视频质量

### IDR

一个序列的第一帧叫做 IDR帧（Instantaneous Decoding Refresh，立即解码刷新）。

I 帧和 IDR 帧都是使用帧内预测，本质上是同一个东西，在解码和编码中为了方便，将视频序列中第一个 I 帧和其他 I 帧区分开，所以把第一个 I 帧称作 IDR，这样就方便控制编码和解码流程。

IDR 帧的作用是立刻刷新，使错误不致传播，从 IDR 帧开始，重新算一个新的序列开始编码。

核心作用

H.264 引入 IDR 帧是为了解码的重同步，当解码器解码到 IDR 帧时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会，IDR 帧之后的帧永远不会使用 IDR 之前的图像的数据来解码。


## 播放器的基本原理

> https://mp.weixin.qq.com/s/vxrpMmf-UsVdGFvwWRLYKw

- **解协议**

在播放视频前，我们一般会拿到一个视频的播放地址，如果是本地视频，就是一个文件路径；如果是一个在线视频，那么可能有多种流媒体协议，常见的如HTTP、RTMP、HLS、DASH等。解协议的过程就是通过拿到的播放地址判断出当前视频的流媒体协议，然后用对应的协议去获取媒体文件数据。

FFmpeg中内置了常见的流媒体格式协议的解析，对于一个视频 `url http:www.qq.com/test.mp4`, 常见的解析的过程如下：

- 首先取出 url 中的协议头如 "http"

- 和初始化好的协议列表中的协议名进行对比，如果匹配上则使用该协议解析器；这里会匹配上 http 协议，*http 协议的 name 就是:http，实现在 http.c*

- 如果是一个本地文件的地址，会解析为 file 协议，*file 协议的 name 就是:file，实现在 file.c*

解析完成后就会使用对应协议实现的获取媒体数据的方式来读取媒体流。就比如说 File 协议的实现就是读取本地文件；Http 协议的实现就是通过 http 请求的方式向服务器请求数据

- **解封装**

视频有多种格式，如常见的有MP4、3GP、AVI、FLV、RMVB。一般来说视频的格式名就对应着他的封装协议名称。**封装协议的主要作用就是将已经编码好的视频数据和音频数据按照协议规则放在一个文件中**。

一个完整的视频文件中，除了有已经编码后的音视频信息外，一般还会有描述媒体数据的组织结构的信息。如MP4封装格式如下所示，就是一个一个的box及其嵌套，不同的box里面存储了不同的信息，MP4的所有信息都以box的方式进行组织，box可以相互嵌套。其中最重要的就是moov box和mdat box，在moov box中存储了描述音视频格式如视频宽高、分辨率、码率等相关的格式信息，也有如moov box其中嵌套的stbl包含了所有音视频sample的时间戳pts和在文件中的偏移位置offset的信息；而mdat box则完全是存储的压缩后的音视频数据。

解封装的过程就是通过 moov box中的媒体结构信息，从mdta box中分离出Audio Track和Video Track，再把一份一份的视频数据或音频数据取出来的过程。

- **解码**

目前视频常用的压缩格式为H264和H265，音频常用的压缩格式为AAC。解码的过程就是将这些按照压缩算法解码为可直接送给播放器播放的原始数据类型。通常视频是解码YUV或RGB格式，音频是解码为PCM格式。

使用FFmpeg自带的软解解码器大致的解码流程如下：

  - 通过 `find_probe_decoder` 找到合适的解码器，详细寻找过程和寻找 demuxer 类似：

  - 解码器也会在初始化的时候初始化好放在 codec_list 中；寻找的过程就是找到解码器的AVCodecID相等的即可；AVCodecID 存放在 track box 中，是在解析视频header的时候初始化的，如果该视频是HEVC编码格式的，找到的就是hevc的decoder；

  - 找到匹配的decoder为hevc，hevc格式的name就是:hevc,，实现在hevcdec.c；

  - 找到对应的decoder后，先通过hevc_decode_init初始化；

  - 解码的时候会单独在一个解码线程，通过读取解封装数据缓存区的数据来进行解码，然后将解码后的数据放入缓存池中。

- **音视频同步**

在视频数据解码完成后，不会立即渲染到View上，还需要通过音视频同步机制，等到合适的渲染时机。

音视频同步主要分为三种：

- 音频时钟为基准：以当前正在播放的音频时钟基准，比较视频和音频的pts差值，如果视频过慢，则通过丢帧的方式进行追赶；如果视频播放过快，则一直渲染当前帧，直到音频跟上；

- 视频时钟为基准：以当前正在播放的视频时钟为基准，比较视频和音频的pts差值，这里和音频时钟为基准不同的是，这里音频是通过重采样的方式适当缩减或添加audio sample来达到同步的目的。

- 以外部时钟为基准：音频和视频在输出时，都需要和外部时钟进行对比，然后音视频按照各自同步的方法进行同步(视频丢帧或等待、音频重采样)，外部时钟的更新依赖于最近同步过的音频时钟或视频时钟。

![](https://mmbiz.qpic.cn/mmbiz_png/csvJ6rH9Mcsic1Nv6MialKaMsbg9KNYCKykpiaqqf3zd7DVbbBBvZibhcaEBMGhYnavCKjHic97YRZRSasrdJxMZCmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 使用边缓存边播放对视频播放进行优化

一般情况下，播放音视频时，播放器数据的请求是由播放器内部发起的，我们只是提供了一个url，而不能控制数据的请求过程，都是要先进行下载，下载到一定量之后播放器再开始播放，当下载进度减去播放进度小于一定阀值，进入缓冲状态。一般最小缓存大小是 4M，最大 20M。

但是这样有两个弊端

- 卡顿恢复时长比较高，影响播放体验。
- 每次都要重新跟进 url 重新下载视频，造成了严重的流量浪费。

> 遇到这个问题之后，我去知乎上请教了一些人，他们建议我去参考一下 AndroidVideoCache 这个开源项目。然后实现机制就参考率这个开源项目

[参考](https://mp.weixin.qq.com/s/LWTAAXgjojoQF6vFio7MCg)

### 实现原理

加入本地代理，通过 Socket 的的方式，首先建立本地的 socketServer，监听local host和指定端口的请求，也就是 bind的时候指定让系统来分配一个可用的端口。每次数据的请求都发给 local host，socketSrever 监听到有 Socket 连接时，由 socketServer 来代理视频数据的请求，请求到的数据不返回给播放器，而是直接写入到文件缓存中,再从这个文件缓存中读取 buffer 数据给到播放器。

- 首先生成本地代理服务器  「在 AndroidVideoCache 中是通过构造器来实现」
  - 根据 host 生成本地代理服务器的地址
  - 然后创建ServerSocket，最大可于8个client进行连接
  - 接着由系统自动分配一个端口
  - 然后开启一个线程，在线程中轮训，检测是否有新的socket连接，并接收socket 连接

- 处理这个 socket 连接
  
- 然后就到 边缓存边播放 的实现

  - 把数据先以流的方式 写入到缓存，在通过socket的outStream给到播放器
  - 也就是每次从网络流中读取8192个字节，先写入到缓存文件，再从缓存文件中取出给到播放器
  - 在先往文件中写入数据时，直到写完（整个文件写完或者8192个写完）或者中断。

> 在请求远程url时将文件写到本地缓存中，然后从这个本地缓存中读数据，写入到客户端socket里面。服务器Socket主要还是一个代理的作用，从中间拦截掉网络请求，然后实现对socket的读取和写入。

> 不过这种方法在 Seek的场景 是有缺陷的。    
> https://mp.weixin.qq.com/s/kSBu_wY9pHJhg2euCuZWvg


## 优化卡顿带来的积累迟延

在网络状态良好的情况下，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

> 在拉流时，音频流、视频流是单独保存到缓冲队列的。如果发生网络抖动，就会引起缓冲抖动，可以总结为网络卡顿导致音视频缓冲队列增大，从而导致解码滞后、播放滞后。此时，我们需要主动丢包来跟进当前时间戳。因为音视频同步一般以音频时钟为基准，人们对音频更加敏感，所以我们优先丢掉视频队列的包。但是，丢视频数据包时，需要丢掉整个GOP的数据包，因为B帧、P帧依赖I帧来解码，否则会引起花屏


[参考](https://blog.csdn.net/u011686167/article/details/85256101?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)

## 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？

选择UDP协议，UDP实时性好。TCP要保证丢失的package会被再次重发，确保对方能够收到。 而在视频播放中，如果有一秒钟的信号确实，导致画面出现了一点瑕疵，那么最合适的办法是把这点瑕疵用随便哪些信号补充上，这样虽然画面有一点点瑕疵但是不影响观看。如果用的TCP的话，这点缺失的信号会被一遍又一遍的发送过来直到接收端确认收到。这不是音视频播放所期待的。而UDP就很适合这种情况。UDP不会一遍遍发送丢失的package。

## 音视频直播清晰度、画面质量，降低迟延(TCP改成UDP)

> 但是为了保证参数的可靠性，又实现了 UDP 的可靠传输

> https://zhuanlan.zhihu.com/p/82854047

有一个项目采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如果想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频推送到云服务端的流媒体，再进行大规模的分发和转发。但是我们摄像头以前只支持 TCP 长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将 TCP 推流改成 UDP 推流，

> https://www.sohu.com/a/305914050_134613

首先基本上直播的推流和拉流一般都是在基于 RTMP 协议的，在网络状态良好的情况下 RTMP 协议无论是推流还是拉流都是可以很好工作的，至少在网络不丢包的情况下是很流畅的，但是因为 RTMP 是基于 TCP 协议的，因为 TCP 协议需要通过 3次 握手建立链接，还需要 TLS 2 次握手，还用拥塞不可控等情况，这些情况都会影响音视频播放的流畅度，在弱网环境下延迟会增大。

因此我们选择了使用 UDP，因为 UDP 不需要建立连接，而且速度快，占用资源少。

> 摄像头，并发少，RTMP，TCP --> UDP QUIC协议

我当时采用的是 QUIC 协议 ，QUIC 就是快速 UDP 互联网连接，QUIC 位于应用层(比如 Http)和网络层(UDP)之间，是在 **UDP 上实现的一个多路复用的协议**。

QUIC 替换掉了 TCP 在应用层和传输层中间的角色

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/QUIC.6egow7866pw0.png)


## UDP 实现可靠传输

> [参考](https://www.toutiao.com/a6589194755962307080/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1534206661&app=news_article&utm_source=mobile_qq&iid=40708017633&utm_medium=toutiao_ios&group_id=6589194755962307080&wid=1638594135814)

> 因为 RTMP 底层是基于 TCP 的

> 在弱网环境下，需要做低延迟可靠通信，如果用 TCP 通信延迟可能会非常大，还有一种情况，有时候客户端数据上传需要突破本身 TCP 公平性的限制来达到高速低延时和稳定，也就是说要用特殊的流控算法来压住客户端上传带宽，这种情况就需要实现一种机制，实现不仅能压榨带宽，也能更好地增加通信的稳定性，避免类似 TCP 的频繁断开重连。     
> 当然还有避免 TCP 的三次握手和四次挥手的过程。

然后实现 UDP 的可靠最主要的就是：**重传**

我们都知道 IP 协议在设计的时候就不是为了数据可靠到达而设计的，所以 UDP 要保证可靠，就依赖于重传。

我们实现的重传是发送端通过接收端 ACK 的丢包信息反馈来进行数据重传，发送端会根据场景来设计自己的重传方式，设计的重传方式分为三类：定时重传、请求重传和 FEC 选择重传。

- 定时重传（固定时间）

就是发送端如果在发出数据包（T1）时刻经过一个 RTO（重传时间) 之后还未收到这个数据包的 ACK 消息，那么发送端就重传这个数据包。这种方式依赖于接收端的 ACK 和 RTO，但是这种重传机制容易产生误判，比如：

- 对方收到了数据包，但是 ACK 发送途中丢失。

- ACK 在途中，但是发送端的时间已经超过了一个 RTO。

所以超时重传的方式主要集中在 RTO 的计算上，如果应对的（比如书写同步）场景是一个对延迟敏感但对流量成本要求不高的场景，就可以将 RTO 的计算设计得比较小，这样能尽较大可能保证你的延时足够小。比如 125ms

- 请求重传

请求重传就是接收端在发送 ACK 的时候携带自己丢失报文的信息反馈，发送端接收到 ACK 信息时根据丢包反馈进行报文重传

这个反馈过程最关键的步骤就是回送 ACK 的时候应该携带哪些丢失报文的信息，因为 UDP 在网络传输过程中会乱序会抖动，接收端在通信的过程中要评估网络的 `jitter_time`（抖动时间），也就是 `rtt_var`（RTT 方差值），当发现丢包的时候记录一个时刻 t1，当 `t1 + rtt_var < curr_t`(当前时刻)，我们就认为它丢失了。

这个时候后续的 ACK 就需要携带这个丢包信息并更新丢包时刻 t2，后续持续扫描丢包队列，如果 `t2 + RTO < curr_t`，则再次在 ACK 携带这个丢包信息，以此类推，直到收到报文为止。

这种方式是由丢包请求引起的重发，如果网络很不好，接收端会不断发起重传请求，造成发送端不停的重传，引起网络风暴，通信质量会下降，所以我们还需要在发送端设计一个**拥塞控制模块**来限流，

> 整个请求重传机制依赖于 jitter time 和 RTO 这个两个时间参数，评估和调整这两个参数和对应的传输场景也息息相关。请求重传这种方式比定时重传方式的延迟会大，一般适合于带宽较大的传输场景，例如：视频、文件传输、数据同步等。

- FEC 选择重传

除了定时重传和请求重传模式以外，还有一种方式就是以 FEC 分组方式选择重传，这也是在设计UDP可靠中最重要的重传机制。FEC（Forward Error Correction）是一种前向纠错技术，一般通过 XOR 类似的算法来实现，也有多层的 EC 算法和 raptor 涌泉码技术，其实是一个解方程的过程。

在发送方发送报文的时候，会根据 FEC 方式把几个报文进行 FEC 分组，通过 XOR 的方式得到若干个冗余包，然后一起发往接收端，如果接收端发现丢包但能通过 FEC 分组算法还原，就不向发送端请求重传，如果分组内包是不能进行 FEC 恢复的，就向发送端请求原始的数据包

### 窗口与拥塞控制

> 因为我们主要指通过重传来解决可靠问题的，但是重传会引来两个问题，一个是延时，一个是重传的带宽，尤其是带宽，如果控制不好会引来网络风暴，所以在发送端会设计一个窗口拥塞机制了避免并发带宽占用过高的问题。

这里需要一个收发的滑动窗口系统来配合对应的拥塞算法做流量控制，如果涉及到可靠有序的 UDP 重传，接收端就要做窗口排序和缓冲，如果是无序可靠或者尽力可靠的场景，接收端一般就不做窗口缓冲，只做位置滑动。

> 画个图说说

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/滑动窗口.2uin6jnicyo0.png)

比如发送端从发送窗口中发了 6 个数据报文给接收端，接收端收到 101，102，103，106 时会先判断报文的连续性并滑动窗口开始位置到 103，接着每个包都回应 ACK，发送端在接收到 ACK 的时候，会确认报文的连续性（通过seq序号），并滑动窗口到 103，发送端会再判断窗口的空余，然后填补新的发送数据，这就是整个窗口滑动的流程。

在接收端收到 106 时的处理，如果是有序可靠，那么 106 不会通知上层业务进行处理，而是等待 104、105。如果是尽力可靠和无序可靠场景，会将 106 通知给上层业务先进行处理。在收到 ACK 后，发送端的窗口要滑动多少是由自己的拥塞机制定的，也就是说窗口的滑动速度受拥塞机制控制，拥塞控制实现要么基于丢包率来实现，要么基于双方的通信时延来实现，

**使用经典的拥塞控制算法**

- 慢启动（slow start）

当连接链路刚刚建立后，不可能一开始将 cwnd 设置得很大，这样容易造成大量重传，经典拥塞里面会在开始将 cwnd = 1，然后根据通信过程的丢包情况来逐步扩大 cwnd 来适应当前的网络状态，直到达到慢启动的门限阈值 (ssthresh)，步骤如下：

1、初始化设置 cwnd = 1，并开始传输数据。

2、收到回馈的 ACK，会将 cwnd 加 1。

3、当发送端一个 RTT 后且未发现有丢包重传，就会将 cwnd = cwnd * 2。

4、当 cwnd >= ssthresh 或发生丢包重传时慢启动结束，进入拥塞避免状态。

- 拥塞避免

当通信连接结束慢启动后，有可能还未到网络传输速度的上线，这个时候需要进一步通过一个缓慢的调节过程来进行适配。一般是一个 RTT 后如果未发现丢包，就将 cwnd = cwnd + 1。一但发现丢包和超时重传，就进入拥塞处理状态。

- 拥塞处理

拥塞处理在 TCP 里面实现很暴力，如果发生丢包重传，直接将 cwnd = cwnd / 2，然后进入快速恢复状态。

- 快速恢复

通过确认丢包只发生在窗口一个位置的包来确定是否进行快速恢复，如图 6 中描述，如果只是 104 发生了丢失，而 105 和 106 是收到了的，那么 ACK 总是会将 ACK 的 base = 103，如果连续 3 次收到 base 为 103 的 ACK，就进行快速恢复，也就是立即重传 104，而后如果收到新的 ACK 且 base > 103，将 cwnd = cwnd + 1，并进入拥塞避免状态。

### RTT 与 RTO 的计算

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/RTO计算.55cj031ga5s0.png)

RTT = T2 - T1

这个计算方式只是计算了某一个报文时刻的 RTT，但网络是会波动的，这难免会有噪声现象，所以在计算的过程中引入了加权平均收敛的方法(参考 RFC793)

`SRTT = (α * SRTT) + (1-α)RTT`

这样可以求得逼近的 SRTT，在公式中一般`α=0.8`，确定了 SRTT，下一步就是计算 RTT_VAR(方差)，我们设 `RTT_VAR = |SRTT – RTT|`，那么 `SRTT_VAR =(α * SRTT_VAR) + (1-α) RTT_VAR`，这样可以得到 RTT_VAR 的值。

但最终我们是需要 RTO，因为涉及到报文重传，RTO 就是一个报文的重传周期，从网络的通信流程我们很容易知道，重传一个包以后，如果一个 `RTT+RTT_VAR` 之后的时间还没收到确定，那我们就可以再次重传，则可知：`RTO = SRTT + SRTT_VAR`。

但一般网络在严重抖动的情况下还是会有较大的重复率问题，所以：`RTO = β*(SRTT + RTT_VAR)`，`1.2 <β<2.0`，当然可以根据不同的传输场景来选择 `β` 的值。

-----


## RTMP 协议

RTMP(Real Time Messaging Protocol) 实时消息传送协议，RTMP协议是应用层协议，是要靠底层可靠的传输层协议（通常是TCP）来保证信息传输的可靠性的。在基于传输层协议的链接建立完成后，RTMP 协议也要客户端和服务器通过“握手”来建立基于传输层链接之上的 RTMP Connection 链接。

### RTMP 的握手过程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/rtmp握手过程.1f66gb6erwtc.webp)

客户端被指定依次向服务器发送 C0,C1,C2 三个 chunk，服务器向客户端发送 S0,S1,S2 三个 chunk。 详细发送要求：

- 客户端开始发送 C0,C1；
- 客户端必须收到 S1 后，才发送C2;
- 客户端必须收到 S2 后才开始发送其他信息（控制信息和音视频数据） 服务器要等收到 C0 才能发送 S0 和 S1；
- 服务器必须等 C1 后才能发送 S2 服务器必须等收到 C2 之后才能发送其他数据（控制信息和音视频数据）

### 大概流程

- `client--> server` : 发送一个创建流的请求（C0、C1）。  

- `server--> client` : 返回一个流的索引号（S0、S1、S2）。 

> 对了，这里的 C0 和 S0 都是 rtmp 版本包，大小 1 字节, C0 表示客户端需求的 rtmp 版本，然后 S0 表示服务器选择的 rtmp 版本。

 - `client--> server` : 开始发送 （C2）

 - `client--> server` : 发送音视频数据（这些包用流的索引号来唯一标识）

> 然后，RTMP 的传输是通过 消息块 & 消息封包传输的，这里就要设计到 RTMP 消息块，消息体的组成那些了，这里我需要展开讲吗？     
> [参考](https://blog.csdn.net/chenxijie1985/article/details/118889103?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7EHighlightScore-5.queryctrv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7EHighlightScore-5.queryctrv2&utm_relevant_index=10)

### RTMP 的传输：消息块 & 消息封包传输

RTMP 协议为了维持稳定连续传递，避免单次传输数据量问题，采用了传输层封包，数据流切片的实现形式。被用来对当前带宽进行划分和复用的最小传输单位，被称为 Chunk 即**消息块**。通常情况下，一个有效的消息，如果数据量超出当前 Chunk Size 的话，则会被拆分成多个分块来分批传输。通过指定首个 Chunk 和后续 Chunk 类型，以及 Chunk Header 其他标志性数据，来使当前被切割的消息，能够在对端得到有效的还原和执行。我们以 MetaData 类型消息（Data AFM3 16）举例：

![](https://img-blog.csdnimg.cn/img_convert/93e346f7f17491650e1962b818ce2774.png)

例子 中用来作为演示的 MetaData 高达 400 Bytes（不会吧？不会吧？不会吧？），而我们用 RTMP 的默认 Chunk Size 为 128 bytes。因此，当我们使用 Message Type 为 16 的 Data AFM3 类型的数据消息，通知对端当前元数据信息的时候，就需要切片了。即上图 所示。

### RTMP 的传输：消息块的组成

想要了解 RTMP 则必须对其使用的网络传输数据封装格式有一定的了解。RTMP 协议是以分组形式传送数据包。一个完整的数据块包含两个部分：Chunk Header 和 Chunk Data，这两者组合在一起，构成了一个有效的消息类型，结构如下：

![](https://img-blog.csdnimg.cn/img_convert/cf0c57852af86d1d593c7e1c415f7f8d.png)

- 基础数据头（Basic Header）：保存 CS ID、Chunk Type（决定 Msg Header 类型）
- 消息数据头（Message Header）：包含被发送消息的相关信息，类型Chunk Type决定
- 扩展时间戳（Extended Timestamp）（32-bits）：消息头携带的时间戳扩展位

#### 基础数据头

基础数据头，Chunk stream ID 可以配置为3～65599 这 65597 个不同标志中的其中一种。根据持有 Chunk stream ID 的长度，RTMP 规格将基础数据头分为3种：ID 在 2～63 范围内的 1-Byte 版；ID 在 64～319 范围内的 2-Byte 版；ID 在 64～65599 范围内的 3-Byte 版。基础数据头组成，也包含三个部分。

![](https://img-blog.csdnimg.cn/img_convert/5af945aa09723b3ed89554df3ad9f5f4.png)

ormat message type 标志位 fmt（2-bits）：用来标志消息类型，也被称为 Chunk Type

cs id 字段（6-bits）：用来表示 63 以内的ID的标志位，0、1两个标记被占用做扩展标记cs id - 64字段（8 or 16-bits）：用来根据扩展标志扩充的，广范围标志位

需要注意的是，Chunk stream ID 是用来区分消息信道的。因为 RTMP 协议，所有的通信都是通过同一个 TCP 来完成的，因此所有类型的通信信道需要由 Chunk stream ID 来进行区分，从而判断当前收到的消息所属的信道类型。当然，这个是由用户定义的，不做区分其实也不影响实际操作（虽然 Adobe 官方有预留，但是规范约束是一种手段，自己做双端协定的时候也可以不按这种规范来，虽然那样就要做一系列的配套实现了），Adobe 建议及目前市面上大部分采用如下的分类以便于操作分割：

![](https://img-blog.csdnimg.cn/img_convert/e65f4031d9f3eb99015248a678717cad.png)

每个前后都有预留编号，以便扩充使用。

#### 消息数据头

消息数据头的类型，是由基础数据头中的 fmt 字段来标记的。总共分为4种类型

。。。。

----

## FFmpeg 数据结构 AVBuffer

[参考](https://www.cnblogs.com/leisure_chn/p/10399048.html)

AVBuffer 是 FFmpeg 中很常用的一种缓冲区，缓冲区使用引用计数(reference-counted)机制。

AVBufferRef 则对 AVBuffer 缓冲区提供了一层封装，最主要的是作引用计数处理，实现了一种安全机制。

用户不应直接访问 AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

**数据结构定义**

- struct AVBuffer

struct AVBuffer 定义于 “`libavutil/buffer_internal.h`”，`buffer_internal.h` 位于 FFmpeg 工程源码中，而 FFmpeg 提供的开发库头文件中并无此文件，因此这是一个内部数据结构，不向用户开放，用户不应直接访问AVBuffer，应通过 AVBufferRef 来访问 AVBuffer，以保证安全。

内部通过 `av_buffer_ref()` 来增加引用计数，通过 `av_buffer_unref()` 来减少引用计数

销毁一个 AVBufferRef 时，将其AVBuffer缓冲区引用计数减1，若缓冲区引用计数变为0，则将缓冲区也回收，



## AVFormatContext 和 AVInputFormat之间的关系

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/ffmpeg数据结构04.fnx3k76bak0.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.


## 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率 1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。

一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。

后来怀疑是 ABR 模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。 

后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！