- [<font color="orange">实习项目</font>](#font-colororange实习项目font)
  - [Hobot-SDK-<font color="orange">实习项目</font>](#hobot-sdk-font-colororange实习项目font)
  - [静态变量](#静态变量)
    - [怎么使用静态变量](#怎么使用静态变量)
    - [为什么使用宏函数](#为什么使用宏函数)
    - [介绍一下工厂模式](#介绍一下工厂模式)
      - [简单工厂模式](#简单工厂模式)
      - [工厂方法模式](#工厂方法模式)
      - [抽象工厂模式](#抽象工厂模式)
    - [detach 函数](#detach-函数)
      - [使用 detach 时候需要注意什么问题(线程安全问题)](#使用-detach-时候需要注意什么问题线程安全问题)
    - [怎么进行测试](#怎么进行测试)
    - [说一下`C++` 的智能指针](#说一下c-的智能指针)
      - [RAII 是什么](#raii-是什么)
  - [什么是 RTTI](#什么是-rtti)
    - [dynamic_cast 运算符](#dynamic_cast-运算符)
    - [typeid 运算符](#typeid-运算符)
- [<font color="orange">TCP通信框架</font>](#font-colororangetcp通信框架font)
  - [这个自己申请的域名吗](#这个自己申请的域名吗)
  - [设置进程名称](#设置进程名称)
    - [怎么修改进程名称](#怎么修改进程名称)
  - [日志打印实现](#日志打印实现)
    - [printf 的实现](#printf-的实现)
    - [日志等级划分](#日志等级划分)
    - [日志输出时遇到了问题 (行缓存造成日志输出混乱问题)](#日志输出时遇到了问题-行缓存造成日志输出混乱问题)
    - [日志写入混乱问题 和 掉电导致数据丢失问题](#日志写入混乱问题-和-掉电导致数据丢失问题)
    - [怎么解决掉电导致数据丢失问题](#怎么解决掉电导致数据丢失问题)
      - [fwrite 和 write 有什么区别](#fwrite-和-write-有什么区别)
      - [fwrite 实现原理](#fwrite-实现原理)
    - [说下你的日志系统的运行机制](#说下你的日志系统的运行机制)
    - [日志系统使用了单例模式](#日志系统使用了单例模式)
    - [日志系统采用异步，为什么要异步？和同步的区别是什么？](#日志系统采用异步为什么要异步和同步的区别是什么)
    - [缓冲区用什么实现](#缓冲区用什么实现)
      - [什么是生产者消费者模式](#什么是生产者消费者模式)
    - [你的项目中用到哪些设计模式](#你的项目中用到哪些设计模式)
    - [懒汉模式和饿汉模式具体怎么实现](#懒汉模式和饿汉模式具体怎么实现)
      - [单例模式会带来哪些问题](#单例模式会带来哪些问题)
  - [线程池相关](#线程池相关)
    - [线程的同步机制有哪些](#线程的同步机制有哪些)
    - [线程同步的方式](#线程同步的方式)
    - [死锁问题](#死锁问题)
    - [手写线程池](#手写线程池)
      - [线程池中的工作线程是一直等待吗？](#线程池中的工作线程是一直等待吗)
      - [你的线程池工作线程处理完一个任务后的状态是什么](#你的线程池工作线程处理完一个任务后的状态是什么)
      - [如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢](#如果同时1000个客户端进行访问请求线程数不多怎么能及时响应处理每一个呢)
      - [如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢](#如果一个客户请求需要占用线程很久的时间会不会影响接下来的客户请求呢有什么好的策略呢)
    - [多线程理解](#多线程理解)
    - [阻塞与非阻塞](#阻塞与非阻塞)
    - [异步和同步](#异步和同步)
    - [I/O 多路复用](#io-多路复用)
    - [怎么实现非阻塞 socket](#怎么实现非阻塞-socket)
  - [Epoll技术简介](#epoll技术简介)
    - [epoll_create 函数](#epoll_create-函数)
    - [epoll_ctl 函数](#epoll_ctl-函数)
    - [epoll_wait 函数](#epoll_wait-函数)
  - [向内核双链表增加节点](#向内核双链表增加节点)
  - [使用 epoll 函数来实现数据的收发](#使用-epoll-函数来实现数据的收发)
    - [创建连接池的目的](#创建连接池的目的)
    - [ET 和 LT 模式](#et-和-lt-模式)
      - [LT 模式-水平触发](#lt-模式-水平触发)
      - [ET 模式-边缘触发](#et-模式-边缘触发)
    - [事件驱动](#事件驱动)
    - [腾讯面试题](#腾讯面试题)
    - [深入理解ET LT](#深入理解et-lt)
    - [Epoll 中 ET 和 LT 模式的处理编码不同](#epoll-中-et-和-lt-模式的处理编码不同)
  - [什么是 Reactor 模式](#什么是-reactor-模式)
    - [常见的 Reactor 实现方案](#常见的-reactor-实现方案)
      - [第一种方案单 Reactor 单进程-线程](#第一种方案单-reactor-单进程-线程)
    - [什么是 Proactor](#什么是-proactor)
      - [理解 Reactor 和 Proactor 的区别](#理解-reactor-和-proactor-的区别)
    - [同步I/O模型的工作流程](#同步io模型的工作流程)
  - [定时器](#定时器)
    - [什么是定时事件](#什么是定时事件)
    - [什么是定时器](#什么是定时器)
      - [连接资源包括什么](#连接资源包括什么)
      - [超时时间](#超时时间)
    - [什么是定时器容器](#什么是定时器容器)
      - [什么是定时任务](#什么是定时任务)
      - [什么是定时任务处理函数？](#什么是定时任务处理函数)
    - [说一下定时器的工作原理](#说一下定时器的工作原理)
      - [双向链表删除和添加的时间复杂度还可以优化](#双向链表删除和添加的时间复杂度还可以优化)
      - [最小堆怎么优化](#最小堆怎么优化)
  - [压力测试](#压力测试)
    - [Webbench实现的核心原理](#webbench实现的核心原理)
  - [压力测试 Bug 排查](#压力测试-bug-排查)
    - [排查过程](#排查过程)
      - [listen](#listen)
      - [connect](#connect)
      - [accept](#accept)
      - [定位 accept](#定位-accept)
      - [Epoll的ET、LT](#epoll的etlt)
      - [代码分析解决](#代码分析解决)
      - [Bug原因](#bug原因)
  - [综合能力](#综合能力)
    - [使用 crc32 算法解决数据包收发过程中内容被篡改的问题](#使用-crc32-算法解决数据包收发过程中内容被篡改的问题)
    - [服务器突然运行很慢怎么处理](#服务器突然运行很慢怎么处理)
    - [你的项目相比于其他项目的优点](#你的项目相比于其他项目的优点)
- [<font color="orange">音视频项目</font>](#font-colororange音视频项目font)
  - [音视频直播清晰度、画面质量、流畅度如何保障，降低迟延](#音视频直播清晰度画面质量流畅度如何保障降低迟延)
  - [RTMP 协议与 HLS 协议](#rtmp-协议与-hls-协议)
  - [FFmpeg 将 MP4 文件切割成 HLS 格式](#ffmpeg-将-mp4-文件切割成-hls-格式)
  - [flv.js 和 video.js](#flvjs-和-videojs)
    - [`flv.js` 基本工作原理](#flvjs-基本工作原理)
  - [直播卡顿优化](#直播卡顿优化)
  - [谈谈FFmpeg的解码流程，说说你能够认识的函数作用](#谈谈ffmpeg的解码流程说说你能够认识的函数作用)
  - [音视频同步问题](#音视频同步问题)
  - [实现H.264的实时传输](#实现h264的实时传输)
    - [H.264的RTP报头](#h264的rtp报头)
  - [基于 VideoToolbox 来实现 H264 硬编码和硬解码](#基于-videotoolbox-来实现-h264-硬编码和硬解码)
    - [H264 硬编码](#h264-硬编码)
      - [遇到难题](#遇到难题)
    - [H264 硬解码](#h264-硬解码)
  - [ffmpeg 博客](#ffmpeg-博客)
    - [ffmpeg 源码中内存管理](#ffmpeg-源码中内存管理)
    - [AVFormatContext 和 AVInputFormat之间的关系](#avformatcontext-和-avinputformat之间的关系)

-------

# <font color="orange">实习项目</font>

## Hobot-SDK-<font color="orange">实习项目</font>

Hobot Framework 提供了一个通过有向图的形式, 将基础代码功能模块组织成较复杂功能模块的机制。

基于 Hobot Framework, 所有的基础的功能以 `Module` 的形式提供; 功能之间的数据交换以 `Message` 为单位。

一个 `Module` 由多个 `Forward{n}` 组成，其中 `Forward{n}` 又可以以一个或多个别的 `Module::Forward{n}` 产生的 `Message` 为输入, 可以输出自己的 `Message`。

这样, 基础功能的开发者可以将自己的基础功能封装成一个 包含多个 `Forward` 的 `Module`, 并说明自己的输入输出;

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/concept.2nnp25ds1lu0.png)

## 静态变量

- 函数内部定义一个局部对象时，使用 static 关键字时，这个变量就保存在静态存储区，在编译的时候初始化，如果不给初始化值，它的值就会被初始化为 0，并且，下次调用这个函数的时候该变量中保存的值就是上一次函数调用结束时的值

- 在全局变量时使用 static 关键字，那么这个全局变量只能在本文件中使用，无法在其他文件中被引用。

- 在函数之前加 static 时，那么函数只能在本源程序文件中调用，无法在其他源程序文件中调用。

- 在一个类中定义 static 成员，那么这个成员不属于某个对象，而是属于整个类。

> 编译阶段初始化，全局变量，函数，对象

### 怎么使用静态变量

在实现测试用例的自动保存的时候，会用一个宏函数 [`GTEST_TEST_CLASS_NAME_`]，这个宏函数里面有一个静态变量 test_info_ ，它利用”静态变量在程序运行前被初始化“的特性，抢在 main 函数执行之前，执行一段代码，从而有机会将测试用例放置于一个固定的位置。这个是”自动“保存测试用例的本质所在。

然后就是 test_info_ 的初始化的时候会插入一个模板类，这个模板类是一个 工厂类 ，类继承于 TestFactoryBase，并重载了 CreateTest 方法，主要的作用是 new 出一个 TestInfo 类对象，并实现一个 AddTestInfo 方法，将其测试用例保存起来。

在 AddTestInfo 中是通过 测试用例名 等信息获取测试用例，然后调用测试用例对象去新增一个测试特例—— test_info 

测试用例的保存使用一个 vetor 容器来保存

### 为什么使用宏函数

> 主要是 利用空间换时间的思想吧

普通函数和宏函数的区别就在于，宏函数占用了大量的空间，而函数占用了时间。大家都知道的是，函数调用是要使用系统的栈来保存数据的，*如果编译器里有栈检查选项，一般在函数的头会嵌入一些汇编语句对当前栈进行检查；同时，CPU 也要在函数调用时保存和恢复当前的现场，* 会涉及进行压栈和弹栈操作，所以，函数调用需要一些 CPU 时间。 而宏函数不存在这个问题。

宏在编译之前进行,也就是先用宏体替换宏名,然后再编译的,而函数显然是编译之后,在执行时,才调用的.因此,宏占用的是编译的时间,而函数占用的是执行时的时间.

宏函数仅仅作为预先写好的代码嵌入到当前程序，不会产生函数调用，所以仅仅是占用了空间，因为在进行单元测试的时候会频繁调用同一个宏函数，所以这样效率会高很多。

### 介绍一下工厂模式

> https://www.cnblogs.com/xiaolincoding/p/11524376.html

- 这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。
- 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，这样客户端就不需要关心对象是如何创建的，并且是通过使用一个共同的接口来指向新创建的对象。

简单来说，使用了 C++ 多态的特性，将存在继承关系的类，通过一个工厂类创建对应的子类（派生类）对象。在项目复杂的情况下，可以便于子类对象的创建。

如果我们直接 new 一个对象时，如果需要的对象构造方法比较复杂，那么可能需要一连串的代码去创建对象，如果在别的类中又需要创建该对象，那么代码的重复度肯定不小。通过工厂模式的话，我们把对象创建的具体逻辑给隐藏起来了，交给工厂统一管理，这样不仅减少了代码量，以后如果想改代码的话，只需要改一处即可，也方便我们日常的维护。

> 构造方法复杂，创建对象代码重复多。把对象创建的逻辑隐藏起来，交给工厂即可，减少代码量，改一处即可，利于维护。

工厂模式的实现方式可分别简单工厂模式、工厂方法模式、抽象工厂模式，每个实现方式都存在优和劣。


以鞋厂为例分析

#### 简单工厂模式

鞋厂可以指定生产耐克、阿迪达斯和李宁牌子的鞋子。哪个鞋炒的火爆，老板就生产哪个，看形势生产。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式01.5dzo9gqbwi00.png)

简单工厂模式的结构组成：

- 工厂类(ShoesFactory)：工厂模式的核心类，会定义一个用于创建指定的具体实例对象的接口。
- 抽象产品类(Shoes)：是具体产品类的继承的父类或实现的接口。
- 具体产品类(NiKeShoes\AdidasShoes\LiNingShoes)：工厂类所创建的对象就是此具体产品实例。

**简单工厂模式的特点**：

工厂类封装了创建具体产品对象的函数。

**简单工厂模式的缺陷**：

扩展性非常差，新增产品的时候，需要去修改工厂类。


#### 工厂方法模式

现各类鞋子抄的非常火热，于是为了大量生产每种类型的鞋子，则要针对不同品牌的鞋子开设独立的生产线，那么每个生产线就只能生产同类型品牌的鞋。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式02.md22sc5xhkw.png)

工厂方法模式的结构组成：
- 抽象工厂类（ShoesFactory）：工厂方法模式的核心类，提供创建具体产品的接口，由具体工厂类实现。
- 具体工厂类（NiKeProducer\AdidasProducer\LiNingProducer）：继承于抽象工厂，实现创建对应具体产品对象的方式。
- 抽象产品类（Shoes）：它是具体产品继承的父类（基类）。
- 具体产品类（NiKeShoes\AdidasShoes\LiNingShoes）：具体工厂所创建的对象，就是此类。

**工厂方法模式的特点**：

- 工厂方法模式抽象出了工厂类，提供创建具体产品的接口，交由子类去实现。
- 工厂方法模式的应用并不只是为了封装具体产品对象的创建，而是要把具体产品对象的创建放到具体工厂类实现。

**工厂方法模式的缺陷**：

- 每新增一个产品，就需要增加一个对应的产品的具体工厂类。相比简单工厂模式而言，工厂方法模式需要更多的类定义。
- 一条生产线只能一个产品。

#### 抽象工厂模式

鞋厂为了扩大了业务，不仅只生产鞋子，把运动品牌的衣服也一起生产了。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式03.4kdzxh4jx5e0.png)

抽象工厂模式的结构组成（和工厂方法模式一样）：
- 抽象工厂类厂（ShoesFactory）：工厂方法模式的核心类，提供创建具体产品的接口，由具体工厂类实现。
- 具体工厂类（NiKeProducer）：继承于抽象工厂，实现创建对应具体产品对象的方式。
- 抽象产品类（Shoes\Clothe）：它是具体产品继承的父类（基类）。
- 具体产品类（NiKeShoes\NiKeClothe）：具体工厂所创建的对象，就是此类。

**抽象工厂模式的特点**：

提供一个接口，可以创建多个产品族中的产品对象。如创建耐克工厂，则可以创建耐克鞋子产品、衣服产品、裤子产品等。

**抽象工厂模式的缺陷**：

同工厂方法模式一样，新增产品时，都需要增加一个对应的产品的具体工厂类。


> 在使用 detach 分离父进程和子进程，会导致线程安全问题，当时使用的是 临时对象的方式解决的。

> 以上代码实现见[这里](/算法/我要进大厂/10其他.md)

### detach 函数

如果创建了很多子线程，让主线程逐个等待子线程结束，这种方法就显得不是很好，所以需要引入 detach 这种写法，**让主线程和子线程分离**，主线程不必等待子进程运行结束。

```cpp
mytoobj.detach();
```
> 一旦调用了 detach，就不可再调用 join 了。当然可以使用 joinable 判断是否成功使用 join 或者 detach 。

但是使用 detach 时候会导致内存安全问题，就比如在线程中传递引用，指针之类的参数，这样的话形参的地址是原来主线程中分配的，一旦主线程退出后，子线程再使用这块内存肯定是不安全的。

所以需要使用值传递，在创建线程这一行就构造出 **临时对象**，主要用这个临时构造的对象传递给线程入口函数，那么线程中的到的第二个参数就一定能够在主线程执行完毕之前构造出来。从而确保 detach 线程是安全的。

#### 使用 detach 时候需要注意什么问题(线程安全问题)

- 不要往线程中传递引用，指针之类的参数
- 建议使用值传递，建议在创建线程这一行就构造出临时对象，然后线程入口函数的形参**使用引用**来作为形参。 


```cpp
//不能这样使用
void myprint(const int& i,const string& pmybuf) {...}

//main()中
std::thread mytojob(myprint,mvar,mybuf);
mytojob.detach();

//使用类
std::thread mytojob(myprint,mvar,A(myobj));
```

C++ 语言只会为 const 引用临时对象，第一个参数不建议使用引用，因为 主线程可能先执行结束被回收了，导致 mvar 变量被回收。

第二个参数系统内部隐式将 char 数组转成 string 对象，但是这个转换时机可能发生在 主线程 执行结束后，这时候 mybuf 被系统回收了。

更改：
```
std::thread mytojob(myprint,mvar,string(mybuf));
```

直接将 mybuf 转换成 string 对象，`string(mybuf))`会生成一个临时对象，并将这个临时对象绑定到  pmybuf ，因此可以保证 pmybuf 肯定是有效的。

> 给线程入口函数传递类型对象时，只要使用临时对象作为实参，就可以确保线程入口函数的形参在 main 主函数退出前就已经创建完毕，可以保证线程安全。


### 怎么进行测试

我们建立一个 `class A`，如果直接在在形参中写 `A(mysecondpar)` ;这个对象是由 主线程传入的时候构造出来的，那么就可以出现主线程先结束的时候，就不能再构造了。

那么在给线程入口函数传递类型对象形参时，传在主线程中构造一个临时对象`A(mysecondpar)` ，就可以保证线程入口函数的形参在 主线程 函数退出前就已经创建完完毕，可以安全使用。

```cpp
std::thread mytobj(myprint2,A(mysecondpar) );  //现在主线程构造出临时对象
```

###  说一下`C++` 的智能指针

 `C++`里面的四个智能指针，`auto_ptr`，`unique_ptr`，`shared_ptr`，`weak_ptr`，其中后三个是 c++11 支持，并且第一个已经被`c++11`弃用。

 使用原因：智能指针的作用是管理一个指针，因为在程序设计中动态分配的堆内存没有正确释放或无法释放，导致资源浪费，程序运行速度变慢等问题。使用智能指针可以很大程度上的避免这个问题，因为智能指针是一个类，当超出了类的实例对象的作用域时，会自动调用对象的析构函数，析构函数会自动释放资源。

 所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。**智能指针其实就是一个类模板**。

- <font color="#b5822d" size=5>auto_ptr</font>：采用所有权模式。`p2`剥夺了`p1`的所有权，但是当程序运行时访问`p1`将会报错。所以`auto_ptr`的缺点是：存在潜在的内存崩溃问题。

- <font color="#b5822d" size=5>unique_ptr</font>：是一种独占式智能指针，也就是同一时刻，只能有一个 unique_ptr 指针指向这个对象（这块内存）。在 C++14 的时候，unique_ptr 提供了 make_unique 函数。可以使用这个函数初始化性能更高。

- <font color="#b5822d" size=5>shared_ptr</font>：是一个共享指针，多个指针指向同一个对象（多个指针指向同一块内存），最后一个指针被销毁的时候，这个对象被释放，`shared_ptr`的工作机制是使用**引用计数**。            
可以使用 移动语义将对象变为空或者赋值，比如:

```cpp
shared_ptr<int> p1(new int(100)); //初始化：p1 指向这个对象（内存）
std::share_ptr<int> p2 = new int(100); //不行，因为智能指针是显式的类型转换 explicit ,不可以隐式类型转换

shared_ptr<int> p2(std::move(p1));  //移动语义，移动构造 p2，p1 不再指向这个对象而变成空

shared_ptr <int> p3;   
p3 = std::move(p2);     //移动赋值，p2 指向空，p3 指向这个对象，整个对象引用计数依旧为 1
```

<font color="orange" size=4 font-weight="bold">make_shared</font>

这是一个标准库里的函数模板，被认为最安全和更高效的分配和使用 shared_ptr 智能指针的一个 函数模板。

它能够在动态内存（堆）中分配并初始化一个对象，然后返回指向这个对象的 shared_ptr 。

```cpp
shared_ptr<int> p2 = std::make_shared<int>(100);  // 这个 shared_ptr 指向 100 的整数的内存，类似 int *p1 = new int(100);
shared_ptr<string> p3 = std::make_shared<string>(5,"a");  //类似 string p3(5,'a);

cout << *p2 << endl; //100
cout << *p3 << endl; //aaaaa
```


- <font color="#b5822d" size=5>weak_ptr</font>：也是一个类模板，这个指针指针指向一个由 shared_ptr 管理的对象，但是这种智能指针并不控制所指向的对象的生命周期，也就是说，将 weak_ptr 绑定到 shared_ptr 不会改变 shared_ptr 的引用计数。另外，weak_ptr 来直接访问对象，必须使用一个叫做 `lock` 的成员函数，`lock` 的功能就是检查 weak_ptr 所指向的对象是否还存在，如果存在，`lock` 就能返回一个共享对象的 shared_ptr 如果不存在，就返回一个空的 shared_ptr 。



#### RAII 是什么


RAII 全称是“Resource Acquisition is Initialization”，直译过来是“资源获取即初始化”，

RAII 依托「栈」和「析构函数」，来对所有的资源 —— 包括堆内存在内 —— 进行管理

也就是说在构造函数中申请分配资源，在析构函数中释放资源。


智能指针（`std::shared_ptr`和`std::unique_ptr`）即 RAII 最具代表的实现，使用智能指针，可以实现自动的内存管理，再也不需要担心忘记 delete 造成的内存泄漏。


C++ 支持将对象存储在栈上面。但是，在很多情况下，对象不能，或不应该，存储在栈上。比如：
- 对象很大；
- 对象的大小在编译时不能确定；
- 对象是函数的返回值，但由于特殊的原因，不应使用对象的值返回。

比如说在工厂方法或其他面向对象编程的情况下，返回值类型是基类（的指针或引用）。

我们怎样才能确保不会发生内存泄漏呢

答案就在析构函数和它的**栈展开**行为上。我们只需要把这个返回值放到一个本地变量里，并确保其析构函数会删除该对象即可。这就是 RAII 的基本用法。

> 在发生异常时对析构函数的调用，也叫 **栈展开**，


## 什么是 RTTI

父类指针可以指向 new 一个子类对象

就比如：

```cpp
Human * phuman = new Men;
```

这时候很难确定 phuman 指向哪个类，要向得到所指向的对象相关的类信息就比较困难，所以 RTTI 就是要来解决这类问题的，也就是获取 phuman 所指向的对象相关的类的信息。

RTTI 的目的就是 让程序运行时能根据 **基类**的 指针 或者 引用 来获取这个 指针 或者 引用 指向的对象的实际类型

RTTI 通过两个运算符来实现：

- dynameic_cast `[daɪˈnæmɪk]` 运算符：将父类的指针或者引用安全地转换为 子类的指针或者引用
- typeid 运算符：返回指针或者引用所指向对象的实际类型

### dynamic_cast 运算符

想区分一个指针是父类类型还是子类类型，使用 dynamic_cast 运算符就能够判断出来 —— 用 dynamic_cast 能转换成功，就说明这个指针实际上是要转换到的那个类型，所以说 dynamic_cast 是帮助开发者做安全检查的。

> 使用 dynamic_cast 的前提条件是 父类中必须至少有一个虚函数。否则就会编译出错，或者可能无法得到正确的运行结果

### typeid 运算符

通过这个运算符可以获取对象的信息，这个运算符返回一个常量对象的引用。

- 只要两个指针定义时的类型相同，不管它们指向的是父类还是子类，typeid 就相等。

```cpp
Human *phuman = new Men;
Human *phuman2 = new Women;
```

- 只要两个指针运行时指向的类型相同，typeid 就相等，不管它们定义的类型是否相同。

```cpp
Human *phuman = new Men;
Human *phuman2 = new Men;

Human *phman3 = phuman2;
if( (typeid(*phuman) == ) typeid(*phuman2) )  ???
```

----------

# <font color="orange">TCP通信框架</font>

## 这个自己申请的域名吗

没有申请，因为服务器是放在同一网段的虚拟机里，然后在本地的浏览器里面访问。

或者也可以在同一局域网的不同主机下实验，在同一局域网下通过私有 IP+端口号就可以访问。

又或者或者直接把服务器程序放在本地，然后使用本地回环地址 `127.0.0.1` 就可以。

本地回环地址主要作用有两个：
- 一是测试本机的网络配置，能 ping 通 `127.0.0.1`说明本机的网卡和  IP协议安装都没有问题；
- 另一个作用是某些 `server/client` 的应用程序在运行时需调用服务器上的资源，一般要指定 server 的 IP， server 的 IP 地址设为 `127.0.0.1` 同样也可以运行。

## 设置进程名称

> [进程名称 参数被覆盖问题](/Cpp项目/通讯实战项目/note?id=设置进程名称)

进程名称实际上是保存在 `argc[0]` 所指向的内存中。CMD 会把 argv 所指向的命令参数全部显示出来，因为我们设置的进程名称是 `./nginx` 是保存在 `argv[0]`中，所以 `argv[0]`改变，进程名也就改变了。

> 在这里遇到了个问题，一旦设置的进程名称的长度大于字符串 `./nginx`的长度，就可能导致设置的进程名称覆盖其他参数。

由于环境变量信息也是保存在内存中的，并且**保存的位置紧紧邻 argv 所指向的内存**。所以若果设置的进程名称太长，不但会覆盖掉命令行参数，而且很可能覆盖掉环境变量所指向的内容。

为此，借助了 nginx 的源码，想到了一个解决方案，就是将环境变量搬家，大致思路是：

- **重新分配一块内存**：足够容纳新的 environ 所指向的内容，把 environ 内容搬到这块内存中来。

- 将以往 `argv[0]` 指向的内容替换成实际要修改的新进程名称

### 怎么修改进程名称

大致逻辑：

- 计算进程名称的长度

- 计算命令行参数所占内存与环境变量所占内存的总和

- 设置新的进程名称

## 日志打印实现

> [日志打印实现](/Cpp项目/通讯实战项目/note?id=日志打印实现)

打印输出相关函数借鉴了 nginx 的实现，并做一些改动，这部分主要是 学习 `printf`,`vprintf` 这类函数的内部实现。

### printf 的实现

C 语言中`printf`函数的**参数是可变**的，是通过**栈**实现参数的传递。`printf`至少有一个参数，就是字符串指针，如果还有其他参数，比如

`printf("a=%d b=%d",i,j); `会**从右往左**依次把参数压入栈内，先压`j`然后压`i`，然后压这个串`"a=%d b=%d"`的首地址也压入栈。在进行解析的时候是后入先出，按照`%d`去寻找后面的参数。这样其实`printf`并不知道一共有几个参数,完全按照`%d`或者其他格式的类型去处理。

这就有个问题就是要特别注意变量定义的类型一定要与格式控制符表示的格式一致,不一致会出现读取错误.当然`printf`**在转换参数时，对栈只读不写**，不会造成栈错误。


### 日志等级划分

参考 nginx 的日志等级划分，nginx 日志分成 8 个等级，级别从高到低，数字最小的级别最高，数字最大的级别最低，nginx 中有专门的日志处理模块处理日志(*很复杂，没看*)

```
#define NGX_LOG_STDERR            0    //控制台错误【stderr】：最高级别日志，日志的内容不再写入log参数指定的文件，而是会直接将日志输出到标准错误设备比如控制台屏幕
#define NGX_LOG_EMERG             1    //紧急 【emerg】
#define NGX_LOG_ALERT             2    //警戒 【alert】
#define NGX_LOG_CRIT              3    //严重 【crit】
#define NGX_LOG_ERR               4    //错误 【error】：属于常用级别
#define NGX_LOG_WARN              5    //警告 【warn】：属于常用级别
#define NGX_LOG_NOTICE            6    //注意 【notice】
#define NGX_LOG_INFO              7    //信息 【info】
#define NGX_LOG_DEBUG             8    //调试 【debug】：最低级别
```

### 日志输出时遇到了问题 (行缓存造成日志输出混乱问题)

`printf` 函数不加“`\n`”无法及时输出，就是说，我们在实现`ngx_vslprintf`函数 [打印日志] 测试的时候，等待了好几秒，发现屏幕上迟迟没有日志输出的结果，然后突然之间，在屏幕上出现一大堆输出结果。

后来查了 `printf()`底层实现后发现，这是 **行缓存(输出缓冲区)的问题**，标准输入输出函数都是带有缓存的，一般是行缓存（还发现 window 系统上没有这个问题，但是 Unix 系统就有），就是把需要输出的数据先缓存到某个地方，等待 **行刷新标志** 或者 **缓存已满** 的情况下，才会把缓存的数据显示出来。

“`\n`” 可以认为是刷新标志，也可以通过调用 `fflush(stdout)` 函数刷新缓冲区，将结果显示出来。

### 日志写入混乱问题 和 掉电导致数据丢失问题

> 各个进程写日志，出现混乱，就将日志打印出来看看，发现日志一开始没打印，过一会才一下子出现很多，参考了 printf 实现解决了，然后发现日志直接输出没问题，但是写日志时候还是有问题

本项目是 1 个 master 进程， 4 个 worker 进程，假如 5 个进程间同时不停地调用，同时向日志文件中写日志，就会造成日志文件混乱问题

首先先在 master 进程和 worker 进程的功能函数中添加一条日志输出，返发现并没有出现混乱现象，所以初步确定 写日志代码在应对多个进程向同一个日志文件中写日志的时候是没有问题的。

当时还专门去研究了 write 函数内部是怎么解决这个问题的？然后参考这个方案在项目中实现。

> **write 内部是如何解决多进程同时写一个文件不出现混乱问题的呢**？

- 在日志初始化函数中调用了 open 函数，open 函数中的 O_APPEND 标志可以保证多个进程操作同一个文件的时候不会相互覆盖，如果不加这个标记，某些情况下就会出现数据彼此覆盖的问题。

- write 函数在写入文件的时候是原子操作，2 个进程同时写入是竞争关系，最终只会由某个进程写入数据。

- 我的实现是：父进程 fork 出子进程，在父进程都会执行的公共代码就已经调用了 open 函数打开了日志文件，然后才通过 fork 创建出子进程，这种父子进程之间会共享文件表项，文件表项里有当前文件偏移量，子进程用 write 原子操作写了一个日志，文件偏移量会移动到文件末尾，父进程的当前文件偏移量也会移动到文件末尾，因为是共享文件表项，所以父进程 write 是接着子进程写的内容末尾开始写，因此不会混乱。

> 所以本项目中利用这种机制解决了写日志时日志混乱的问题

### 怎么解决掉电导致数据丢失问题

> 这里就需要涉及到 write 内部的写数据过程

内核可以在任何时候写磁盘，但并不是所有的 write 操作都会导致内核的写操作，内核会把待写数据暂存在缓冲区，积累到一定数量后再一次性写入磁盘，如果出现意外，断电，计算机崩溃等，内核还没来得及把内核缓冲区的数据写入磁盘，这些数据就会丢失。

为了确保内核缓冲区中的数据被及时写入磁盘，内核缓冲区中设立了一个时间上限，达到时间上限后，内核会把所有内核缓冲区中的“脏数据”直接写到磁盘。

> 怎么解决掉电导致 write 写入的数据丢失


- 直接 `I/O`,直接访问物理磁盘，但是这样效率会降低。

本项目的 `ngx_log.cxx` 中的 `ngx_log_init` 函数

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT,0644);  
```

如果 open 参数增加 O_DIRECT 就会绕过缓冲区

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT|O_DIRECT,0644);  
```

- 设置 open 文件时的 O_SYNC 选项

O_SYNC 选项也叫**同步选项**，只针对 write 函数有效，使每次 write 操作等待物理 `I/O` 操作完成。也就是说将写入缓冲区的数据立即写入磁盘，而不用等到时间上限，这样将计算机崩溃或者断电时造成的数据丢失减到最小。

也是通过更改 open 第二个参数实现

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_SYNC,0644);  
```

但是直接向磁盘写数据的效率不高，因此磁盘是按 页 或者 扇区 来写数据的，而且还要进行磁盘寻道，也就是说要找到写的位置，这些都需要花时间。

所以使用 O_SYNC 标记写数据时要批量写，不要每次只写几个字节。

- 缓冲同步

这是最推荐的方法，项目中也是使用这种方法

这里涉及到 3 个函数：`sync`,`fsync`,`fdatasync`

(1) `sync(void)`: 将所有修改过的块缓冲区排入写队列，然后立即返回，不等待实际写磁盘操作。但是数据是否写入磁盘并没有保障

(2) `fsync(int fd)`: 将 fd 对应文件的缓冲区理解写入磁盘，并将等待实际写磁盘操作结束后返回。可用于数据库这样的应用程序，因为这种应用程序需要确保修改过的数据理解写到磁盘上。

(3) `fdatasync(int fd)`: 类似于 fsync，但只影响文件的数据部分。除数据外，fsync 还会同步更新文件属性（如文件大小，文件访问时间等。文件属性和文件内容是分开存储的，写磁盘会涉及 2 次寻道）。所以 `fdatasync`比 `fsync` 速度更快。

我们采取的方法是：

调用多次 write 函数，在调用 1 次 fsync 函数，因为频繁调用 fsync 函数效率会很低。

如果文件很大，就都写完，然后调用 1 次 fsync 函数

还有如果整个文件需要调用 write 函数 10 次才能写完，那么每写 1 次，就调用 fsync 函数 1 次意义就不大， 所以应该写 10 次后，再调用 fsync 函数 1 次。

> 本项目中写日志使用 write 系统调用，工作没有问题，当时还尝试了使用 fwrite 来写日志，就会出现日志混乱问题。

#### fwrite 和 write 有什么区别

read write 这类函数时属于 **系统调用**，

而 fwrite printf 属于标准 IO 库里面的函数，**内部实现有缓冲区的，此时写日志可能就要用到锁机制**。

#### fwrite 实现原理

当调用 fwrite 函数的时候，写入的内容会被放入一个系统的 CLib 缓冲区中（可以理解成 stdio 这个库里面提供的缓冲区）。当 CLib 缓冲区满之后，会将内容移至内核缓冲区。所以这里相当于在应用程序和内核之间加了一层。所以 IO 库函数相当于一层用于缓存，最终还是调用底层 IO，也就是系统调用来实现相关功能。

所谓的缓存就是内存，用于在输入输出设备和 CPU 之间临时保存数据，使低速输入输出设备和高速输入输出设备能够协调工作，避免低速的输入输出设备占用 CPU，解放出 CPU，使其能够高效工作。

> [参考](https://zhuanlan.zhihu.com/p/269247362)

### 说下你的日志系统的运行机制

步骤：

1：单例模式（局部静态变量懒汉方法）获取实例

2：主程序一开始`Log::get_instance()->init()`初始化实例。初始化后：服务器启动按当前时刻创建日志（前缀为时间，后缀为自定义`log`文件名，并记录创建日志的时间`day`和行数`count`）。如果是异步(通过是否设置队列大小判断是否异步，0为同步)，工作线程将要写的内容放进阻塞队列，还创建了写线程用于在阻塞队列里取出一个内容(指针)，写入日志。

3：其他功能模块调用`write_log()`函数写日志。（`write_log`：实现日志分级、分文件、按天分类，超行分类的格式化输出内容。）里面会根据异步、同步实现不同的写方式。

问题1.1：阻塞队列是什么？

本项目将 生产者-消费者 模型封装为阻塞队列，使用循环数组实现队列，作为两者共享的缓冲区。

### 日志系统使用了单例模式

[手写一个单例模式](/算法/我要进大厂/10其他?id=写一个单例模式)

单例模式的目的就是，用户在调用该类的时候，只能使用建立一个该类对象。于是呢就把该类的构造函数给私有化了，这样外部就根本没办法直接实例化调用该类，只能类内部调用。这时候就在类内部创建一个公有化的函数，然后让该函数返回一个该类的指针，这样外部就可以通过这个函数调用该类了。但是问题是，调用该成员函数必须实例化一个该类对象，而现在已经不能实例化该类对象了，所以为了可以成功调用该函数，把该函数设置为静态函数，静态函数的作用范围是全局整个文件，这样外部就可以调用了。

但是这样的话实际上并不能保证主函数调用时该类对象指针的唯一性，因为该静态成员函数每次返回的都是一个新的new出来的值。每次都不一样。于是办法就是设置一个私有化的静态对象指针，在外部初始化这个指针为空。在静态成员函数中，如果该静态指针为空就创建对象指针，否则直接返回对象指针。这样就确保了在外部使用时，该对象的唯一性。

但是问题又来了，这样做可能会导致内存泄漏，因为你静态成员函数申请的指针并没有释放，还需要用户手动释放。改动的话就把该静态成员函数中的创建指针改为创建一个静态对象成员，然后返回该成员的地址。

但是返回一个地址就需要用指针去接收，用户就有可能对该指针进行`delete`造成错误，所以直接静态成员函数返回一个引用更好。这样`delete`就会无效。

又但是引用之后，主函数可以通过赋值号产生新的类对象。突破了唯一性的设定。所以现在又需要对拷贝构造函数进行私有化设置。或者直接对拷贝构造`=delete`，进行禁用。又或者把默认的运算符重载给禁用了。

```cpp
#include<iostream>
#include<stdlib.h>
#include<algorithm>
#include<vector>
#include<string>
using namespace std;

class Singleton{
public:
	~Singleton(){
	printf("~Singleton() destruct");
	}
	static Singleton& CreateObject()
	{
		static Singleton obj;
		return obj;
	}
	
	//Singleton(Singleton& obj=delete;编译不能通过，但事实也可以
private:
	Singleton(){
	  printf("Singleton() Construct");
	}
	Singleton(Singleton& obj){
	  printf("Singleton(Singleton& obj) Construct");
	}
};
int main(){
	Singleton& p0bj1=Singleton::CreateObject();

	return 0;
}

```

### 日志系统采用异步，为什么要异步？和同步的区别是什么？

因为同步日志时，日志写入函数与工作线程串行执行，由于涉及到`I/O`操作，在单条日志比较大的时候，同步模式会阻塞整个处理流程，服务器所能处理的并发能力将有所下降，尤其是在峰值的时候，写日志可能成为系统的瓶颈。

而异步日志采用 生产者-消费者 模型，工作线程将所写的日志内容先存入缓冲区，写线程从缓冲区中取出内容，写入日志。并发能力比较高。

（工作线程就是生产者，写线程是消费者）

### 缓冲区用什么实现

本项目将 生产者-消费者 模型进行了封装，使用**循环数组实现队列**，作为两者共享的缓冲区。

#### 什么是生产者消费者模式

某个模块负责产生数据，这些数据由另一个模块来负责处理（此处的模块是广义的，可以是类、函数、线程、进程等）。产生数据的模块，就形象地称为生产者；而处理数据的模块，就称为消费者。

单单抽象出生产者和消费者，还够不上是 生产者／消费者 模式。该模式还需要有一个**缓冲区**处于生产者和消费者之间，作为一个中介。生产者把数据放入缓冲区，而消费者从缓冲区取出数据。大概的结构如下图。

![](https://pic2.zhimg.com/80/v2-d17e83ec46c199e61e1af7c50f6f5405_1440w.jpg)

同一个机器：使用观察者模式（有的叫发布订阅模式）

但是多机器，借助 redis 数据库的消息队列的发布订阅模式。实现分布式日志系统。

### 你的项目中用到哪些设计模式

单例模式

### 懒汉模式和饿汉模式具体怎么实现

看书的最后一页。

单例模式要两次加锁的原因是，如果针对多线程，两个线程同时判断出指针为空，就会同时创建两个单例的指针，判断一次之后进行加锁后再创建实例，就可以避免有另一个同时判断成功也迅速创建了实例，因为加锁后另一个进程不能再进入。

#### 单例模式会带来哪些问题

单例模式一般没有接口，**扩展困难**。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。

在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。

单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。


---------------------------------

## 线程池相关

### 线程的同步机制有哪些

### 线程同步的方式

实现线程间同步的方法：

**互斥量，自旋锁，读写锁，条件变量**

- **互斥量**：比如说有两个线程，线程 1 和线程 2，分别充当生产者与消费者的角色，那么这两个线程就很有可能同时去操作临界资源，如果同时去操作临界资源的话就会引起线程同步问题，互斥量的话就是来解决这个问题，当一个线程，比如说线程 1 在操作临界资源的时候，它就会阻止另外的线程去访问这个临界资源。其实引发线程同步问题的最根本原因是**这两个线程的指令是交叉执行的**，互斥量能够保证指令执行的原子性，也就是说先执行完线程 1 的指令再执行线程2的指令，或者先执行完线程 2 的指令再执行线程1的指令。保证他们之间不会出现交叉执行的情况。互斥量也称为**互斥锁**，它要么处于加锁状态要么处于解锁状态。保证资源访问的串行。操作系统提供的 API 是 `pthread_mutex_t`。

- **自旋锁**：其实自旋锁和互斥锁的原理是一样的，都是在使用临界资源之前加一个锁，阻止其他线程对它进行访问，完成之后再把锁给释放掉，保证临界资源的串行访问。但是它和互斥锁还是存在差别的，使用自旋锁的线程会一直循环反复检查锁的变量是否可用，因此**它不会让出CPU**，会处于忙等待的状态。其实自旋锁还是有很多好处的，它避免了进程或者线程上下文切换的开销，如果锁使用的时间不是很长的话，使用自旋锁的代价也是很小的，同时在操作系统内部很多地方使用的是自旋锁而不是互斥量的。这里还要提一点就是**自旋锁不适合在单核`CPU`中使用**。因为自旋锁在等待的时候并不会释放`CPU`，而是死循环地去等待。会引起其他的进程或者线程无法去执行。操作系统提供的API是`pthread_spinock_t`。

- **读写锁**: 读写锁和互斥锁还有自旋锁是类似的，但是做了一些改进，基于临界资源的考量，因为在开发环境中，临界资源很可能会出现多读少写的特性，就比如有一个数据库存储的是历史订单信息，而这些订单我们一般只是去查询很少去改变它，这个存储历史订单的数据库就属于多读少写的临界资源，如果在读写的时候也给它加锁，这样的话效率会很低的。读写锁的话是一种特殊的自旋锁，**它允许多个读者同时读取临界资源，但是不允许多个写操作同时访问这个资源**。在操作系统中提供的API是`thread_rwlock_t`，读锁是通过`thread_rwlock_rdlock`来加的，写锁是通过`thread_rwlock_wdlock`来加的.

- **条件变量**：条件变量是一种先对复杂的线程同步方法，它允许线程睡眠，在满足一定条件的时候再唤醒线程，就是当满足条件时，可以向这个线程发送信号，唤醒这个线程。因为在生产者和消费者模型中是存在问题的，举个例子，比如当缓冲区小于或者等于 0 时，这时候应该不允许消费者继续消费，消费者必须等待，当缓冲区满的时候，这个时候应该不允许生产者往里面生成数据了，生产者必须处于等待状态。条件变量呢就是对这个问题进行了约束，当缓冲区为 0 的时候，如果有生产者生产一个产品，那么就要唤醒可能等待的消费者；当缓冲区满的时候，如果有消费者消费了产品，就需要唤醒其他可能在等待的生产者。操作系统提供的`API`是`pthread_cont_t`来定义的,等待是通过`pthread_cont_wait`定义的，,唤醒是通过`pthread_cont_notify`定义的。

> 互斥量：保证只有一个进程去操作 临界资源，同步问题是因为两个指令交叉执行，所以需要保证原子性。   
> 自旋锁：加锁，保证串行执行，但是不会让出 CPU ，反复检查锁是是否可用，     
> 读写锁：考虑临界资源多读少写，允许多读不允许多写      
> 条件变量：适当的时候唤醒线程，缓冲区为 0 消费者等待，缓冲区满了，生产者等待

### 死锁问题

> http://c.zhizuobiao.com/c-18060400024/


### 手写线程池

C++实现的简易线程池，包含线程数量，启动标志位，线程列表以及条件变量。

其中**构造函数**主要是声明未启动和线程数量的。start 函数为启动线程池，将 num 个线程绑定 threadfunc 自定义函数并执行，加入线程列表。stop 是暂时停止线程，并由条件变量通知所有线程。**析构函数**是停止，阻塞所有线程并将其从线程列表剔除后删除，清空线程列表。

```cpp
#include<vector>
#include<string>
#include<list>
#include<thread>
#include<condition_variable>
using namespace std;
class ThreadPool {
public:
	ThreadPool(int threadnum):started(false),thread_num(threadnum) {

}
~ThreadPool()
{
	stop();
	for (int i = 0; i < thread_num; ++i) {
		threadlist[i]->join();
	}
	for (int i = 0; i < thread_num; ++i) {
		delete threadlist[i];
	}
	threadlist.clear();
}
void threadFunc(){}//线程执行函数,可自定义。
int getThreadNum() { return thread_num; }
void start() {
	if (thread_num > 0) {
		started = true;
		for (int i = 0; i < thread_num; ++i) {
			thread* pthread = new thread(&threadFunc, this);
			threadlist.push_back(pthread);
		}	
	}
}
void stop() {
	started = false;
	condition.notify_all();
}

private:
	int thread_num;
	bool started;
	vector<thread*> threadlist;
	condition_variable condition;
};

```



#### 线程池中的工作线程是一直等待吗？

线程池中的工作线程是处于一直阻塞等待的模式下的。因为在我们创建线程池之初时，我们通过循环调用 `pthread_create` 往线程池中创建了8个工作线程，工作线程处理函数接口为`pthread_create`函数原型中第三个参数函数指针所指向的worker函数（自定义的函数），然后调用线程池类成员函数run（自定义）。

-------这里可能会有疑问？为什么不直接将第三个参数直接指向 run 函数，而是要通过向worker中传入对象从而调用run呢？原因是因为我们已经将worker设置为静态成员函数，而我们都知道静态成员函数只能访问静态成员变量，所以为了能够访问到类内非静态成员变量，我们可以通过在worker中调用run这个非静态成员变量来达到这一要求。在run函数中，我们为了能够处理高并发的问题，将线程池中的工作线程都设置为阻塞等待在请求队列是否不为空的条件上，因此项目中线程池中的工作线程是处于一直阻塞等待的模式下的。

#### 你的线程池工作线程处理完一个任务后的状态是什么

这里要分两种情况考虑

（1） 当处理完任务后如果请求队列为空时，则这个线程重新回到阻塞等待的状态

（2） 当处理完任务后如果请求队列不为空时，那么这个线程将处于与其他线程竞争资源的状态，谁获得锁谁就获得了处理事件的资格。

#### 如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢

首先这种问法就相当于问服务器如何处理高并发的问题。

本项目中是通过对子线程循环调用来解决高并发的问题的。

**具体实现过程如下**：

我们在创建线程的同时时就调用`pthread_detach`将线程进行分离，这样就不用单独对工作线程进行回收，但是一般情况只要我们设置了分离属性，那么这个线程在处理完任务之后，也就是子线程结束后，资源会被自动回收。那这种情况下我们服务器基本就只能处理8个请求事件了（线程池里只有8个线程）。那怎么实现高并发的请求呢？可能会说让线程池里创建足够多的线程数，这当然是理想化的，现实中线程数量过大会导致更多的线程上下文切换，占用更多内存，这显然是不合理的。

> 接下来所叙述的就是本项目中用来处理高并发问题的方法了

我们知道调用了 pthread_detach 的线程只有等到他结束时系统才会回收他的资源，那么我们就可以从这里下手了。我们通过子线程的run调用函数进行 while 循环，让每一个线程池中的线程永远都不会终止，说白了就是让他处理完当前任务就去处理下一个，没有任务就一直阻塞在那里等待。这样就能达到服务器高并发的要求，同一时刻8个线程都在处理请求任务，处理完之后接着处理，直到请求队列为空表示任务全部处理完成。

#### 如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢

会影响接下来的客户请求，因为线程池内线程的数量时有限的，如果客户请求占用线程时间过久的话会影响到处理请求的效率，当请求处理过慢时会造成后续接受的请求只能在请求队列中等待被处理，从而影响接下来的客户请求。

**应对策略**：

我们可以为线程处理请求对象设置处理超时时间, 超过时间先发送信号告知线程处理超时，然后设定一个时间间隔再次检测，若此时这个请求还占用线程则直接将其断开连接。


### 多线程理解


如果要让服务器服务多个客户端，最简单的方式就是一个连接创建一个线程，但是这样不停的创建和销毁线程，会造成性能能开销和资源浪费。

我们可以使用 「资源复用」的方式解决这个问题，就是创建一个「线程池」，将连接分配给线程，然后一个线程可以处理多个连接的业务。

> 线程怎样才能高效地处理多个连接的业务?

当一个连接对应一个线程时，线程一般采用「read -> 业务处理 -> send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 read 操作上（ socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。

但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 read 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。

要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 read 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。

上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 read 去试探。

### 阻塞与非阻塞

- 阻塞 IO

阻塞：就是调用一个函数，该函数就卡在这里，整个程序流程不会往下走了（此时程序进入休眠状态）。这个函数等待一个时间发生，只有这个事件发生了，程序才会继续玩下走（也就是程序才会继续运行）

这种函数就是 **阻塞函数**，比如服务器使用 accept 函数，调用 accept 时，程序执行流程就卡在 accept 这里，等待客户端连接，只有客户端连接，三次握手成功，accept 才会返回。

- 非阻塞 IO

非阻塞 IO 和 阻塞 IO 是相对的，就比如说刚刚说到的 accept ，如果通过调用某个函数，把监听套接字设置成非阻塞，那么调用 accept 的时候，就算没有客户端连接，这个 accept 调用也不会卡住，会立即返回（当然返回时会有个错误码，我们可以根据这个错误码判断 accept 返回的原因），这样就能充分利用操作系统给进程分配的时间片来做别的事情，执行效率就更高了。

### 异步和同步

- 异步 IO

调用一个异步 IO 函数接收数据时，不管有没有数据，该函数都会立即返回。我们在调用异步 IO 函数时要指定一个接受数据的缓冲区，还要指定一个回调函数，其他的事情操作系统去做了，程序可以自由地干其他事情。


> **非阻塞 IO 和 异步 IO 的差别**？
> - 非阻塞 IO 要不停地调用 IO 函数检查数据是否到来，如果数据到来了，就卡在 IO 函数这里把数据从内核缓冲区复制到用户缓冲区，然后这个 IO 函数才能返回
> - 异步 IO 不需要不停地调用 IO 函数检查数据是否到来，只需要调用 1 次，然后就去做其他事情了，由内核检查数据的到来，内核负责把数据复制到指定缓冲区，整个过程进程并没有被卡住

- 同步 IO

调用一个同步 IO 函数接受数据时，在没有得到结果之前，这个调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。同步 IO 需要调用 2 个函数才能取到数据，它的优点就是得到了所谓的 IO 复用的能力。

> 调用 1 个函数就能判断一批 TCP 连接是否有数据到的能力，就是 IO 复用

###  I/O 多路复用

使用 I/O 多路复用，实现当连接上有数据的时候，线程才去发起读请求。

> select/poll/epoll 是如何获取网络事件的呢?

- 如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮训调用 read 操作来判断是否有数据。
- 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。


-----

> [参考](https://blog.csdn.net/qq_34827674/article/details/116175772?spm=1001.2014.3001.5501)



### 怎么实现非阻塞 socket

- 使用 `ioct1()`函数，第二个参数和第三个参数可以 设置 或 清除 非阻塞I/O标记：0：清除，1：设置
- 调用`fcntl()` 函数把套接口描述符设置成非阻塞



```cpp
//设置socket连接为非阻塞模式【这种函数的写法很固定】
bool CSocekt::setnonblocking(int sockfd) 
{    
    int nb=1; //0：清除，1：设置  
    if(ioctl(sockfd, FIONBIO, &nb) == -1) //FIONBIO：设置 或 清除 非阻塞I/O标记：0：清除，1：设置
    {
        return false;
    }
    return true;

    //如下也是一种写法，跟上边这种写法其实是一样的，但上边的写法更简单
    /* 
    //fcntl:file control【文件控制】相关函数，执行各种描述符控制操作
    //参数1：所要设置的描述符，这里是套接字【也是描述符的一种】
    int opts = fcntl(sockfd, F_GETFL);  //用F_GETFL先获取描述符的一些标志信息
    if(opts < 0) 
    {
        ngx_log_stderr(errno,"CSocekt::setnonblocking()中fcntl(F_GETFL)失败.");
        return false;
    }
    opts |= O_NONBLOCK; //把非阻塞标记加到原来的标记上，标记这是个非阻塞套接字【如何关闭非阻塞呢？opts &= ~O_NONBLOCK,然后再F_SETFL一下即可】
    if(fcntl(sockfd, F_SETFL, opts) < 0) 
    {
        ngx_log_stderr(errno,"CSocekt::setnonblocking()中fcntl(F_SETFL)失败.");
        return false;
    }
    return true;
    */
}
```

## Epoll技术简介

[网络编程](/寻offer总结/计算机网络/网络编程相关)

以了一位网友【[王博靖](https://github.com/wangbojing/NtyTcp)】自己写的一套 Epoll 源码为入口，通过阅读源码学习了 Epoll 函数内部的实现原理。


**Epoll 就是一种在 Linux 上使用的 IO 多路复用并支持高并发的典型技术**。

比如说有 10 万个并发连接（也就是同一时刻有 10 万个客户端保持和服务器的连接），这 10 万个连接通常也不可能同一时刻都在收发数据，一般在**同一时刻通常只有其中几十个或者几百个连接在收发数据，其他连接可能处于只连接而没有收发数据的状态**。

如果以 100ms 为间隔判断一次，可能这 100ms 内只有 100 个活跃连接（就是有数据收发的连接），把这 100 个活跃连接的数据放在一个专门的地方，后续到这个专门的地方来，只需要处理 100 条数据，处理起来的压力就没那么大了。

这也就是 Epoll 的处理方式。而 select 和 poll 是依次判断这 10w 个连接有没有收发数据（可能实际上有数据的只有 100 个连接），有数据就处理。所以不难看出每次检查 10w 个连接与每次检查 100 个连接相比，浪费了巨大的资源和时间。

> 实际上，`epoll` 在内核里使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的 `socket` 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 `select/poll` 每次操作时都传入整个 `socket` 集合，只需要传入一个待检测的 `socket` 就可以了，减少了内核和用户空间大量的数据拷贝和内存分配。


此外 Epoll 采用了 **事件驱动机制**，只在单独的进程或者线程里收集和处理各种事件，没有进程或线程之间上下文切换的开销。

> 也就是说，在内核中维护了一个「链表」来记录就绪事件，当某个 `socket` 有事件发生时候，通过回调函数，内核会将这个 事件 加入到 就绪事件 列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件的 socket 文件描述符，不需要像 `select/poll` 那样轮询扫描整个` socket` 集合，大大提高了检测的效率。

`epoll` 通过两个方面，很好解决了 `select/poll` 的问题。

从下图你可以看到 `epoll` 相关的接口作用：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epoll01.58ud4l3nxm00.png)


源码中 `nty_epoll_rb.c` 和 `nty_epoll_inner.h` 这 2 个文件是 Epoll 相关的 3 个函数的实现文件。

### epoll_create 函数

- **格式**

```c
int epoll_create(int size);  // size 必须 > 0
```

创建一个 `eventpoll` 结构如下。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create01.ik3e4xapf3k.png)

`eventpoll` 的结构中有两个比较重要的成员

(1) `rbr`,可以理解成代表一颗红黑树的根节点（的指针）。

红黑树是一种高效的数据结构，用于保存数据，一般都是存“键值对（`key-value`）”，红黑树的特点是能够快速地根据给的 key 找到并取出 value ，这里的 key 一般是一个数字，而 value 代表的可能是一批数据。**红黑树查找的时间复杂度**是：`O(logn)`

一开始的时候红黑树还是空的，也就是 rbr 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create02.5zdre53dgpo0.png)

(2) `rdlist`，可以理解成代表一个双向链表的表头指针

双向链表能快速顺序地访问里面的节点。

一开始的时候双向链表也是空的，`rdlist` 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create03.5zueg4dj5yg0.png)

- **总结**：
  - 创建一个 eventpoll 结构的对象，被系统保存起来
  - 对象中的 rbr 成员被初始化成指向一颗红黑树的根节点，
  - 对象中的 rdlist 成员被初始化成指向一个双向链表的头结点。

### epoll_ctl 函数

- **格式**：

```c
int epoll_ctl (int efpd,int op,int sockid,struct epoll_event *event);
```

- **功能**：

我们可以通过 `epoll_ctl` 函数把程序中需要关注的事件添加到 epoll 对象描述符中，当有数据来往时，系统会通知程序。

**epoll_ctl 函数中参数的介绍**：

- `efpd`：`epoll_create()`返回的`epoll`对象描述符
- `op`：一个操作类型，添加/删除/修改 ，对应数字是`1,2,3`. 分别对应： `EPOLL_CTL_ADD`（添加事件）, `EPOLL_CTL_DEL`（删除事件）， `,EPOLL_CTL_MOD`（修改事件）
- `sockid`：表示一个 TCP 连接，添加事件（也就是往红黑树中添加节点）时，就是用 sockid 作为 key 往红黑树中增加节点的。
- `event`: 向 `epoll_ctl` 函数传递信息，比如要增加一些书剑，就可以通过 `event` 参数将具体事件传递进 `epoll_ctl` 函数。

- **原理**：

假如传递进来的是一个 `EPOLL_CTL_MOD` ,首先使用 `RB_FIND` 来查找红黑树上是否已经有了这个节点，如果有了，程序就直接返回，如果没有，程序流程就继续往下走。

>  **EPOLL_CTL_ADD 怎么往红黑树你增加节点**

**确定红黑树没有该节点**的情况下，会生成一个 epitem 对象。

通过执行下面代码创建 `epitem` 对象

```cpp
epi = (struct epitem*)calloc(1, sizeof(struct epitem));
```

这个对象就是后续增加到红黑树中的一个节点，该节点的 key 保存在 sockfd 中，要增加的事件保存在 event 中，然后使用 `RB_INSERT` 宏将该节点插入红黑树中，对于红黑树来说,每个节点都要记录自己的左子树、右子树和父节点，这里是通过 rbn 成员，指向父节点和子节点的。如果将来多个用户连入服务器，需要向这颗红黑树加入很多节点，这些节点彼此也要连接起来。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_ctl01.7d2r4etg4d40.png)

> EPOLL_CTL_ADD：等价于往红黑树中增加节点

> EPOLL_CTL_DEL：等价于从红黑树中删除节点

> EPOLL_CTL_MOD：等价于修改已有的红黑树的节点

**每一个连入客户端都应该调用 `epoll_ctl` 向红黑树增加一个红黑树节点**，如果有 100w 个并发连接，红黑树上就会有个 100w 个节点

### epoll_wait 函数

- **格式**：

```c
int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout);
```

- **功能**：

阻塞一小段时间并等待事件发生，返回事件集合，也就是获取内核的事件通知；

其实就是遍历这个双向链表，把这个双向链表里边的节点数据拷贝出去，拷贝完毕的就从双向链表里移除；因为所有数据的 socket（ TCP 连接）都在双链表里记着。

- 参数`epfd`：是`epoll_create()`返回的`epoll`对象描述符

- 参数`events`：是内存，也是数组，长度 是`maxevents`，表示此次`epoll_wait`调用可以收集到的`maxevents`个已经就绪【已经准备好的】的读写事件；换句话说返回的是有事件发生的 TCP 连接数目

- 参数`timeout`：阻塞等待的时长；

> 总的来说，epoll_wait 函数就是到双链表中去，把此刻同时连入的连接中有事件发生的连接拿出来，后续 read，write，或者 send，secv 之类的函数调用收到数据，某个 socket 只要在双链表中，这个 socket 上一定发生了 某个/某些 事件，也就是说，只有发生了某个/某些 事件的 socket 才会在双向链表中实现。

> 这也就是 epoll 高效的原因，因为 epoll 每次值遍历发生事件的一小部分 socket 连接（这些 socket 都在这个双向链表中），而不用到全部 socket 连接中逐个遍历以判断事件是否到来。


epitem 结构既能作为红黑树节点插入红黑树中，又能作为双向链表的节点插入双向链表中，不需要再创建红黑树节点时设计一个结构，创建双向链表接地那时再设计一个结构。

如果把 epitem 节点同时插入双向链表中，加入有 3 个TCP连接都收到了事件，那么这 3 个 TCP连接肯定都呆在双向链表里了。

## 向内核双链表增加节点

epoll_wait 函数实际上就是去双向链表，那么，**操作系统什么时候向双向链表中插入节点呢**？

- 客户端完成三次握手时，操作系统会向双向链表插入节点，这时服务器往往要调用 accept 函数把该连接从已完成连接队列中取走

- 当客户端发送来数据时，操作系统会向双向链表插入节点，这时服务器也要调用 close 关闭对应的 socket

- 当客户端发送数据时，操作系统会向双向链表插入节点，这时服务器要调用 read 或者 recv 来收数据

- 当可以发送数据时，操作系统会向双向链表插入节点，这时服务器可以调用 send 或者 write 向客户端发送数据。可以这样理解：如果客户端接收话剧慢，服务器发送数据快，那么服务器就得等客户端收完一批数据后才能再发下一批。

## 使用 epoll 函数来实现数据的收发

nginx 源码中的 `ngx_c_socket.cc` 中的 `CSocekt::ngx_epoll_init()`

对 epoll 功能初始化，在子进程中进行 ，这个函数被`ngx_worker_process_init()`所调用.

**实现逻辑**

- 首先调用 epoll_create 函数创建一个 epoll 对象，也就是创建了一个红黑树，还创建了一个双向链表，直接以 epoll 连接的最大项数为参数

```cpp
m_epollhandle = epoll_create(m_worker_connections); 
```

- 接着创建一个连接池

```cpp
 m_pconnections = new ngx_connection_t[m_connection_n]  //m_connection_n 连接池的大小
```
### 创建连接池的目的

目前项目有 2 个监听套接字，以后客户端连入后，每个用户还会产生 1 个套接字，套接字本身只是一个数字，但往往需要保存很多与这个数字相关的信息，这就需要把套接字数字本身与一块内存捆绑起来。所以，引入连接池的目的就是把套接字与连接池中的某个元素捆绑起来，将来就可以通过套接字取得连接池中的元素（内存），一遍读写其中的数据。

### ET 和 LT 模式

#### LT 模式-水平触发

epoll 默认采用的是 LT 模式，只有使用 EPOLLET 参数才会使用 ET（边缘触发）

发生一个事件，如果程序不处理，那么这个事件就一直被触发，具体地说，就是一个新用户连入后，如果程序不调用 accept4 或者 accept 函数将这个用户接入（从已完成连接队列中取出来），使用 epoll_wait 函数获取事件时，就每次都能获取到用户连入的事件通知，也就是 EPOLLIN 事件，显然，这种触发方式效率不高。

#### ET 模式-边缘触发

这种触发只是对非阻塞 socket 有用，因为项目中用的都是非阻塞 socket，所以可以使用边缘触发模式，发生一个事件，内核只会通知程序 1 次，如果一个新用户连入，内核通知程序 1 次，程序必须使用 accept4 或者 accept 将这个新用户接入，如果这次没接进来就麻烦了，因为内核不会再次通知程序。因为边缘触发这种模式减少了通知的次数，所以效率更高。

目前的代码中，这几个监听套接字，在调用 epoll_ctl 增加事件的时候用的都是默认的 LT 模式，这样就能保证不丢失客户端的连接，因为内核会反复通知程序。

对于接入的 socket 连接（accept4 或者 accept 返回的 scoket 连接），程序中用了 ET 模式，从而提高程序工作效率。

### 事件驱动

事件驱动架构，就比如说客户端连入，三次握手完成，只要服务器注册了获取读事件，内核就会通知服务器，这就产生了一个事件，这里的事件发生源是客户端，通过事件收集器来收集和分发事件（这里的事件收集就是 epoll_wait 函数）。然后比如`CSocket::ngx_event_accept`这些函数就是事件处理函数，服务器准备用这些函数来处理或者消费事件。

> 注意：每个事件消费者（处理函数）都不能有阻塞行为，否则整个执行通道就会堵塞了。

### 腾讯面试题

> 使用 linux epoll 模型，水平触发模式，当 socket 可写时，会不停地触发 socket 可写事件，如何处理？

- 第一种方式

需要向 socket 写数据的时候才把 socket 可写事件通知加入 epoll 的红黑树节点，等待可写事件。当程序接受到来自系统的可写事件通知后，调用 write 或者 send 发送数据。所有数据都发送完毕后，把 socket 可写事件通知从 epoll 的红黑树节点中移除（移除的是可写事件通知，而不是红黑树节点）

这种方式的缺点：即使发送很少的数据，也要把 socket 可写事件通知加入 epoll 红黑树节点，写完后再把可写事件通知从 epoll 红黑树节点中删除，有一点的操作代价。

- 第二种方式

开始不把 socket 可写通知事件加入 epoll 的红黑树节点，需要发送数据时，直接调用 write 或者 send 发送，如果 write 或者 send 返回 EAGIN（缓冲区满了，需要等待可写事件才能继续往发送缓冲区写数据），再把 socket 的写事件通知加入 epoll 的红黑树节点。这就变成了在 epoll 的驱动下发送数据，全部数据发送完毕后，再把可写事件通知从 epoll 红黑树节点中删除

这种方式的优点是：数据不多的时候避免 epoll 的红黑树节点中针对写事件通知的增删，提高了程序执行效率。

### 深入理解ET LT

- LT 是水平触发，属于低速模式，如果事件还没处理完，就会被一直触发
- ET 是边缘触发，属于高速模式，这个事件的通知只会出现一次

### Epoll 中 ET 和 LT 模式的处理编码不同

如果发送来了数据，一个读事件就会被内核放到双向链表，如果我们不使用 recv 来接受数据或者只使用 recv 接受了部分数据，也就是说 TCP 连接的接受缓冲区中还有数据没有接受完

在 LT 模式下，内核就不会把这个读事件的节点从双向链表中删除，这样每次程序调用 epoll_wait 都能获取通知。

ET 模式不一样，不管我们是否调用 recv 来接受数据，一旦从双向链表中把读事件对应的节点取走，内核肯定把这个节点从双向链表中删除了，所以下次用 epoll_wait 去取事件时取不到的，除非后续客户端又发来了数据，内核会再次向这个双向链表中添加一个读事件的节点，程序使用 epoll_wait 才能再次收到读事件。


一般来讲，本项目的服务器程序，如果收发的数据包后固定格式，都建议采用 LT 模式--编程简单，清晰，写好了效率上估计也不会很差。

如果收发数据包没有固定格式，可以考虑采用 ET 模式，反复收数据，收完为止，编程难度较大，但是效率会高一些。再浏览器反问一个 web 服务器页面时，发送的数据就可能没有固定格式，浏览器一次可能向 web 服务器发送一大批数据，然后等 web 服务器回应。所以 nginx 采用的是 ET 模式。





## 什么是 Reactor 模式

> [参考](https://blog.csdn.net/qq_34827674/article/details/116175772)

如果说让服务器服务多个客户端，那么最直接的方式就是一个连接创建一个线程，处理完成后关闭连接，线程销毁，但是这样会带来很大的性能开销，也会浪费资源，当然我们也可以使用「资源复用」，也就是创建一个线程池，这样就不用每个连接都分配一个线程，可以创建一个线程多个连接复用。

但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 read 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。

要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 read 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。

所以采用 I/O 多路复用，只有当连接上有数据的时候，线程才去发起读请求

> `select/poll/epoll` 是如何获取网络事件的呢？

在获取事件时，先把我们要关心的连接传给内核，再由内核检测：

- 如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮训调用 read 操作来判断是否有数据。
- 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。

**Reactor 模式** 就是基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写。

Reactor 模式主要由 **Reactor** 和**处理资源池**这两个核心部分组成，它俩负责的事情如下：
- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send

Reactor 模式是灵活多变的，可以应对不同的业务场景，

一般重用的 Reactor 方案有：

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多线程 / 进程；
- 多 Reactor 多进程 / 线程

> Nginx 使用的是进程

### 常见的 Reactor 实现方案

#### 第一种方案单 Reactor 单进程-线程

进程里有 Reactor、Acceptor、Handler 这三个对象：

- Reactor 对象的作用是监听和分发事件；
- Acceptor 对象的作用是获取连接；
- Handler 对象的作用是处理业务；

![](https://img-blog.csdnimg.cn/img_convert/3b2723e04cf760ee137d87bf8e99d471.png)

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。






> Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。

### 什么是 Proactor

前面提到的 Reactor 是非阻塞同步网络模式，而 Proactor 是异步网络模式

Proactor 采用了异步 I/O 技术，所以被称为异步网络模型。

#### 理解 Reactor 和 Proactor 的区别

- Reactor 是非阻塞**同步网络**模式，感知的是就绪可读写事件。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- Proactor 是**异步网络**模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 `read/write` 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。
- 因此，Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。

无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件。

### 同步I/O模型的工作流程

同步I/O模型的工作流程如下（`epoll_wait`为例）：

主线程往`epoll`内核事件表注册`socket`上的读就绪事件。

主线程调用`epoll_wait`等待`socket`上有数据可读

当`socket`上有数据可读，`epoll_wait`通知主线程,主线程从`socket`循环读取数据，直到没有更多数据可读，然后将读取到的数据封装成一个请求对象并插入请求队列。

睡眠在请求队列上某个工作线程被唤醒，它获得请求对象并处理客户请求，然后往`epoll`内核事件表中注册该`socke`t上的写就绪事件

主线程调用`epoll_wait`等待`socket`可写。

当`socket`上有数据可写，`epoll_wait`通知主线程。主线程往`socket`上写入服务器处理客户请求的结果。

读就绪事件：当有事件到来，epoll_wait()单纯通知主线程有事件来了，主线程把事件放入请求队列。应用程序利用工作线程通过read（）等函数把数据从内核缓冲区读到用户缓冲区。

读完成事件：有事件来了，主线程往内核注册这个读时间（就是告诉内核注意了一会要读数据）。注册了之后，主线程就去干其他事情，内核就自动会负责将数据从内核缓冲区放到用户缓冲区。不用用户程序管。

而对于用reactor模式模拟的的proactor模式来说，之前proactor是用主线程调用aio_read函数向内核注册读事件，这里它主线程使用epoll向内核注册读事件。但是这里内核不会负责将数据从内核读到用户缓冲区，最后还是要靠主线程也就是用户程序`read()`函数等负责将内核数据循环读到用户缓冲区。对于工作线程来说，收到的都是已读完成的数据，模拟就体现在这里。

有人可能会问，他们都是通过主线程调用不同函数进行注册，然后一个注册之后可以直接内核负责数据从内核到用户。另一个注册之后好像没啥用，那注册还有什么用？直接主线程循环读取然后封装放请求队列不就行了么？

不对，如果数据一直没来，直接进行循环读取就会持续在这里发生阻塞，这就是同步IO的特点，所以一定要注册一下然后等通知，这样就可以避免长期阻塞等候数据。

## 定时器

由于非活跃连接占用了连接资源，严重影响服务器的性能，通过实现一个服务器定时器，处理这种非获取连接，释放连接资源。

我们要将每个定时事件分别封装成定时器，并使用某种容器类数据结构，比如链表、排序链表和时间轮，将所有定时器串联起来，以实现对定时事件的统一管理。

本项目是为了方便释放那些超时的非活动连接，关闭被占用的文件描述符，才使用定时器。

### 什么是定时事件

定时事件，是指固定一段时间之后触发某段代码，由该段代码处理一个事件。这里是删除非活动的 epoll 树上的注册事件，并关闭对应的socket，连接次数减一。

### 什么是定时器

是指利用结构体或其他形式，将多种定时事件进行封装起来。这里只涉及一种定时事件，这里将该定时事件与连接资源封装为一个定时器类。具体包括连接资源、超时时间和回调函数，这里的回调函数指向定时事件。

#### 连接资源包括什么

连接资源包括客户端套接字地址、文件描述符和定时器

#### 超时时间

超时时间=浏览器和服务器连接时刻 + 固定时间(TIMESLOT)，可以看出，定时器使用绝对时间作为超时值.

### 什么是定时器容器

项目中的定时器容器为带头尾结点的**升序双向链表**，具体的为每个连接创建一个定时器，将其添加到链表中，并按照超时时间升序排列。

#### 什么是定时任务

将超时的定时器从链表中删除。

#### 什么是定时任务处理函数？

定时任务处理函数，该函数封装在容器类中，具体的，函数遍历升序链表容器，根据超时时间，删除对应的到期的定时器，并调用回调函数（即定时事件）。

(注意：定时任务处理函数在主循环中调用)

### 说一下定时器的工作原理

服务器主循环为每一个连接创建一个定时器，并对每个连接进行定时。另外，利用升序时间链表容器将所有定时器串联起来，若主循环接收到定时通知，则在链表中依次执行定时任务处理函数。

> 怎么通知主循环？

利用alarm函数周期性地触发**SIGALRM信号**，信号处理函数利用管道通知主循环（注意，本项目信号处理函数仅仅发送信号通知程序主循环，将信号对应的处理逻辑放在程序主循环中，由主循环执行信号对应的逻辑代码。）

#### 双向链表删除和添加的时间复杂度还可以优化

|  位置   | 添加  | 删除  |
|  ----  | ----  | ----  |
| 刚好在头节点  | O(1) | O(1) |
| 刚好在尾节点  | O(n) | O(1) |
| 平均  | O(n) | O(1) |

备注：

a.添加的尾节点时间复杂度为O(n)，是因为本项目的逻辑是先从头遍历新定时器在链表的位置，如果位置恰好在最后，才插入在尾节点后，所以是O(n)。

b.删除的复杂度都是O(1)，因为这里的删除都是已知目标定时器在链表相应位置的删除。（看1.7可知，函数遍历升序链表容器，根据超时时间，删除对应的到期的定时器）

优化：

a.在双向链表的基础上优化：

添加在尾节点的时间复杂度可以优化：在添加新的定时器的时候，除了检测新定时器是否在小于头节点定时器的时间外，再先检测新定时器是否在大于尾节点定时器的时间，都不符合再使用常规插入。

b.不使用双向链表，使用**最小堆**结构可以进行优化。

#### 最小堆怎么优化

时间复杂度：

添加：O(lgn)

删除：O(1)

工作原理：

将所有定时器中超时时间最小的一个定时器的超时值作为alarm函数的定时值。这样，一旦定时任务处理函数tick()被调用，超时时间最小的定时器必然到期，我们就可以在tick 函数中处理该定时器。然后，再次从剩余的定时器中找出超时时间最小的一个（堆），并将这段最小时间设置为下一次alarm函数的定时值。如此反复，就实现了较为精确的定时。



## 压力测试

阅读 Webbench 源码，对 进程 加深理解

通过 Webbench 创建多个进程，每个进程通过 HTTP 连接访问服务器，完成压力测试。

可以实现 上万 并发连接
- 每秒钟相应请求数：552852 /min
- 每秒钟传输数据量：1031990 bytes/sec 
- 所有连接访问均成功

### Webbench实现的核心原理

- 进程fork若干个子进程，每个子进程在用户要求时间或默认的时间内对目标web循环发出实际访问请求;
- 父子进程通过管道进行通信，子进程通过管道写端向父进程传递在若干次请求访问完毕后记录到的总信息；
- 父进程通过管道读端读取子进程发来的相关信息，子进程在时间到后结束，父进程在所有子进程退出后统计并给用户显示最后的测试结果，然后退出。

  - 1.命令行解析，`getopt()`

  - 2.构造http请求报文 `build_request`

  - 3.压力测试：`bench` 函数

- 每个 `fork` 的子进程都调用 `benchcore` 函数，在要求时间内发送请求报文，改函数可记录请求的成功次数、失败次数、以及服务器回复的字节数。

## 压力测试 Bug 排查

使用 Webbench 对服务器进行压力测试，创建 1000 个客户端，并发访问服务器 10s ，正常情况下接近 8w 个请求访问服务器

结果显示请求 7 个请求被成功处理，0 个请求失败，服务器也没有返回错误，这时候尝试从浏览器访问服务器，发现这个请求也不能被处理和响应，必须将服务器重启，浏览器才能正常访问。

### 排查过程

通过查询服务器运行日志，通过日志观察猜想是 接受请求连接 部分发生了错误。

其中，服务器接收请求的连接步骤为：`socket --> bind --> listen --> accept`      
客户端请求连接的步骤为：`socket --> connect`


#### listen 

```cpp
#include<sys/socket.h>
int listen(int sockfd, int backlog)
```

- 函数功能，把一个未连接的套接字转换成一个被动套接字，指示内核应接受指向该套接字的连接请求。根据 TCP 状态转换图，调用 listen 导致套接字从 CLOSED 状态转换成 LISTEN 状态。

- backlog 是队列的长度，内核为任何一个给定的监听套接口维护两个队列：
  - **未完成连接队列**（incomplete connection queue），每个这样的 SYN 分节对应其中一项：已由某个客户发出并到达服务器，而服务器正在等待完成相应的 TCP 三次握手过程。这些套接口处于 SYN_RCVD 状态。
  - **已完成连接队列**（completed connection queue），每个已完成 TCP 三次握手过程的客户对应其中一项。这些套接口处于 ESTABLISHED `[ɪˈstæblɪʃt]` 状态。

#### connect

当有客户端主动连接（connect）服务器，Linux 内核就自动完成 TCP 三次握手，该项就从未完成连接队列移到已完成连接队列的队尾，将建立好的连接自动存储到队列中，如此重复。

#### accept

- 函数功能，从处于 ESTABLISHED 状态的连接队列头部取出一个已经完成的连接(三次握手之后)。

- 如果这个队列没有已经完成的连接，accept 函数就会阻塞，直到取出队列中已完成的用户连接为止。

- 如果，服务器不能及时调用 accept 取走队列中已完成的连接，队列满掉后，TCP 就绪队列中剩下的连接都得不到处理，同时新的连接也不会到来。

从上面的分析中可以看出，accept 如果没有将队列中的连接取完，就绪队列中剩下的连接都得不到处理，也不能接收新请求，这个特性与压力测试的 Bug 十分类似。


#### 定位 accept

分析代码发现，web端和服务器端建立连接，采用 Epoll 的 边缘触发模式 同时监听多个文件描述符。

#### Epoll的ET、LT

- LT水平触发模式

`epoll_wait` 检测到文件描述符有事件发生，则将其通知给应用程序，应用程序可以不立即处理该事件。

当下一次调用 `epoll_wait` 时，`epoll_wait` 还会再次向应用程序报告此事件，直至被处理。

- ET边缘触发模式

  - `epoll_wait` 检测到文件描述符有事件发生，则将其通知给应用程序，应用程序必须立即处理该事件。

  - 必须要一次性将数据读取完，使用非阻塞 I/O，读取到出现 eagain。

从上面的定位分析，问题可能是错误使用 epoll 的 ET 模式。

#### 代码分析解决

尝试将 listenfd 设置为 LT 阻塞

```cpp
for(int i=0;i<number;i++)
{
    int sockfd=events[i].data.fd;

    //处理新到的客户连接
    if(sockfd==listenfd)
    {
        struct sockaddr_in client_address;
        socklen_t client_addrlength=sizeof(client_address);

        //从listenfd中接收数据
        //这里的代码出现使用错误
        while ((connfd = accept (listenfd, (struct sockaddr *) &remote, &addrlen)) > 0){
            if(connfd<0)
            {
                printf("errno is:%d\n",errno);
                continue;
            }
            //TODO,逻辑处理
        }
    }
}
```

将代码修改后，重新进行压力测试，问题得到解决，服务器成功完成 75617 个访问请求，且没有出现任何失败的情况。压测结果如下：

#### Bug原因

established 状态的连接队列 backlog 参数，历史上被定义为已连接队列和未连接队列两个的大小之和，大多数实现默认值为5。当连接较少时，队列不会变满，即使 listenfd 设置成 ET 非阻塞，不使用 while 一次性读取完，也不会出现 Bug。

若此时 1000个 客户端同时对服务器发起连接请求，连接过多会造成 established 状态的连接队列变满。但 accept 并没有使用 while 一次性读取完，只读取一个。因此，连接过多导致 TCP 就绪队列中剩下的连接都得不到处理，同时新的连接也不会到来。

解决方案

将 listenfd 设置成LT阻塞，或者 ET 非阻塞模式下 while 包裹 accept 即可解决问题。

-----------------------------------

## 综合能力

### 使用 crc32 算法解决数据包收发过程中内容被篡改的问题

引入 CRC32 的目的是对收发的数据包进行简单校验，以确保数据包中的内容是没有被篡改的。这部分代码是借鉴过来的。

项目中主要是用到 Get_CRC 这个成员函数，这个函数的作用是：给定一段内存以及该内存的长度，可以计算出 crc32 值并返回，这段给定的内存内容或长度如果不同，返回的数字一般就会不同

当客户端将要发送一个数据包给服务器时，会提前把这个数据包的包体通过该函数计算出一个 crc32 值，放到要发送的数据包内。服务器收到一个完整的数据包之后，会根据收到的包体内容计算包体 crc32 值，与客户端发送过来的 crc32 值比较，如果 2 个 值相同，就认为这个数据包合法，否则就认为不合法丢弃。

如果一个恶意的客户端，就算破解出本项目的包格式（包头+包体），只要他破解不出服务端用的 crc32 算法，他发过来的数据包就不会被服务器认可，会被服务器丢弃。为本项目的网络服务器多了一层保障。



### 服务器突然运行很慢怎么处理

先查看后台服务器的运行状态，包括磁盘，CPU，内存的使用情况等（top，free）。如果是磁盘满了，做好备份，清理下磁盘；如果是CPU的问题，查找下占用率较高的进程，kill 掉与系统应用无关的进程。

还有一种情况可能是close_wait或者time_wait状态过多了，消耗了服务器的资源，使用netstat命令查看下网络连接的状态。



### 你的项目相比于其他项目的优点

> 丢包策略，安全方面

-------

# <font color="orange">音视频项目</font>

## 音视频直播清晰度、画面质量、流畅度如何保障，降低迟延

> https://zhuanlan.zhihu.com/p/82854047

有一个项目采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如何想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频上云到服务端的流媒体，再进行大规模的分发和转发，这也是视频监控的基本玩法。但是我们摄像头以前只支持TCP长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将 TCP 推流改成 UDP 推流，

> https://www.sohu.com/a/305914050_134613

首先基本上直播的推流和拉流一般都是在基于 RTMP 协议的，在网络状态良好的情况下 RTMP 协议无论是推流还是拉流都是可以很好工作的，至少在网络不丢包的情况下是很流畅的，但是因为 RTMP 是基于 TCP 协议的，因为 TCP 协议需要通过 3次 握手建立链接，还需要 TLS 2 次握手，还用拥塞不可控等情况，这些情况都会影响音视频播放的流畅度，在弱网环境下延迟会增大。

因此我们选择了使用 UDP，因为 UDP 不需要建立连接，而且速度快，占用资源少。

> 摄像头，并发少，RTMP，TCP --> UDP QUIC协议

我当时采用的是 QUIC 协议 ，QUIC 就是快速 UDP 互联网连接，QUIC 位于应用层(比如 Http)和网络层(UDP)之间，是在 **UDP 上实现的一个多路复用的协议**。

QUIC 替换掉了 TCP 在应用层和传输层中间的角色

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/QUIC.6egow7866pw0.png)

**QUIC 的总体特点**

- 一是可靠性改进，QUIC为了保持TCP的可靠性，几乎继承了TCP所有的特性，比如说序列号改进、拥塞控制、重传机制优化，安全保密性

- 还有一点就是，无队头阻塞的多路复用。QUIC 的多路复用，在一条 QUIC 连接上可以发送多个请求 (stream)，一个连接上的多个请求( stream )之间没有依赖。比如说这个 packet 丢失了，不会影响其他的 stream。这个特性对于直播来说，弱网下推流更流畅。

- ORTT 连接。QUIC 的连接将版本协商、加密、和传输握手交织在一起以减少连接建立延迟。这个特性对于直播来说，使首幀更快，延迟更小。

- 改进的拥塞控制。这个是 QUIC 最重要的一个特性，TCP 的拥塞控制包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复。QUIC 协议当前默认使用了 TCP 协议的 Cubic `[ˈkjuːbɪk]` 拥塞控制算法，同时 QUIC 拥有完善的数据包同步机制，在应用层做了很多网络拥塞控制层面的优化，能有效降低数据丢包率，这有助降低复杂网络下的直播卡顿率，提升传输效率，可是使推流更流畅。

> 在 UDP 上实现多路复用协议，继承 TCP 的序列号，拥塞控制，重传等特性    
> 无队头阻塞的多路复用，一个连接多个请求，packet 丢失互不干扰   
> 版本协商，加密，握手交织在一起，减少建立迟延      
> 改进拥塞控制,TCP 4 个算法，默认使用了 TCP 的 Cubic 算法，完善同步机制



## RTMP 协议与 HLS 协议

- RTMP 协议是实时消息协议。但它实际上并不能做到真正的实时，一般情况最少都会有几秒到几十秒的延迟，底层是基于 TCP 协议的

- HLS，全称 HTTP Live Streaming，是苹果公司实现的基于 HTTP 的流媒体传输协议。它可以支持流媒体的直播和点播，主要应用在 iOS 系统和 HTML5 网页播放器中。   
HLS 协议的本质就是通过 HTTP 下载文件，然后将下载的切片缓存起来。由于切片文件都非常小，所以可以实现边下载边播的效果。HLS 规范规定，播放器至少下载一个 ts 切片才能播放，所以 HLS 理论上至少会有一个切片的延迟。

HLS 最主要的问题就是实时性差。由于 HLS 往往采用 10s 的切片，所以最小也要有 10s 的延迟，一般是 20～30s 的延迟，有时甚至更差。HLS 之所以能达到 20～30s 的延迟，主要是由于 HLS 的实现机制造成的。HLS 使用的是 HTTP 短连接，且 HTTP 是基于 TCP 的，所以这就意味着 HLS 需要不断地与服务器建立连接。TCP 每次建立连接时都要进行三次握手，而断开连接时，也要进行四次挥手，基于以上这些复杂的原因，就造成了 HLS 延迟比较久的局面。

在 PC 上，我们使用 RTMP 协议，因为 PC 基本都安装了 Flash 播放器，直播效果要好很多。

点播系统使用 HLS 协议。因为点播没有实时互动需求，延迟大一些是可以接受的，并且可以在浏览器上直接观看。

> 在拉取 HLS 媒体流时，客户端首先通过 HLS 协议将 m3u8 索引文件下载下来，然后按索引文件中的顺序，将 .ts 文件一片一片下载下来，然后一边播放一边缓冲。此时，你就可以在 PC、手机、平板等设备上观看直播节目了。

**对于使用 HLS 协议的直播系统来说，最重要的一步就是切片**。源节点服务器收到音视频流后，先要数据缓冲起来，保证到达帧的所有分片都已收到之后，才会将它们切片成 TS 流。

> PC 上使用 RTMP 协议，点播使用 HLS 协议，点播没有实时互动需求    
> RTMP 基于 TCP，实时性较好，但还是存在   
> HLS 通过 HTTP 下载文件 --> 切片缓存，切片小，可边下载边播放，切片 10s,迟延 10 s，而且也是基于 TCP     
> 切割使用 ffmpeg

## FFmpeg 将 MP4 文件切割成 HLS 格式

我们是通过 FFmpeg 将 MP4 文件切割成 HLS 格式

```
ffmpeg -i test.mp4 -c copy -start_number 0 -hls_time 10 -hls_list_size 0 -hls_segment_filename test%03d.ts index.m3u8
```

- -i ，输入文件选项，可以是磁盘文件，也可以是媒体设备。
- -c copy，表示只是进行封装格式的转换。不需要将多媒体文件中的音视频数据重新进行编码。
- -start_number，表示 .ts 文件的起始编号，这里设置从 0 开始。当然，你也可以设置其他数字。
- -hls_time，表示每个 .ts 文件的最大时长，单位是秒。这里设置的是 10s，表示每个切片文件的时长，为 10 秒。当然，由于没有进行重新编码，所以这个时长并不准确。
- -hls_list_size，表示播放列表文件的长度，0 表示不对播放列表文件的大小进行限制。
- -hls_segment_filename，表示指定 TS 文件的名称。index.m3u8，表示索引文件名称。

## flv.js 和 video.js 

`flv.js` 是由 `bilibili` 公司开源的项目。它可以解析 FLV 文件，从中取出音视频数据并转成 BMFF 片段（一种 MP4 格式），然后交给 HTML5 的`<video>`标签进行播放。通过这种方式，使得浏览器在不借助 Flash 的情况下也可以播放 FLV 文件了。

`flv.js` 更聚焦在多媒体格式方面，其主要是将 FLV 格式转换为 MP4 格式，而对于播放器的音量控制、进度条、菜单等 UI 展示部分没有做特别的处理。而 `video.js` 对音量控制、进度条、菜单等 UI 相关逻辑做了统一处理，对媒体播放部分设计了一个插件框架，可以集成不同媒体格式的播放器进去。所以相比较而言，`video.js` 更像是一款完整的播放器。

> flv.js 将 FLV 文件，提出去音频转成 BMFF ，一种 MP4 格式，交给 `<video>` 播放。没有做 播放控制，进度条 等处理    
> `video.js` 对音量控制，进度统一处理


### `flv.js` 基本工作原理

`flv.js` 的工作原理非常简单，它首先将 FLV 文件转成 ISO BMFF（MP4 片段）片段，然后通过浏览器的 Media Source Extensions 将 MP4 片段播放出来。

flv.js 播放器，它包括以下四部分：

- Fetch Stream Loader，指通过 URL 从互联网获取 HTTP-FLV 媒体流。其主要工作就是通过 HTTP 协议下载媒体数据，然后将下载后的数据交给 IO Controller。

- IO Controller ，一个控制模块，负责数据的加载、管理等工作。它会将接收到的数据传给 FLV Demux。

- FLV Demux ，主要的工作是去掉 FLV 文件头、TAG 头等，拿到 H264/AAC 的裸流。关于 FLV 文件格式，

- MP4 Remux ，它的工作是将 H264/AAC 裸流加上 MP4 头，采用的多媒体格式协议是 BMFF。它会将封装好格式的文件传给浏览器的 Data Source 对象。



## 直播卡顿优化

优化缓冲策略
在点播场景下，为了减少播放过程中的卡顿，可以在缓冲一定的数据后再解码播放。但是这样，就会影响视频的首屏播放速度。

增大播放器的缓冲区，使得每次下载时能够加载足够的数据再进行播放，能够降低播放过程中卡顿的频次，但是这样也会延长首屏播放速度以及每次卡顿后恢复播放的速度。所以对于缓冲区的大小的设置，需要考虑卡顿和快速开播两个因素，尽量取得平衡。

在 iOS 平台上，使用系统的 AVPlayer 时，属性 `automaticallyWaitsToMinimizeStalling` 就是控制播放器缓冲策略的。当该值为 YES 时，`AVPlayer` 会努力尝试延迟开始播放，加载足够的数据来保证整个播放过程中尽量卡顿最少。这个接口在 iOS 10 及以上版本才开放，在 iOS 10 之前的版本，在播放 HLS 这种流媒体视频时，效果如同 `automaticallyWaitsToMinimizeStalling` 为 YES，播放基于文件的视频资源，包括通过网络传输的网络视频文件，则效果如同 `automaticallyWaitsToMinimizeStalling` 为 NO。


## 谈谈FFmpeg的解码流程，说说你能够认识的函数作用

https://zhuanlan.zhihu.com/p/126693434


> 比如说从本地读取 AAC 码流，然后解码

大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)

解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的一帧数据， `AVFrame` 存放的是解码后的一帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)

- `avcodec_register_all()`：注册所有的编解码器。「新版本不需要这步」
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是- `AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。

## 音视频同步问题

> https://blog.csdn.net/titer1/article/details/39613123

> 这一块不是很了解，但是知道可以使用 时间戳 来解决

- 大概就是首先选择一个参考时钟；
- 生成数据流时依据参考时钟上的时间给每个数据块都打上时间戳（一般包括开始时间和结束时间）；
- 在播放时，读取数据块上的时间戳，同时参考当前参考时钟上的时间来安排播放（如果数据块的开始时间大于当前参考时钟上的时间，则不急于播放该数据块，直到参考时钟达到数据块的开始时间；
- 如果数据块的开始时间小于当前参考时钟上的时间，则“尽快”播放这块数据或者索性将这块数据“丢弃”，以使播放进度追上参考时钟）。


## 实现H.264的实时传输

> https://mp.weixin.qq.com/s/gHWkqxpZhE4_oq8Q26GeSw

在实时传输是，H.264视频传输协议一般是遵循 RTP 标准的，

### H.264的RTP报头

![](https://mmbiz.qpic.cn/mmbiz_png/JnjAFibwDiaicVGy6Wiceo04obg4eIKth06R3n7apA6hRiaVic4VMAr7piaeewzSekeic0SeGbcFmlUlicia9nLy8N4dOMOQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 标记位 (M)

对RTP时间戳所对应访问单元的最后一个数据包来设置标记位，符合视频中M位的正常使用格式，以允许有效的播放缓冲处理。解码器可以使用这个位作为访问单元最后一个数据包的早期指示，但是不能完全依赖这个属性。

- 负载类型 (PT)：7位

没有特别指定的负载类型，需要通过协商来确定。

- 序列号（SN）：16位

根据 RFC 3550 设置和使用。对于单 NAL 单元和非交错打包模式，序列号用于确定NAL单元的解码顺序。

- 时间戳：32位

RTP时间戳设置为视频内容的采样时间戳。必须使用90 kHz时钟频率。 

## 基于 VideoToolbox 来实现 H264 硬编码和硬解码

### H264 硬编码

> https://zfpp25.blog.csdn.net/article/details/108219421

iOS8.0及以上我们可以通过 VideoToolbox 实现视频数据的硬编解码。

基本步骤

- 1、通过 VTCompressionSessionCreate 创建编码器
- 2、通过 VTSessionSetProperty 设置编码器属性
- 3、设置完属性调用 VTCompressionSessionPrepareToEncodeFrames 准备编码
- 4、输入采集到的视频数据，调用 VTCompressionSessionEncodeFrame 进行编码
- 5、获取到编码后的数据并进行处理
- 6、调用 VTCompressionSessionCompleteFrames 停止编码器
- 7、调用 VTCompressionSessionInvalidate 销毁编码器

> 创建编码器 --> 设置编码器的属性 --> 然后准备进行编码 --> 接着是对编码后的数据进行处理 --> 最后还需要 停止和销毁编码器     
> 主要是用 `VTCompressionSession` 下的几个函数

#### 遇到难题

在弄视频编解码的时候，发现720P的分辨率，码率1Mbps，在画面晃动的时候马赛克很严重，码率设置的再低一点更严重。一开始我以为是编码器的某些属性漏了设置了，或者是参数设置错了。查阅了很多资料都找不到原因。后来怀疑是ABR模式当画面从静止到晃动码率一下子上不去，导致马赛克，这个假设似乎成立，结果去打印编码出来的码率，画面晃动的时候码率是有上去的，说明这个思路还是不对。后来，我发现，摄像头采集的数据是720P，也就是1280x720的分辨率，我给编码器设置编码宽高的时候也是按1280x720的宽高设给编码器的，但实际上我解码、播放是展示的画面尺寸(像素)只有320x180，于是我尝试了一下把编码的宽高设置为320x180，马赛克问题解决了！

### H264 硬解码

> https://zfpp25.blog.csdn.net/article/details/108219440

硬解码流程很简单：
- 1、解析H264数据
- 2、初始化解码器（VTDecompressionSessionCreate）
- 3、将解析后的 H264 数据送入解码器（VTDecompressionSessionDecodeFrame）
- 4、解码器回调输出解码后的数据（CVImageBufferRef）

## ffmpeg 博客

ffmpeg：https://www.cnblogs.com/leisure_chn/category/1351812.html

### ffmpeg 源码中内存管理

> https://blog.csdn.net/King1425/article/details/70613310

`av_malloc()`: 是内存分配函数，`av_malloc() `就是简单的封装了系统函数malloc()，并做了一些错误检查工作。

`av_realloc()`用于对申请的内存的大小进行调整. `av_realloc()` 简单封装了系统的`realloc()`函数。


`av_free()`用于释放申请的内存,`av_free()` 简单的封装了`free`

`av_freep()`简单封装了`av_free()`。并且在释放内存之后将目标指针设置为 NULL

### AVFormatContext 和 AVInputFormat之间的关系

![](./img/ffmpeg数据结构04.png)


`AVInputFormat`被封装在`AVFormatContext`里

`AVFormatContext` 作为`API`被外界调用

`AVInputFormat` 主要是`FFmpeg`内部调用

`AVFormatContext`里保存了视频文件封装格式相关信息，它是负责储存数据的结构体。而`AVInputFormat`代表了各个封装格式，属于方法，这是一种面向对象的封装。
 
通过 `int avformat_open_input(AVFormatContext **ps, const char *filename,AVInputFormat *fmt, AVDictionary **options)`函数装载解封装器.