- [<font color="orange">实习项目</font>](#font-colororange实习项目font)
  - [Hobot-SDK-<font color="orange">实习项目</font>](#hobot-sdk-font-colororange实习项目font)
  - [静态变量](#静态变量)
    - [怎么使用静态变量](#怎么使用静态变量)
    - [为什么使用宏函数](#为什么使用宏函数)
    - [介绍一下工厂模式](#介绍一下工厂模式)
      - [简单工厂模式](#简单工厂模式)
      - [工厂方法模式](#工厂方法模式)
      - [抽象工厂模式](#抽象工厂模式)
  - [死亡测试](#死亡测试)
    - [死亡测试运行方式](#死亡测试运行方式)
    - [detach 函数](#detach-函数)
      - [怎么进行测试](#怎么进行测试)
    - [说一下`C++` 的智能指针](#说一下c-的智能指针)
      - [RAII 是什么](#raii-是什么)
  - [什么是 RTTI](#什么是-rtti)
    - [dynamic_cast 运算符](#dynamic_cast-运算符)
    - [typeid 运算符](#typeid-运算符)
- [<font color="orange">个人服务器项目</font>](#font-colororange个人服务器项目font)
  - [这个自己申请的域名吗](#这个自己申请的域名吗)
  - [设置进程名称](#设置进程名称)
    - [怎么修改进程名称](#怎么修改进程名称)
  - [日志打印实现](#日志打印实现)
    - [printf 的实现](#printf-的实现)
    - [日志等级划分](#日志等级划分)
    - [日志输出时遇到了问题 (行缓存造成日志输出混乱问题)](#日志输出时遇到了问题-行缓存造成日志输出混乱问题)
    - [日志写入混乱问题 和 掉电导致数据丢失问题](#日志写入混乱问题-和-掉电导致数据丢失问题)
    - [怎么解决掉电导致数据丢失问题](#怎么解决掉电导致数据丢失问题)
      - [fwrite 和 write 有什么区别](#fwrite-和-write-有什么区别)
      - [fwrite 实现原理](#fwrite-实现原理)
    - [说下你的日志系统的运行机制](#说下你的日志系统的运行机制)
    - [日志系统使用了单例模式](#日志系统使用了单例模式)
  - [线程池相关](#线程池相关)
    - [线程的同步机制有哪些](#线程的同步机制有哪些)
    - [线程同步的方式](#线程同步的方式)
    - [手写线程池](#手写线程池)
      - [线程池中的工作线程是一直等待吗？](#线程池中的工作线程是一直等待吗)
      - [你的线程池工作线程处理完一个任务后的状态是什么](#你的线程池工作线程处理完一个任务后的状态是什么)
      - [如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢](#如果同时1000个客户端进行访问请求线程数不多怎么能及时响应处理每一个呢)
      - [如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢](#如果一个客户请求需要占用线程很久的时间会不会影响接下来的客户请求呢有什么好的策略呢)
    - [多线程理解](#多线程理解)
    - [阻塞与非阻塞](#阻塞与非阻塞)
    - [异步和同步](#异步和同步)
    - [I/O 多路复用](#io-多路复用)
    - [怎么实现非阻塞 socket](#怎么实现非阻塞-socket)
  - [Epoll技术简介](#epoll技术简介)
    - [epoll_create 函数](#epoll_create-函数)
    - [epoll_ctl 函数](#epoll_ctl-函数)
    - [epoll_wait 函数](#epoll_wait-函数)
      - [关于 epitem 节点](#关于-epitem-节点)
  - [向内核双链表增加节点](#向内核双链表增加节点)
  - [使用 epoll 函数来实现数据的收发](#使用-epoll-函数来实现数据的收发)
    - [创建连接池的目的](#创建连接池的目的)
    - [ET 和 LT 模式](#et-和-lt-模式)
      - [LT 模式-水平触发](#lt-模式-水平触发)
      - [ET 模式-边缘触发](#et-模式-边缘触发)
    - [事件驱动](#事件驱动)
    - [腾讯面试题](#腾讯面试题)
    - [深入理解ET LT](#深入理解et-lt)
    - [Epoll 中 ET 和 LT 模式的处理编码不同](#epoll-中-et-和-lt-模式的处理编码不同)
  - [什么是 Reactor 模式](#什么是-reactor-模式)
      - [常见的 Reactor 实现方案](#常见的-reactor-实现方案)
    - [什么是 Proactor](#什么是-proactor)
      - [理解 Reactor 和 Proactor 的区别](#理解-reactor-和-proactor-的区别)
    - [同步I/O模型的工作流程](#同步io模型的工作流程)
  - [定时器](#定时器)
    - [什么是定时事件](#什么是定时事件)
    - [什么是定时器](#什么是定时器)
      - [连接资源包括什么](#连接资源包括什么)
      - [超时时间](#超时时间)
    - [什么是定时器容器](#什么是定时器容器)
      - [什么是定时任务](#什么是定时任务)
      - [什么是定时任务处理函数？](#什么是定时任务处理函数)
    - [说一下定时器的工作原理](#说一下定时器的工作原理)
      - [双向链表删除和添加的时间复杂度还可以优化](#双向链表删除和添加的时间复杂度还可以优化)
      - [最小堆怎么优化](#最小堆怎么优化)
  - [压力测试](#压力测试)
    - [Webbench实现的核心原理](#webbench实现的核心原理)
  - [压力测试 Bug 排查](#压力测试-bug-排查)
    - [排查过程](#排查过程)
      - [listen](#listen)
      - [connect](#connect)
      - [accept](#accept)
      - [定位 accept](#定位-accept)
      - [Epoll的ET、LT](#epoll的etlt)
      - [代码分析解决](#代码分析解决)
      - [Bug原因](#bug原因)
  - [综合能力](#综合能力)
    - [使用 crc32 算法解决数据包收发过程中内容被篡改的问题](#使用-crc32-算法解决数据包收发过程中内容被篡改的问题)
    - [服务器突然运行很慢怎么处理](#服务器突然运行很慢怎么处理)
    - [你的项目相比于其他项目的优点](#你的项目相比于其他项目的优点)
  - [Nginx 相关](#nginx-相关)
    - [Nginx虚拟主机怎么配置](#nginx虚拟主机怎么配置)
      - [基于虚拟主机配置域名](#基于虚拟主机配置域名)
      - [基于端口的虚拟主机](#基于端口的虚拟主机)
    - [什么是正向代理和反向代理](#什么是正向代理和反向代理)
      - [使用 反向代理服务器的优点是什么?](#使用-反向代理服务器的优点是什么)
    - [nginx是如何实现高并发的](#nginx是如何实现高并发的)
      - [进程模型](#进程模型)
      - [事件模型](#事件模型)
- [<font color="orange">课题流媒体项目</font>](#font-colororange课题流媒体项目font)
  - [WebRTC 项目](#webrtc-项目)
    - [传统直播基本架构](#传统直播基本架构)
    - [RTMP 介绍](#rtmp-介绍)
    - [HLS](#hls)
    - [HLS 的不足](#hls-的不足)
    - [如何选择 RTMP 和 HLS](#如何选择-rtmp-和-hls)
  - [HLS 直播架构](#hls-直播架构)
    - [m3u8 格式分析](#m3u8-格式分析)
    - [TS 格式分析](#ts-格式分析)
    - [优化卡顿带来的累积延时](#优化卡顿带来的累积延时)
    - [http响应耗时](#http响应耗时)

-------

# <font color="orange">实习项目</font>

## Hobot-SDK-<font color="orange">实习项目</font>

Hobot Framework 提供了一个通过有向图的形式, 将基础代码功能模块组织成较复杂功能模块的机制。

基于 Hobot Framework, 所有的基础的功能以 `Module` 的形式提供; 功能之间的数据交换以 `Message` 为单位。

一个 `Module` 由多个 `Forward{n}` 组成，其中 `Forward{n}` 又可以以一个或多个别的 `Module::Forward{n}` 产生的 `Message` 为输入, 可以输出自己的 `Message`。

这样, 基础功能的开发者可以将自己的基础功能封装成一个 包含多个 `Forward` 的 `Module`, 并说明自己的输入输出;

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/concept.2nnp25ds1lu0.png)

## 静态变量

- 函数内部定义一个局部对象时，使用 static 关键字时，这个变量就保存在静态存储区，在编译的时候初始化，如果不给初始化值，它的值就会被初始化为 0，并且，下次调用这个函数的时候该变量中保存的值就是上一次函数调用结束时的值

- 在全局变量时使用 static 关键字，那么这个全局变量只能在本文件中使用，无法在其他文件中被引用。

- 在函数之前加 static 时，那么函数只能在本源程序文件中调用，无法在其他源程序文件中调用。

- 在一个类中定义 static 成员，那么这个成员不属于某个对象，而是属于整个类。

> 编译阶段初始化，全局变量，函数，对象

### 怎么使用静态变量

在实现测试用例的自动保存的时候，会用一个宏函数 [`GTEST_TEST_CLASS_NAME_`]，这个宏函数里面有一个静态变量 test_info_ ，它利用”静态变量在程序运行前被初始化“的特性，抢在 main 函数执行之前，执行一段代码，从而有机会将测试用例放置于一个固定的位置。这个是”自动“保存测试用例的本质所在。

然后就是 test_info_ 的初始化的时候会插入一个模板类，这个模板类是一个 工厂类 ，类继承于 TestFactoryBase，并重载了 CreateTest 方法，主要的作用是 new 出一个 TestInfo 类对象，并实现一个 AddTestInfo 方法，将其测试用例保存起来。

在 AddTestInfo 中是通过 测试用例名 等信息获取测试用例，然后调用测试用例对象去新增一个测试特例—— test_info 

测试用例的保存使用一个 vetor 容器来保存

### 为什么使用宏函数

> 主要是 利用空间换时间的思想吧

普通函数和宏函数的区别就在于，宏函数占用了大量的空间，而函数占用了时间。大家要知道的是，函数调用是要使用系统的栈来保存数据的，如果编译器里有栈检查选项，一般在函数的头会嵌入一些汇编语句对当前栈进行检查；同时，CPU 也要在函数调用时保存和恢复当前的现场，进行压栈和弹栈操作，所以，函数调用需要一些 CPU 时间。 而宏函数不存在这个问题。

宏在编译之前进行,也就是先用宏体替换宏名,然后再编译的,而函数显然是编译之后,在执行时,才调用的.因此,宏占用的是编译的时间,而函数占用的是执行时的时间.

宏函数仅仅作为预先写好的代码嵌入到当前程序，不会产生函数调用，所以仅仅是占用了空间，因为在进行单元测试的时候会频繁调用同一个宏函数，所以这样效率会高很多。

### 介绍一下工厂模式

> https://www.cnblogs.com/xiaolincoding/p/11524376.html

- 这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。
- 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，这样客户端就不需要关心对象是如何创建的，并且是通过使用一个共同的接口来指向新创建的对象。

简单来说，使用了 C++ 多态的特性，将存在继承关系的类，通过一个工厂类创建对应的子类（派生类）对象。在项目复杂的情况下，可以便于子类对象的创建。

如果我们直接 new 一个对象时，如果需要的对象构造方法比较复杂，那么可能需要一连串的代码去创建对象，如果在别的类中又需要创建该对象，那么代码的重复度肯定不小。通过工厂模式的话，我们把对象创建的具体逻辑给隐藏起来了，交给工厂统一管理，这样不仅减少了代码量，以后如果想改代码的话，只需要改一处即可，也方便我们日常的维护。

> 构造方法复杂，创建对象代码重复多。把对象创建的逻辑隐藏起来，交给工厂即可，减少代码量，改一处即可，利于维护。

工厂模式的实现方式可分别简单工厂模式、工厂方法模式、抽象工厂模式，每个实现方式都存在优和劣。


以鞋厂为例分析

#### 简单工厂模式

鞋厂可以指定生产耐克、阿迪达斯和李宁牌子的鞋子。哪个鞋炒的火爆，老板就生产哪个，看形势生产。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式01.5dzo9gqbwi00.png)

简单工厂模式的结构组成：

- 工厂类(ShoesFactory)：工厂模式的核心类，会定义一个用于创建指定的具体实例对象的接口。
- 抽象产品类(Shoes)：是具体产品类的继承的父类或实现的接口。
- 具体产品类(NiKeShoes\AdidasShoes\LiNingShoes)：工厂类所创建的对象就是此具体产品实例。

**简单工厂模式的特点**：

工厂类封装了创建具体产品对象的函数。

**简单工厂模式的缺陷**：

扩展性非常差，新增产品的时候，需要去修改工厂类。

#### 工厂方法模式

现各类鞋子抄的非常火热，于是为了大量生产每种类型的鞋子，则要针对不同品牌的鞋子开设独立的生产线，那么每个生产线就只能生产同类型品牌的鞋。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式02.md22sc5xhkw.png)

工厂方法模式的结构组成：
- 抽象工厂类（ShoesFactory）：工厂方法模式的核心类，提供创建具体产品的接口，由具体工厂类实现。
- 具体工厂类（NiKeProducer\AdidasProducer\LiNingProducer）：继承于抽象工厂，实现创建对应具体产品对象的方式。
- 抽象产品类（Shoes）：它是具体产品继承的父类（基类）。
- 具体产品类（NiKeShoes\AdidasShoes\LiNingShoes）：具体工厂所创建的对象，就是此类。

**工厂方法模式的特点**：

- 工厂方法模式抽象出了工厂类，提供创建具体产品的接口，交由子类去实现。
- 工厂方法模式的应用并不只是为了封装具体产品对象的创建，而是要把具体产品对象的创建放到具体工厂类实现。

**工厂方法模式的缺陷**：

- 每新增一个产品，就需要增加一个对应的产品的具体工厂类。相比简单工厂模式而言，工厂方法模式需要更多的类定义。
- 一条生产线只能一个产品。

#### 抽象工厂模式

鞋厂为了扩大了业务，不仅只生产鞋子，把运动品牌的衣服也一起生产了。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/工厂模式03.4kdzxh4jx5e0.png)

抽象工厂模式的结构组成（和工厂方法模式一样）：
- 抽象工厂类厂（ShoesFactory）：工厂方法模式的核心类，提供创建具体产品的接口，由具体工厂类实现。
- 具体工厂类（NiKeProducer）：继承于抽象工厂，实现创建对应具体产品对象的方式。
- 抽象产品类（Shoes\Clothe）：它是具体产品继承的父类（基类）。
- 具体产品类（NiKeShoes\NiKeClothe）：具体工厂所创建的对象，就是此类。

**抽象工厂模式的特点**：

提供一个接口，可以创建多个产品族中的产品对象。如创建耐克工厂，则可以创建耐克鞋子产品、衣服产品、裤子产品等。

**抽象工厂模式的缺陷**：

同工厂方法模式一样，新增产品时，都需要增加一个对应的产品的具体工厂类。

## 死亡测试

死亡测试是为了 判断一段逻辑是否会导致进程退出 而设计的。通常在测试过程中，我们需要考虑各种各样的输入，有的输入可能直接导致程序崩溃，这时我们就需要检查程序是否按照预期的方式挂掉，这也就是所谓的“**死亡测试**”。`gtestX`的死亡测试能做到在一个安全的环境下执行崩溃的测试案例，同时又对崩溃结果进行验证。

死亡测试 在 linux 上实现的过程

- 测试实体中准备启动新的进程，进程路径就是本进程可执行文件路径
- 通过 fork 创建子进程，fork 是标准的子进程和父进程分离执行，所以 threadsafe 对应的  ExecDeathTest 死亡测试类在底层调用的是 fork，从而可以保证是安全的。

- 接着子进程传入了标准输入输出句柄
- 启动子进程时传入类型筛选，也就是指定执行该测试用例
- 监听子进程的输出
- 判断子进程退出模式

子进程的执行过程是：

- 执行父进程指定的测试特例
- 运行死亡测试宏中的表达式
- 如果没有 crash「崩溃」 ，则根据情况选择退出模式

### 死亡测试运行方式

- fast方式（默认的方式）

```cpp
testing::FLAGS_gtest_death_test_style = "fast";
```

- threadsafe方式

```cpp
testing::FLAGS_gtest_death_test_style = "threadsafe";
```

我们可以在 `main()` 里为所有的死亡测试设置测试形式，也可以为某次测试单独设置。Google Test 会在每次测试之前保存这个标记并在测试完成后恢复，所以你不需要去管这部分工作 。如：

```cpp
TEST(MyDeathTest, TestOne) {
  testing::FLAGS_gtest_death_test_style = "threadsafe";
  // This test is run in the "threadsafe" style:
  ASSERT_DEATH(ThisShouldDie(), "");
}

TEST(MyDeathTest, TestTwo) {
  // This test is run in the "fast" style:
  ASSERT_DEATH(ThisShouldDie(), "");
}

int main(int argc, char** argv) {
  testing::InitGoogleTest(&argc, argv);
  testing::FLAGS_gtest_death_test_style = "fast";
  return RUN_ALL_TESTS();
}
```

> 在使用 detach 分离父进程和子进程，会导致线程安全问题，当时使用的是 临时对象的方式解决的。

### detach 函数

如果创建了很多子线程，让主线程逐个等待子线程结束，这种方法就显得不是很好，所以需要引入 detach 这种写法，**让主线程和子线程分离**，主线程不必等待子进程运行结束。

```cpp
mytoobj.detach();
```
> 一旦调用了 detach，就不可再调用 join 了。当然可以使用 joinable 判断是否成功使用 join 或者 detach 。

但是使用 detach 时候会导致内存安全问题，就比如在线程中传递引用，指针之类的参数，这样的话形参的地址是原来主线程中分配的，一旦主线程退出后，子线程再使用这块内存肯定是不安全的。

所以需要使用值传递，在创建线程这一行就构造出 **临时对象**，主要用这个临时构造的对象传递给线程入口函数，那么线程中的到的第二个参数就一定能够在主线程执行完毕之前构造出来。从而确保 detach 线程是安全的。

#### 怎么进行测试

我们建立一个 `class A`，如果直接在在形参中写 `A(mysecondpar)` ;这个对象是由 主线程传入的时候构造出来的，那么就可以出现主线程先结束的时候，就不能再构造了。

那么在给线程入口函数传递类型对象形参时，传在主线程中构造一个临时对象`A(mysecondpar)` ，就可以保证线程入口函数的形参在 主线程 函数退出前就已经创建完完毕，可以安全使用。

```cpp
std::thread mytobj(myprint2,A(mysecondpar) );  //现在主线程构造出临时对象
```

###  说一下`C++` 的智能指针

 `C++`里面的四个智能指针，`auto_ptr`，`unique_ptr`，`shared_ptr`，`weak_ptr`，其中后三个是 c++11 支持，并且第一个已经被`c++11`弃用。

 使用原因：智能指针的作用是管理一个指针，因为在程序设计中动态分配的堆内存没有正确释放或无法释放，导致资源浪费，程序运行速度变慢等问题。使用智能指针可以很大程度上的避免这个问题，因为智能指针是一个类，当超出了类的实例对象的作用域时，会自动调用对象的析构函数，析构函数会自动释放资源。

 所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。**智能指针其实就是一个类模板**。

- <font color="#b5822d" size=5>auto_ptr</font>：采用所有权模式。`p2`剥夺了`p1`的所有权，但是当程序运行时访问`p1`将会报错。所以`auto_ptr`的缺点是：存在潜在的内存崩溃问题。

- <font color="#b5822d" size=5>unique_ptr</font>：是一种独占式智能指针，也就是同一时刻，只能有一个 unique_ptr 指针指向这个对象（这块内存）。在 C++14 的时候，unique_ptr 提供了 make_unique 函数。可以使用这个函数初始化性能更高。

- <font color="#b5822d" size=5>shared_ptr</font>：是一个共享指针，多个指针指向同一个对象（多个指针指向同一块内存），最后一个指针被销毁的时候，这个对象被释放，`shared_ptr`的工作机制是使用**引用计数**。            
可以使用 移动语义将对象变为空或者赋值，比如:

```cpp
shared_ptr<int> p1(new int(100)); //初始化：p1 指向这个对象（内存）
std::share_ptr<int> p2 = new int(100); //不行，因为智能指针是显式的类型转换 explicit ,不可以隐式类型转换

shared_ptr<int> p2(std::move(p1));  //移动语义，移动构造 p2，p1 不再指向这个对象而变成空

shared_ptr <int> p3;   
p3 = std::move(p2);     //移动赋值，p2 指向空，p3 指向这个对象，整个对象引用计数依旧为 1
```

<font color="orange" size=4 font-weight="bold">make_shared</font>

这是一个标准库里的函数模板，被认为最安全和更高效的分配和使用 shared_ptr 智能指针的一个 函数模板。

它能够在动态内存（堆）中分配并初始化一个对象，然后返回指向这个对象的 shared_ptr 。

```cpp
shared_ptr<int> p2 = std::make_shared<int>(100);  // 这个 shared_ptr 指向 100 的整数的内存，类似 int *p1 = new int(100);
shared_ptr<string> p3 = std::make_shared<string>(5,"a");  //类似 string p3(5,'a);

cout << *p2 << endl; //100
cout << *p3 << endl; //aaaaa
```


- <font color="#b5822d" size=5>weak_ptr</font>：也是一个类模板，这个指针指针指向一个由 shared_ptr 管理的对象，但是这种智能指针并不控制所指向的对象的生命周期，也就是说，将 weak_ptr 绑定到 shared_ptr 不会改变 shared_ptr 的引用计数。另外，weak_ptr 来直接访问对象，必须使用一个叫做 `lock` 的成员函数，`lock` 的功能就是检查 weak_ptr 所指向的对象是否还存在，如果存在，`lock` 就能返回一个共享对象的 shared_ptr 如果不存在，就返回一个空的 shared_ptr 。



#### RAII 是什么


RAII 全称是“Resource Acquisition is Initialization”，直译过来是“资源获取即初始化”，

RAII 依托「栈」和「析构函数」，来对所有的资源 —— 包括堆内存在内 —— 进行管理

也就是说在构造函数中申请分配资源，在析构函数中释放资源。


智能指针（`std::shared_ptr`和`std::unique_ptr`）即 RAII 最具代表的实现，使用智能指针，可以实现自动的内存管理，再也不需要担心忘记 delete 造成的内存泄漏。


C++ 支持将对象存储在栈上面。但是，在很多情况下，对象不能，或不应该，存储在栈上。比如：
- 对象很大；
- 对象的大小在编译时不能确定；
- 对象是函数的返回值，但由于特殊的原因，不应使用对象的值返回。

比如说在工厂方法或其他面向对象编程的情况下，返回值类型是基类（的指针或引用）。

我们怎样才能确保不会发生内存泄漏呢

答案就在析构函数和它的**栈展开**行为上。我们只需要把这个返回值放到一个本地变量里，并确保其析构函数会删除该对象即可。这就是 RAII 的基本用法。

> 在发生异常时对析构函数的调用，也叫 **栈展开**，


## 什么是 RTTI

父类指针可以指向 new 一个子类对象

就比如：

```cpp
Human * phuman = new Men;
```

这时候很难确定 phuman 指向哪个类，要向得到所指向的对象相关的类信息就比较困难，所以 RTTI 就是要来解决这类问题的，也就是获取 phuman 所指向的对象相关的类的信息。

RTTI 的目的就是 让程序运行时能根据 **基类**的 指针 或者 引用 来获取这个指针或者愿你拥指向的对象的实际类型

RTTI 通过两个运算符来实现：

- dynameic_cast `[daɪˈnæmɪk]` 运算符：将父类的指针或者引用安全地转换为 子类的指针或者引用
- typeid 运算符：返回指针或者引用所指向对象的实际类型

### dynamic_cast 运算符

想区分一个指针是父类类型还是子类类型，使用 dynamic_cast 运算符就能够判断出来 —— 用 dynamic_cast 能转换成功，就说明这个指针实际上是要转换到的那个类型，所以说 dynamic_cast 是帮助开发者做安全检查的。

> 使用 dynamic_cast 的前提条件是 父类中必须至少有一个虚函数。否则就会编译出错，或者可能无法得到正确的运行结果

### typeid 运算符

通过这个运算符可以获取对象的信息，这个运算符返回一个常量对象的引用。

- 只要两个指针定义时的类型相同，不管它们指向的是父类还是子类，typeid 就相等。

```cpp
Human *phuman = new Men;
Human *phuman2 - -new Women;
```

- 只要两个指针运行时指向的类型相同，typeid 就相等，不管它们定义的类型是否相同。

```cpp
Human *phuman = new Men;
Human *phuman2 = new Men;

Human *phman3 = phuman2;
if( (typeid(*phuman) == ) typeid(*phuman2) )  ???
```

----------

# <font color="orange">个人服务器项目</font>

## 这个自己申请的域名吗

没有申请，因为服务器是放在同一网段的虚拟机里，然后在本地的浏览器里面访问。

或者也可以在同一局域网的不同主机下实验，在同一局域网下通过私有IP+端口号就可以访问。

又或者或者直接把服务器程序放在本地，然后使用本地回环地址127.0.0.1就可以。

本地回环地址主要作用有两个：
- 一是测试本机的网络配置，能 ping 通 127.0.0.1 说明本机的网卡和  IP协议安装都没有问题；
- 另一个作用是某些 server/client 的应用程序在运行时需调用服务器上的资源，一般要指定 server 的 IP server 的 IP 地址设为 127.0.0.1 同样也可以运行。

## 设置进程名称

> [进程名称 参数被覆盖问题](/Cpp项目/通讯实战项目/note?id=设置进程名称)

进程名称实际上是保存在 `argc[0]` 所指向的内存中。CMD 会把 argv 所指向的命令参数全部显示出来，因为我们设置的进程名称是 `./nginx` 是保存在 `argv[0]`中，所以 `argv[0]`改变，进程名也就改变了。

> 在这里遇到了个问题，一旦设置的进程名称的长度大于字符串 `./nginx`的长度，就可能导致设置的进程名称覆盖其他参数。

由于环境变量信息也是保存在内存中的，并且**保存的位置紧紧邻 argv 所指向的内存**。所以若果设置的进程名称太长，不但会覆盖掉命令行参数，而且很可能覆盖掉环境变量所指向的内容。

为此，借助了 nginx 的源码，想到了一个解决方案，就是将环境变量搬家，大致思路是：

- **重新分配一块内存**：足够容纳新的 environ 所指向的内容，把 environ 内容搬到这块内存中来。

- 将以往 `argv[0]` 指向的内容替换成实际要修改的新进程名称

### 怎么修改进程名称

大致逻辑：

- 计算进程名称的长度

- 计算命令行参数所占内存与环境变量所占内存的总和

- 设置新的进程名称

## 日志打印实现

> [日志打印实现](/Cpp项目/通讯实战项目/note?id=日志打印实现)

打印输出相关函数借鉴了 nginx 的实现，并做一些改动，这部分主要是 学习 printf,vprintf 这类函数的内部实现。

### printf 的实现

C 语言中`printf`函数的**参数是可变**的，是通过**栈**实现参数的传递。`printf`至少有一个参数，就是字符串指针，如果还有其他参数，比如

`printf("a=%d b=%d",i,j); `会**从右往左**依次把参数压入栈内，先压`j`然后压`i`，然后压这个串`"a=%d b=%d"`的首地址也压入栈。在进行解析的时候是后入先出，按照`%d`去寻找后面的参数。这样其实`printf`并不知道一共有几个参数,完全按照`%d`或者其他格式的类型去处理。这就有个问题就是要特别注意变量定义的类型一定要与格式控制符表示的格式一致,不一致会出现读取错误.当然`printf`**在转换参数时，对栈只读不写**，不会造成栈错误。

### 日志等级划分

参考 nginx 的日志等级划分，nginx 日志分成 8 个等级，级别从高到低，数字最小的级别最高，数字最大的级别最低，nginx 中有专门的日志处理模块处理日志(*很复杂，没看*)

```
#define NGX_LOG_STDERR            0    //控制台错误【stderr】：最高级别日志，日志的内容不再写入log参数指定的文件，而是会直接将日志输出到标准错误设备比如控制台屏幕
#define NGX_LOG_EMERG             1    //紧急 【emerg】
#define NGX_LOG_ALERT             2    //警戒 【alert】
#define NGX_LOG_CRIT              3    //严重 【crit】
#define NGX_LOG_ERR               4    //错误 【error】：属于常用级别
#define NGX_LOG_WARN              5    //警告 【warn】：属于常用级别
#define NGX_LOG_NOTICE            6    //注意 【notice】
#define NGX_LOG_INFO              7    //信息 【info】
#define NGX_LOG_DEBUG             8    //调试 【debug】：最低级别
```

### 日志输出时遇到了问题 (行缓存造成日志输出混乱问题)

`printf` 函数不加“`\n`”无法及时输出，就是说，我们在实现`ngx_vslprintf`函数 [打印日志] 测试的时候，等待了好几秒，发现屏幕上迟迟没有日志输出的结果，然后突然之间，在屏幕上出现一大堆输出结果。

后来查了 `printf()`底层实现后发现，这是 **行缓存（输出缓冲区)的问题**，标准输入输出函数都是带有缓存的，一般是行缓存（还发现 window 系统上没有这个问题，但是 Unix 系统就有），就是把需要输出的数据先缓存到某个地方，等待 **行刷新标志** 或者 **缓存已满** 的情况下，才会把缓存的数据显示出来。

“`\n`” 可以认为是刷新标志，也可以通过调用 `fflush(stdout)` 函数刷新缓冲区，将结果显示出来。

### 日志写入混乱问题 和 掉电导致数据丢失问题

本项目是 1 个 master 进程， 4 个 worker 进程，假如 5 个进程间同时不停地调用，同时向日志文件中写日志，就会造成日志文件混乱问题

首先先在 master 进程和 worker 进程的功能函数中添加一条日志输出，返发现并没有出现混乱现象，所以初步确定 写日志代码在应对多个进程向同一个日志文件中写日志的时候是没有问题的。

当时还专门去研究了 write 函数内部是怎么解决这个问题的？然后参考这个方案在项目中实现。

> **write 内部是如何解决多进程同时写一个文件不出现混乱问题的呢**？

- 在日志初始化函数中调用了 open 函数，open 函数中的 O_APPEND 标志可以保证多个进程操作同一个文件的时候不会相互覆盖，如果不加这个标记，某些情况下就会出现数据彼此覆盖的问题。

- write 函数在写入文件的时候是原子操作，2 个进程同时写入是竞争关系，最终只会由某个进程写入数据。

- 父进程 fork 出子进程，在父进程都会执行的公共代码就已经调用了 open 函数打开了日志文件，然后才通过 fork 创建出子进程，这种父子进程之间会共享文件表项，文件表项里有当前文件偏移量，子进程用 write 原子操作写了一个日志，文件偏移量会移动到文件末尾，父进程的当前文件偏移量也会移动到文件末尾，因为是共享文件表项，所以父进程 write 是接着子进程写的内容末尾开始写，因此不会混乱。

> 所以本项目中利用这种机制解决了写日志时日志混乱的问题

### 怎么解决掉电导致数据丢失问题

> 这里就需要涉及到 write 内部的写数据过程

内核可以在任何时候写磁盘，但并不是所有的 write 操作都会导致内核的写操作，内核会把待写数据暂存在缓冲区，积累到一定数量后再一次性写入磁盘，如果出现意外，断电，计算机崩溃等，内核还没来得及把内核缓冲区的数据写入磁盘，这些数据就会丢失。

为了确保内核缓冲区中的数据被及时写入磁盘，内核缓冲区中设立了一个时间上限，达到时间上限后，内核会把所有内核缓冲区中的“脏数据”直接写到磁盘。

> 怎么解决掉电导致 write 写入的数据丢失


- 直接 `I/O`,直接访问物理磁盘，但是这样效率会降低。

本项目的 `ngx_log.cxx` 中的 `ngx_log_init` 函数

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT,0644);  
```

如果 open 参数增加 O_DIRECT 就会绕过缓冲区

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT|O_DIRECT,0644);  
```

- 设置 open 文件时的 O_SYNC 选项

O_SYNC 选项也叫**同步选项**，只针对 write 函数有效，使每次 write 操作等待物理 `I/O` 操作完成。也就是说将写入缓冲区的数据立即写入磁盘，而不用等到时间上限，这样将计算机崩溃或者断电时造成的数据丢失减到最小。

也是通过更改 open 第二个参数实现

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_SYNC,0644);  
```

但是直接向磁盘写数据的效率不高，因此磁盘是按 页 或者 扇区 来写数据的，而且还要进行磁盘寻道，也就是说要找到写的位置，这些都需要花时间。

所以使用 O_SYNC 标记写数据时要批量写，不要每次只写几个字节。

- 缓冲同步

这是最推荐的方法，项目中也是使用这种方法

这里涉及到 3 个函数：`sync`,`fsync`,`fdatasync`

(1) sync(void): 将所有修改过的块缓冲区排入写队列，然后立即返回，不等待实际写磁盘操作。但是数据是否写入磁盘并没有保障

(2) fsync(int fd): 将 fd 对应文件的缓冲区理解写入磁盘，并将等待实际写磁盘操作结束后返回。可用于数据库这样的应用程序，因为这种应用程序需要确保修改过的数据理解写到磁盘上。

(3) fdatasync(int fd): 类似于 fsync，但只影响文件的数据部分。除数据外，fsync 还会同步更新文件属性（如文件大小，文件访问时间等。文件属性和文件内容是分开存储的，写磁盘会涉及 2 次寻道）。所以 `fdatasync`比 `fsync` 速度更快。

我们采取的方法是：

调用多次 write 函数，在调用 1 次 fsync 函数，因为频繁调用 fsync 函数效率会很低。

如果文件很大，就都写完，然后调用 1 次 fsync 函数

还有如果整个文件需要调用 write 函数 10 次才能写完，那么每写 1 次，就调用 fsync 函数 1 次意义就不大， 所以应该写 10 次后，再调用 fsync 函数 1 次。

> 本项目中写日志使用 write 系统调用，工作没有问题，当时还尝试了使用 fwrite 来写日志，就会出现日志混乱问题。

#### fwrite 和 write 有什么区别

read write 这类函数时属于 **系统调用**，

而 fwrite printf 属于标准 IO 库里面的函数，**内部实现有缓冲区的，此时写日志可能就要用到锁机制**。

#### fwrite 实现原理

当调用 fwrite 函数的时候，写入的内容会被放入一个系统的 CLib 缓冲区中（可以理解成 stdio 这个库里面提供的缓冲区）。当 CLib 缓冲区满之后，会将内容移至内核缓冲区。所以这里相当于在应用程序和内核之间加了一层。所以 IO 库函数相当于一层用于缓存，最终还是调用底层 IO，也就是系统调用来实现相关功能。

所谓的缓存就是内存，用于在输入输出设备和 CPU 之间临时保存数据，使低速输入输出设备和高速输入输出设备能够协调工作，避免低速的输入输出设备占用 CPU，解放出 CPU，使其能够高效工作。

> [参考](https://zhuanlan.zhihu.com/p/269247362)

### 说下你的日志系统的运行机制

步骤：

1：单例模式（局部静态变量懒汉方法）获取实例

2：主程序一开始Log::get_instance()->init()初始化实例。初始化后：服务器启动按当前时刻创建日志（前缀为时间，后缀为自定义log文件名，并记录创建日志的时间day和行数count）。如果是异步(通过是否设置队列大小判断是否异步，0为同步)，工作线程将要写的内容放进阻塞队列，还创建了写线程用于在阻塞队列里取出一个内容(指针)，写入日志。

3：其他功能模块调用write_log()函数写日志。（write_log：实现日志分级、分文件、按天分类，超行分类的格式化输出内容。）里面会根据异步、同步实现不同的写方式。

问题1.1：阻塞队列是什么？

本项目将生产者-消费者模型封装为阻塞队列，使用循环数组实现队列，作为两者共享的缓冲区。

### 日志系统使用了单例模式

[手写一个单例模式](/算法/我要进大厂/其他?id=写一个单例模式)

单例模式的目的就是，用户在调用该类的时候，只能使用建立一个该类对象。于是呢就把该类的构造函数给私有化了，这样外部就根本没办法直接实例化调用该类，只能类内部调用。这时候就在类内部创建一个公有化的函数，然后让该函数返回一个该类的指针，这样外部就可以通过这个函数调用该类了。但是问题是，调用该成员函数必须实例化一个该类对象，而现在已经不能实例化该类对象了，所以为了可以成功调用该函数，把该函数设置为静态函数，静态函数的作用范围是全局整个文件，这样外部就可以调用了。

但是这样的话实际上并不能保证主函数调用时该类对象指针的唯一性，因为该静态成员函数每次返回的都是一个新的new出来的值。每次都不一样。于是办法就是设置一个私有化的静态对象指针，在外部初始化这个指针为空。在静态成员函数中，如果该静态指针为空就创建对象指针，否则直接返回对象指针。这样就确保了在外部使用时，该对象的唯一性。

但是问题又来了，这样做可能会导致内存泄漏，因为你静态成员函数申请的指针并没有释放，还需要用户手动释放。改动的话就把该静态成员函数中的创建指针改为创建一个静态对象成员，然后返回该成员的地址。

但是返回一个地址就需要用指针去接收，用户就有可能对该指针进行delete造成错误，所以直接静态成员函数返回一个引用更好。这样delete就会无效。

又但是引用之后，主函数可以通过赋值号产生新的类对象。突破了唯一性的设定。所以现在又需要对拷贝构造函数进行私有化设置。或者直接对拷贝构造=delete，进行禁用。又或者把默认的运算符重载给禁用了。

```cpp
#include<iostream>
#include<stdlib.h>
#include<algorithm>
#include<vector>
#include<string>
using namespace std;

class Singleton{
public:
	~Singleton(){
	printf("~Singleton() destruct");
	}
	static Singleton& CreateObject()
	{
		static Singleton obj;
		return obj;
	}
	
	//Singleton(Singleton& obj=delete;编译不能通过，但事实也可以
private:
	Singleton(){
	printf("Singleton() Construct");
	}
	Singleton(Singleton& obj){
	printf("Singleton(Singleton& obj) Construct");
	}
};
int main(){
	Singleton& p0bj1=Singleton::CreateObject();

	return 0;
    }

```

为什么要异步？和同步的区别是什么？
因为同步日志的，日志写入函数与工作线程串行执行，由于涉及到I/O操作，在单条日志比较大的时候，同步模式会阻塞整个处理流程，服务器所能处理的并发能力将有所下降，尤其是在峰值的时候，写日志可能成为系统的瓶颈。

而异步日志采用生产者-消费者模型，工作线程将所写的日志内容先存入缓冲区，写线程从缓冲区中取出内容，写入日志。并发能力比较高。

（工作线程就是生产者，写线程是消费者）

2.1缓冲区用什么实现？

本项目将生产者-消费者模型进行了封装，使用循环数组实现队列，作为两者共享的缓冲区。

2.2：什么是生产者消费者模式？

某个模块负责产生数据，这些数据由另一个模块来负责处理（此处的模块是广义的，可以是类、函数、线程、进程等）。产生数据的模块，就形象地称为生产者；而处理数据的模块，就称为消费者。

单单抽象出生产者和消费者，还够不上是生产者／消费者模式。该模式还需要有一个缓冲区处于生产者和消费者之间，作为一个中介。生产者把数据放入缓冲区，而消费者从缓冲区取出数据。大概的结构如下图。

![](https://pic2.zhimg.com/80/v2-d17e83ec46c199e61e1af7c50f6f5405_1440w.jpg)

同一个机器：使用观察者模式（有的叫发布订阅模式）

但是多机器，借助redis数据库的消息队列的发布订阅模式。实现分布式日志系统。

**26、你的项目中用到哪些设计模式、**

单例模式

**27、懒汉模式和饿汉模式具体怎么实现**

看书的最后一页。

单例模式要两次加锁的原因是，如果针对多线程，两个线程同时判断出指针为空，就会同时创建两个单例的指针，判断一次之后进行加锁后再创建实例，就可以避免有另一个同时判断成功也迅速创建了实例，因为加锁后另一个进程不能再进入。

**28、单例模式会带来哪些问题？**

单例模式一般没有接口，扩展困难。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。

在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。

单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。



**29、有用过动态规划来解决实际问题吗？说一下它的思想？**

动态规划最初是为了解决递归重复计算导致超时的问题，后来为了防止多次重复计算，引入了记忆化递归，但是记忆化递归仍然效率不是特别高，在记忆化递归的基础上改进后，形成了自上而下的动态规划方法。



---------------------------------

## 线程池相关

### 线程的同步机制有哪些

### 线程同步的方式

实现线程间同步的方法：

**互斥量，自旋锁，读写锁，条件变量**

- **互斥量**：比如说有两个线程，线程 1 和线程 2，分别充当生产者与消费者的角色，那么这两个线程就很有可能同时去操作临界资源，如果同时去操作临界资源的话就会引起线程同步问题，互斥量的话就是来解决这个问题，当一个线程，比如说线程 1 在操作临界资源的时候，它就会阻止另外的线程去访问这个临界资源。其实引发线程同步问题的最根本原因是**这两个线程的指令是交叉执行的**，互斥量能够保证指令执行的原子性，也就是说先执行完线程 1 的指令再执行线程2的指令，或者先执行完线程 2 的指令再执行线程1的指令。保证他们之间不会出现交叉执行的情况。互斥量也称为**互斥锁**，它要么处于加锁状态要么处于解锁状态。保证资源访问的串行。操作系统提供的 API 是 `pthread_mutex_t`。

- **自旋锁**：其实自旋锁和互斥锁的原理是一样的，都是在使用临界资源之前加一个锁，阻止其他线程对它进行访问，完成之后再把锁给释放掉，保证临界资源的串行访问。但是它和互斥锁还是存在差别的，使用自旋锁的线程会一直循环反复检查锁的变量是否可用，因此**它不会让出CPU**，会处于忙等待的状态。其实自旋锁还是有很多好处的，它避免了进程或者线程上下文切换的开销，如果锁使用的时间不是很长的话，使用自旋锁的代价也是很小的，同时在操作系统内部很多地方使用的是自旋锁而不是互斥量的。这里还要提一点就是**自旋锁不适合在单核`CPU`中使用**。因为自旋锁在等待的时候并不会释放`CPU`，而是死循环地去等待。会引起其他的进程或者线程无法去执行。操作系统提供的API是`pthread_spinock_t`。

- **读写锁**: 读写锁和互斥锁还有自旋锁是类似的，但是做了一些改进，基于临界资源的考量，因为在开发环境中，临界资源很可能会出现多读少写的特性，就比如有一个数据库存储的是历史订单信息，而这些订单我们一般只是去查询很少去改变它，这个存储历史订单的数据库就属于多读少写的临界资源，如果在读写的时候也给它加锁，这样的话效率会很低的。读写锁的话是一种特殊的自旋锁，**它允许多个读者同时读取临界资源，但是不允许多个写操作同时访问这个资源**。在操作系统中提供的API是`thread_rwlock_t`，读锁是通过`thread_rwlock_rdlock`来加的，写锁是通过`thread_rwlock_wdlock`来加的.

- **条件变量**：条件变量是一种先对复杂的线程同步方法，它允许线程睡眠，在满足一定条件的时候再唤醒线程，就是当满足条件时，可以向这个线程发送信号，唤醒这个线程。因为在生产者和消费者模型中是存在问题的，举个例子，比如当缓冲区小于或者等于 0 时，这时候应该不允许消费者继续消费，消费者必须等待，当缓冲区满的时候，这个时候应该不允许生产者往里面生成数据了，生产者必须处于等待状态。条件变量呢就是对这个问题进行了约束，当缓冲区为 0 的时候，如果有生产者生产一个产品，那么就要唤醒可能等待的消费者；当缓冲区满的时候，如果有消费者消费了产品，就需要唤醒其他可能在等待的生产者。操作系统提供的`API`是`pthread_cont_t`来定义的,等待是通过`pthread_cont_wait`定义的，,唤醒是通过`pthread_cont_notify`定义的。

> 互斥量：保证只有一个进程去操作 临界资源，同步问题是因为两个指令交叉执行，所以需要保证原子性。   
> 自旋锁：加锁，保证串行执行，但是不会让出 CPU ，反复检查锁是是否可用，     
> 读写锁：考虑临界资源多读少写，允许多读不允许多写      
> 条件变量：适当的时候唤醒线程，缓冲区为 0 消费者等待，缓冲区满了，生产者等待


### 手写线程池

C++实现的简易线程池，包含线程数量，启动标志位，线程列表以及条件变量。

其中**构造函数**主要是声明未启动和线程数量的。start 函数为启动线程池，将 num 个线程绑定 threadfunc 自定义函数并执行，加入线程列表。stop 是暂时停止线程，并由条件变量通知所有线程。**析构函数**是停止，阻塞所有线程并将其从线程列表剔除后删除，清空线程列表。

```cpp
#include<vector>
#include<string>
#include<list>
#include<thread>
#include<condition_variable>
using namespace std;
class ThreadPool {
public:
	ThreadPool(int threadnum):started(false),thread_num(threadnum) {

}
~ThreadPool()
{
	stop();
	for (int i = 0; i < thread_num; ++i) {
		threadlist[i]->join();
	}
	for (int i = 0; i < thread_num; ++i) {
		delete threadlist[i];
	}
	threadlist.clear();
}
void threadFunc(){}//线程执行函数,可自定义。
int getThreadNum() { return thread_num; }
void start() {
	if (thread_num > 0) {
		started = true;
		for (int i = 0; i < thread_num; ++i) {
			thread* pthread = new thread(&threadFunc, this);
			threadlist.push_back(pthread);
		}	
	}
}
void stop() {
	started = false;
	condition.notify_all();
}

private:
	int thread_num;
	bool started;
	vector<thread*> threadlist;
	condition_variable condition;
};

```



#### 线程池中的工作线程是一直等待吗？

线程池中的工作线程是处于一直阻塞等待的模式下的。因为在我们创建线程池之初时，我们通过循环调用 pthread_create 往线程池中创建了8个工作线程，工作线程处理函数接口为pthread_create函数原型中第三个参数函数指针所指向的worker函数（自定义的函数），然后调用线程池类成员函数run（自定义）。

-------这里可能会有疑问？为什么不直接将第三个参数直接指向run函数，而是要通过向worker中传入对象从而调用run呢？原因是因为我们已经将worker设置为静态成员函数，而我们都知道静态成员函数只能访问静态成员变量，所以为了能够访问到类内非静态成员变量，我们可以通过在worker中调用run这个非静态成员变量来达到这一要求。在run函数中，我们为了能够处理高并发的问题，将线程池中的工作线程都设置为阻塞等待在请求队列是否不为空的条件上，因此项目中线程池中的工作线程是处于一直阻塞等待的模式下的。

#### 你的线程池工作线程处理完一个任务后的状态是什么

这里要分两种情况考虑

（1） 当处理完任务后如果请求队列为空时，则这个线程重新回到阻塞等待的状态

（2） 当处理完任务后如果请求队列不为空时，那么这个线程将处于与其他线程竞争资源的状态，谁获得锁谁就获得了处理事件的资格。

#### 如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢

首先这种问法就相当于问服务器如何处理高并发的问题。

本项目中是通过对子线程循环调用来解决高并发的问题的。

**具体实现过程如下**：

我们在创建线程的同时时就调用pthread_detach将线程进行分离，这样就不用单独对工作线程进行回收，但是一般情况只要我们设置了分离属性，那么这个线程在处理完任务之后，也就是子线程结束后，资源会被自动回收。那这种情况下我们服务器基本就只能处理8个请求事件了（线程池里只有8个线程）。那怎么实现高并发的请求呢？可能会说让线程池里创建足够多的线程数，这当然是理想化的，现实中线程数量过大会导致更多的线程上下文切换，占用更多内存，这显然是不合理的。

> 接下来所叙述的就是本项目中用来处理高并发问题的方法了

我们知道调用了 pthread_detach 的线程只有等到他结束时系统才会回收他的资源，那么我们就可以从这里下手了。我们通过子线程的run调用函数进行 while 循环，让每一个线程池中的线程永远都不会终止，说白了就是让他处理完当前任务就去处理下一个，没有任务就一直阻塞在那里等待。这样就能达到服务器高并发的要求，同一时刻8个线程都在处理请求任务，处理完之后接着处理，直到请求队列为空表示任务全部处理完成。

#### 如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢

会影响接下来的客户请求，因为线程池内线程的数量时有限的，如果客户请求占用线程时间过久的话会影响到处理请求的效率，当请求处理过慢时会造成后续接受的请求只能在请求队列中等待被处理，从而影响接下来的客户请求。

**应对策略**：

我们可以为线程处理请求对象设置处理超时时间, 超过时间先发送信号告知线程处理超时，然后设定一个时间间隔再次检测，若此时这个请求还占用线程则直接将其断开连接。


### 多线程理解


如果要让服务器服务多个客户端，最简单的方式就是一个连接创建一个线程，但是这样不停的创建和销毁线程，会造成性能能开销和资源浪费。

我们可以使用 「资源复用」的方式解决这个问题，就是创建一个「线程池」，将连接分配给线程，然后一个线程可以处理多个连接的业务。

> 线程怎样才能高效地处理多个连接的业务?

当一个连接对应一个线程时，线程一般采用「read -> 业务处理 -> send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 read 操作上（ socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。

但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 read 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。

要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 read 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。

上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 read 去试探。

### 阻塞与非阻塞

- 阻塞 IO

阻塞：就是调用一个函数，该函数就卡在这里，整个程序流程不会往下走了（此时程序进入休眠状态）。这个函数等待一个时间发生，只有这个事件发生了，程序才会继续玩下走（也就是程序才会继续运行）

这种函数就是 **阻塞函数**，比如服务器使用 accept 函数，调用 accept 时，程序执行流程就卡在 accept 这里，等待客户端连接，只有客户端连接，三次握手成功，accept 才会返回。

- 非阻塞 IO

非阻塞 IO 和 阻塞 IO 是相对的，就比如说刚刚说到的 accept ，如果通过调用某个函数，把监听套接字设置成非阻塞，那么调用 accept 的时候，就算没有客户端连接，这个 accept 调用也不会卡住，会立即返回（当然返回时会有个错误码，我们可以根据这个错误码判断 accept 返回的原因），这样就能充分利用操作系统给进程分配的时间片来做别的事情，执行效率就更高了。

### 异步和同步

- 异步 IO

调用一个异步 IO 函数接收数据时，不管有没有数据，该函数都会立即返回。我们在调用异步 IO 函数时要指定一个接受数据的缓冲区，还要指定一个回调函数，其他的事情操作系统去做了，程序可以自由地干其他事情。


> **非阻塞 IO 和 异步 IO 的差别**？
> - 非阻塞 IO 要不停地调用 IO 函数检查数据是否到来，如果数据到来了，就卡在 IO 函数这里把数据从内核缓冲区复制到用户缓冲区，然后这个 IO 函数才能返回
> - 异步 IO 不需要不停地调用 IO 函数检查数据是否到来，只需要调用 1 次，然后就去做其他事情了，由内核检查数据的到来，内核负责把数据复制到指定缓冲区，整个过程进程并没有被卡住

- 同步 IO

调用一个同步 IO 函数接受数据时，在没有得到结果之前，这个调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。同步 IO 需要调用 2 个函数才能取到数据，它的优点就是得到了所谓的 IO 复用的能力。

> 调用 1 个函数就能判断一批 TCP 连接是否有数据到的能力，就是 IO 复用

###  I/O 多路复用

使用 I/O 多路复用，实现当连接上有数据的时候，线程才去发起读请求。

> select/poll/epoll 是如何获取网络事件的呢?

- 如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮训调用 read 操作来判断是否有数据。
- 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。


-----

> [参考](https://blog.csdn.net/qq_34827674/article/details/116175772?spm=1001.2014.3001.5501)



### 怎么实现非阻塞 socket

- 使用 `ioct1()`函数，第二个参数和第三个参数可以 设置 或 清除 非阻塞I/O标记：0：清除，1：设置
- 调用`fcntl()` 函数把套接口描述符设置成非阻塞



```cpp
//设置socket连接为非阻塞模式【这种函数的写法很固定】
bool CSocekt::setnonblocking(int sockfd) 
{    
    int nb=1; //0：清除，1：设置  
    if(ioctl(sockfd, FIONBIO, &nb) == -1) //FIONBIO：设置 或 清除 非阻塞I/O标记：0：清除，1：设置
    {
        return false;
    }
    return true;

    //如下也是一种写法，跟上边这种写法其实是一样的，但上边的写法更简单
    /* 
    //fcntl:file control【文件控制】相关函数，执行各种描述符控制操作
    //参数1：所要设置的描述符，这里是套接字【也是描述符的一种】
    int opts = fcntl(sockfd, F_GETFL);  //用F_GETFL先获取描述符的一些标志信息
    if(opts < 0) 
    {
        ngx_log_stderr(errno,"CSocekt::setnonblocking()中fcntl(F_GETFL)失败.");
        return false;
    }
    opts |= O_NONBLOCK; //把非阻塞标记加到原来的标记上，标记这是个非阻塞套接字【如何关闭非阻塞呢？opts &= ~O_NONBLOCK,然后再F_SETFL一下即可】
    if(fcntl(sockfd, F_SETFL, opts) < 0) 
    {
        ngx_log_stderr(errno,"CSocekt::setnonblocking()中fcntl(F_SETFL)失败.");
        return false;
    }
    return true;
    */
}
```

## Epoll技术简介

[网络编程](/寻offer总结/计算机网络/网络编程相关)

以了一位网友【[王博靖](https://github.com/wangbojing/NtyTcp)】自己写的一套 Epoll 源码为入口，通过阅读源码学习了 Epoll 函数内部的实现原理。


**Epoll 就是一种在 Linux 上使用的 IO 多路复用并支持高并发的典型技术**。

比如说有 10 万个并发连接（也就是同一时刻有 10 万个客户端保持和服务器的连接），这 10 万个连接通常也不可能同一时刻都在收发数据，一般在**同一时刻通常只有其中几十个或者几百个连接在收发数据，其他连接可能处于只连接而没有收发数据的状态**。

如果以 100ms 为间隔判断一次，可能这 100ms 内只有 100 个活跃连接（就是有数据收发的连接），把这 100 个活跃连接的数据放在一个专门的地方，后续到这个专门的地方来，只需要处理 100 条数据，处理起来的压力就没那么大了。

这也就是 Epoll 的处理方式。而 select 和 poll 是依次判断这 10w 个连接有没有收发数据（可能实际上有数据的只有 100 个连接），有数据就处理。所以不难看出每次检查 10w 个连接与每次检查 100 个连接相比，浪费了巨大的资源和时间。

> 实际上，`epoll` 在内核里使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的 `socket` 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 `select/poll` 每次操作时都传入整个 `socket` 集合，只需要传入一个待检测的 `socket` 就可以了，减少了内核和用户空间大量的数据拷贝和内存分配。


此外 Epoll 采用了 **事件驱动机制**，只在单独的进程或者线程里收集和处理各种事件，没有进程或线程之间上下文切换的开销。

> 也就是说，在内核中维护了一个「链表」来记录就绪事件，当某个 `socket` 有事件发生时候，通过回调函数，内核会将这个 事件 加入到 就绪事件 列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件的 socket 文件描述符，不需要像 `select/poll` 那样轮询扫描整个` socket` 集合，大大提高了检测的效率。

`epoll` 通过两个方面，很好解决了 `select/poll` 的问题。

从下图你可以看到 `epoll` 相关的接口作用：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epoll01.58ud4l3nxm00.png)


源码中 `nty_epoll_rb.c` 和 `nty_epoll_inner.h` 这 2 个文件是 Epoll 相关的 3 个函数的实现文件。

### epoll_create 函数

- **格式**

```c
int epoll_create(int size);  // size 必须 > 0
```

- **功能**：创建一个 Epoll 对象，返回一个对象文件描述符来表示这个 Epoll 对象，后续通过操作这个描述符来进行数据的收发。

这个对象最终要用 close 关闭，因为它是个描述符，或者说是个句柄，总是要关闭的，

- **原理**

执行 `struct eventpoll *ep` 生成一个 `eventpoll` 对象

```c
struct eventpoll *ep = (struct eventpoll*)calloc(1, sizeof(struct eventpoll)); 
```

`eventpoll` 的结构如下。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create01.ik3e4xapf3k.png)

`eventpoll` 的结构中有两个比较重要的成员

(1) `rbr`,可以理解成代表一颗红黑树的根节点（的指针）。

红黑树是一种高效的数据结构，用于保存数据，一般都是存“键值对（`key-value`）”，红黑树的特点是能够快速地根据给的 key 找到并取出 value ，这里的 key 一般是一个数字，而 value 代表的可能是一批数据。**红黑树查找的时间复杂度**是：`O(logn)`

一开始的时候红黑树还是空的，也就是 rbr 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create02.5zdre53dgpo0.png)

(2) `rdlist`，可以理解成代表一个双向链表的表头指针

双向链表能快速顺序地访问里面的节点。

一开始的时候双向链表也是空的，`rdlist` 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create03.5zueg4dj5yg0.png)

- **总结**：
  - 创建一个 eventpoll 结构的对象，被系统保存起来
  - 对象中的 rbr 成员被初始化成指向一颗红黑树的根节点，
  - 对象中的 rdlist 成员被初始化成指向一个双向链表的头结点。

### epoll_ctl 函数

- **格式**：

```c
int epoll_ctl (int efpd,int op,int sockid,struct epoll_event *event);
```

- **功能**：

把一个 socket 以及 socket 相关的事件添加到 epoll 对象描述符中，以通过这个 epoll 对象监视该 socket（也就是这个 tcp 连接）上数据的来往情况，当有数据来往时，系统会通知程序。

我们可以通过 `epoll_ctl` 函数吧程序中需要关注的事件添加到 epoll 对象描述符中，当有数据来往时，系统会通知程序。

**epoll_ctl 函数中参数的介绍**：

- `efpd`：`epoll_create()`返回的`epoll`对象描述符
- `op`：一个操作类型，添加/删除/修改 ，对应数字是`1,2,3`. 分别对应： `EPOLL_CTL_ADD`（添加事件）, `EPOLL_CTL_DEL`（删除事件）， `,EPOLL_CTL_MOD`（修改事件）
- `sockid`：表示一个 TCP 连接，添加事件（也就是往红黑树中添加节点）时，就是用 sockid 作为 key 往红黑树中增加节点的。
- `event`: 向 `epoll_ctl` 函数传递信息，比如要增加一些书剑，就可以通过 `event` 参数将具体事件传递进 `epoll_ctl` 函数。

- **原理**：

假如传递进来的是一个 `EPOLL_CTL_MOD` ,首先使用 `RB_FIND` 来查找红黑树上是否已经有了这个节点，如果有了，程序就直接返回，如果没有，程序流程就继续往下走。

>  **EPOLL_CTL_ADD 怎么往红黑树你增加节点**

**确定红黑树没有该节点**的情况下，会生成一个 epitem 对象。

通过执行下面代码创建 `epitem` 对象

```cpp
epi = (struct epitem*)calloc(1, sizeof(struct epitem));
```

这个对象就是后续增加到红黑树中的一个节点，该节点的 key 保存在 sockfd 中，要增加的事件保存在 event 中，然后使用 `RB_INSERT` 宏将该节点插入红黑树中，对于红黑树来说,每个节点都要记录自己的左子树、右子树和父节点，这里是通过 rbn 成员，指向父节点和子节点的。如果将来多个用户连入服务器，需要向这颗红黑树加入很多节点，这些节点彼此也要连接起来。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_ctl01.7d2r4etg4d40.png)

> EPOLL_CTL_ADD：等价于往红黑树中增加节点

> EPOLL_CTL_DEL：等价于从红黑树中删除节点

> EPOLL_CTL_MOD：等价于修改已有的红黑树的节点

**每一个连入客户端都应该调用 `epoll_ctl` 向红黑树增加一个红黑树节点**，如果有 100w 个并发连接，红黑树上就会有个 100w 个节点

### epoll_wait 函数

- **格式**：

```c
int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout);
```

- **功能**：

阻塞一小段时间并等待事件发生，返回事件集合，也就是获取内核的事件通知；

其实就是遍历这个双向链表，把这个双向链表里边的节点数据拷贝出去，拷贝完毕的就从双向链表里移除；因为所有数据的 socket（ TCP 连接）都在双链表里记着。

- 参数`epfd`：是`epoll_create()`返回的`epoll`对象描述符

- 参数`events`：是内存，也是数组，长度 是`maxevents`，表示此次`epoll_wait`调用可以收集到的`maxevents`个已经就绪【已经准备好的】的读写事件；换句话说返回的是有事件发生的 TCP 连接数目

- 参数`timeout`：阻塞等待的时长；

> 总的来说，epoll_wait 函数就是到双链表中去，把此刻同时连入的连接中有事件发生的连接拿出来，后续 read，write，或者 send，secv 之类的函数调用收到数据，某个 socket 只要在双链表中，这个 socket 上一定发生了 某个/某些 事件，也就是说，只有发生了某个/某些 事件的 socket 才会在双向链表中实现。

> 这也就是 epoll 高效的原因，因为 epoll 每次值遍历发生事件的一小部分 socket 连接（这些 socket 都在这个双向链表中），而不用到全部 socket 连接中逐个遍历以判断事件是否到来。

#### 关于 epitem 节点

【【关于 epitem 结构 晚点在补充】】



## 向内核双链表增加节点

epoll_wait 函数实际上就是去双向链表，那么，**操作系统什么时候向双向链表中插入节点呢**？

- 客户端完成三次握手时，操作系统会向双向链表插入节点，这时服务器往往要调用 accept 函数把该连接从已完成连接队列中取走

- 当客户端发送来数据时，操作系统会向双向链表插入节点，这时服务器也要调用 close 关闭对应的 socket

- 当客户端发送数据时，操作系统会向双向链表插入节点，这时服务器要调用 read 或者 recv 来收数据

- 当可以发送数据时，操作系统会向双向链表插入节点，这时服务器可以调用 send 或者 write 向客户端发送数据。可以这样理解：如果客户端接收话剧慢，服务器发送数据快，那么服务器就得等客户端收完一批数据后才能再发下一批。

## 使用 epoll 函数来实现数据的收发

nginx 源码中的 `ngx_c_socket.cc` 中的 `CSocekt::ngx_epoll_init()`

对 epoll 功能初始化，在子进程中进行 ，这个函数被`ngx_worker_process_init()`所调用.

**实现逻辑**

- 首先调用 epoll_create 函数创建一个 epoll 对象，也就是创建了一个红黑树，还创建了一个双向链表，直接以 epoll 连接的最大项数为参数

```cpp
m_epollhandle = epoll_create(m_worker_connections); 
```

- 接着创建一个连接池

```cpp
 m_pconnections = new ngx_connection_t[m_connection_n]  //m_connection_n 连接池的大小
```
### 创建连接池的目的

目前项目有 2 个监听套接字，以后客户端连入后，每个用户还会产生 1 个套接字，套接字本身只是一个数字，但往往需要保存很多与这个数字相关的信息，这就需要把套接字数字本身与一块内存捆绑起来。所以，引入连接池的目的就是把套接字与连接池中的某个元素捆绑起来，将来就可以通过套接字取得连接池中的元素（内存），一遍读写其中的数据。

### ET 和 LT 模式

#### LT 模式-水平触发

epoll 默认采用的是 LT 模式，只有使用 EPOLLET 参数才会使用 ET（边缘触发）

发生一个事件，如果程序不处理，那么这个事件就一直被触发，具体地说，就是一个新用户连入后，如果程序不调用 accept4 或者 accept 函数将这个用户接入（从已完成连接队列中取出来），使用 epoll_wait 函数获取事件时，就每次都能获取到用户连入的事件通知，也就是 EPOLLIN 事件，显然，这种触发方式效率不高。

#### ET 模式-边缘触发

这种触发只是对非阻塞 socket 有用，因为项目中用的都是非阻塞 socket，所以可以使用边缘触发模式，发生一个事件，内核只会通知程序 1 次，如果一个新用户连入，内核通知程序 1 次，程序必须使用 accept4 或者 accept 将这个新用户接入，如果这次没接进来就麻烦了，因为内核不会再次通知程序。因为边缘触发这种模式减少了通知的次数，所以效率更高。

目前的代码中，这几个监听套接字，在调用 epoll_ctl 增加事件的时候用的都是默认的 LT 模式，这样就能保证不丢失客户端的连接，因为内核会反复通知程序。

对于接入的 socket 连接（accept4 或者 accept 返回的 scoket 连接），程序中用了 ET 模式，从而提高程序工作效率。

### 事件驱动

事件驱动架构，就比如说客户端连入，三次握手完成，只要服务器注册了获取读事件，内核就会通知服务器，这就产生了一个事件，这里的事件发生源是客户端，通过事件收集器来收集和分发事件（这里的事件收集就是 epoll_wait 函数）。然后比如`CSocket::ngx_event_accept`这些函数就是事件处理函数，服务器准备用这些函数来处理或者消费事件。

> 注意：每个事件消费者（处理函数）都不能有阻塞行为，否则整个执行通道就会堵塞了。

### 腾讯面试题

> 使用 linux epoll 模型，水平触发模式，当 socket 可写时，会不停地触发 socket 可写事件，如何处理？

- 第一种方式

需要向 socket 写数据的时候才把 socket 可写事件通知加入 epoll 的红黑树节点，等待可写事件。当程序接受到来自系统的可写事件通知后，调用 write 或者 send 发送数据。所有数据都发送完毕后，把 socket 可写事件通知从 epoll 的红黑树节点中移除（移除的是可写事件通知，而不是红黑树节点）

这种方式的缺点：即使发送很少的数据，也要把 socket 可写事件通知加入 epoll 红黑树节点，写完后再把可写事件通知从 epoll 红黑树节点中删除，有一点的操作代价。

- 第二种方式

开始不把 socket 可写通知事件加入 epoll 的红黑树节点，需要发送数据时，直接调用 write 或者 send 发送，如果 write 或者 send 返回 EAGIN（缓冲区满了，需要等待可写事件才能继续往发送缓冲区写数据），再把 socket 的写事件通知加入 epoll 的红黑树节点。这就变成了在 epoll 的驱动下发送数据，全部数据发送完毕后，再把可写事件通知从 epoll 红黑树节点中删除

这种方式的优点是：数据不多的时候避免 epoll 的红黑树节点中针对写事件通知的增删，提高了程序执行效率。

### 深入理解ET LT

- LT 是水平触发，属于低速模式，如果事件还没处理完，就会被一直触发
- ET 是边缘触发，属于高速模式，这个事件的通知只会出现一次

### Epoll 中 ET 和 LT 模式的处理编码不同

如果发送来了数据，一个读事件就会被内核放到双向链表，如果我们不使用 recv 来接受数据或者只使用 recv 接受了部分数据，也就是说 TCP 连接的接受缓冲区中还有数据没有接受完

在 LT 模式下，内核就不会把这个读事件的节点从双向链表中删除，这样每次程序调用 epoll_wait 都能获取通知。

ET 模式不一样，不管我们是否调用 recv 来接受数据，一旦从双向链表中把读事件对应的节点取走，内核肯定把这个节点从双向链表中删除了，所以下次用 epoll_wait 去取事件时取不到的，除非后续客户端又发来了数据，内核会再次向这个双向链表中添加一个读事件的节点，程序使用 epoll_wait 才能再次收到读事件。


一般来讲，本项目的服务器程序，如果收发的数据包后固定格式，都建议采用 LT 模式--编程简单，清晰，写好了效率上估计也不会很差。

如果收发数据包没有固定格式，可以考虑采用 ET 模式，反复收数据，收完为止，编程难度较大，但是效率会高一些。再浏览器反问一个 web 服务器页面时，发送的数据就可能没有固定格式，浏览器一次可能向 web 服务器发送一大批数据，然后等 web 服务器回应。所以 nginx 采用的是 ET 模式。





## 什么是 Reactor 模式

基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写。这就是 **Reactor 模式**

也就是来了一个事件，Reactor 就有相对应的 「反应/响应」。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：
- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有一个，也可以有多个；
- 处理资源池可以是单个 进程 / 线程，也可以是多个 进程 /线程；

> 其中，「多 Reactor 单进程 / 线程」实现方案相比「单 Reactor 单进程 / 线程」方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。

> Nginx 使用的是进程

#### 常见的 Reactor 实现方案

- 第一种方案单 Reactor 单进程 / 线程，不用考虑进程间通信以及数据同步的问题，因此实现起来比较简单，这种方案的缺陷在于无法充分利用多核 CPU，而且处理业务逻辑的时间不能太长，否则会延迟响应，所以不适用于计算机密集型的场景，适用于业务处理快速的场景，比如 Redis 采用的是单 Reactor 单进程的方案。

- 第二种方案单 Reactor 多线程，通过多线程的方式解决了方案一的缺陷，但它离高并发还差一点距离，差在只有一个 Reactor 对象来承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。

- 第三种方案多 Reactor 多进程 / 线程，通过多个 Reactor 来解决了方案二的缺陷，主 Reactor 只负责监听事件，响应事件的工作交给了从 Reactor，Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案，Nginx 则采用了类似于 「多 Reactor 多进程」的方案。

Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。

> Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。

### 什么是 Proactor

前面提到的 Reactor 是非阻塞同步网络模式，而 Proactor 是异步网络模式

Proactor 采用了异步 I/O 技术，所以被称为异步网络模型。

#### 理解 Reactor 和 Proactor 的区别

- Reactor 是非阻塞**同步网络**模式，感知的是就绪可读写事件。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- Proactor 是**异步网络**模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。
- 因此，Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。

无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件。

### 同步I/O模型的工作流程

同步I/O模型的工作流程如下（`epoll_wait`为例）：

主线程往`epoll`内核事件表注册`socket`上的读就绪事件。

主线程调用`epoll_wait`等待`socket`上有数据可读

当`socket`上有数据可读，`epoll_wait`通知主线程,主线程从`socket`循环读取数据，直到没有更多数据可读，然后将读取到的数据封装成一个请求对象并插入请求队列。

睡眠在请求队列上某个工作线程被唤醒，它获得请求对象并处理客户请求，然后往`epoll`内核事件表中注册该`socke`t上的写就绪事件

主线程调用`epoll_wait`等待`socket`可写。

当`socket`上有数据可写，`epoll_wait`通知主线程。主线程往`socket`上写入服务器处理客户请求的结果。

读就绪事件：当有事件到来，epoll_wait()单纯通知主线程有事件来了，主线程把事件放入请求队列。应用程序利用工作线程通过read（）等函数把数据从内核缓冲区读到用户缓冲区。

读完成事件：有事件来了，主线程往内核注册这个读时间（就是告诉内核注意了一会要读数据）。注册了之后，主线程就去干其他事情，内核就自动会负责将数据从内核缓冲区放到用户缓冲区。不用用户程序管。

而对于用reactor模式模拟的的proactor模式来说，之前proactor是用主线程调用aio_read函数向内核注册读事件，这里它主线程使用epoll向内核注册读事件。但是这里内核不会负责将数据从内核读到用户缓冲区，最后还是要靠主线程也就是用户程序`read()`函数等负责将内核数据循环读到用户缓冲区。对于工作线程来说，收到的都是已读完成的数据，模拟就体现在这里。

有人可能会问，他们都是通过主线程调用不同函数进行注册，然后一个注册之后可以直接内核负责数据从内核到用户。另一个注册之后好像没啥用，那注册还有什么用？直接主线程循环读取然后封装放请求队列不就行了么？

不对，如果数据一直没来，直接进行循环读取就会持续在这里发生阻塞，这就是同步IO的特点，所以一定要注册一下然后等通知，这样就可以避免长期阻塞等候数据。

## 定时器

由于非活跃连接占用了连接资源，严重影响服务器的性能，通过实现一个服务器定时器，处理这种非获取连接，释放连接资源。

我们要将每个定时事件分别封装成定时器，并使用某种容器类数据结构，比如链表、排序链表和时间轮，将所有定时器串联起来，以实现对定时事件的统一管理。

本项目是为了方便释放那些超时的非活动连接，关闭被占用的文件描述符，才使用定时器。

### 什么是定时事件

定时事件，是指固定一段时间之后触发某段代码，由该段代码处理一个事件。这里是删除非活动的 epoll 树上的注册事件，并关闭对应的socket，连接次数减一。

### 什么是定时器

是指利用结构体或其他形式，将多种定时事件进行封装起来。这里只涉及一种定时事件，这里将该定时事件与连接资源封装为一个定时器类。具体包括连接资源、超时时间和回调函数，这里的回调函数指向定时事件。

#### 连接资源包括什么

连接资源包括客户端套接字地址、文件描述符和定时器

#### 超时时间

超时时间=浏览器和服务器连接时刻 + 固定时间(TIMESLOT)，可以看出，定时器使用绝对时间作为超时值.

### 什么是定时器容器

项目中的定时器容器为带头尾结点的**升序双向链表**，具体的为每个连接创建一个定时器，将其添加到链表中，并按照超时时间升序排列。

#### 什么是定时任务

将超时的定时器从链表中删除。

#### 什么是定时任务处理函数？

定时任务处理函数，该函数封装在容器类中，具体的，函数遍历升序链表容器，根据超时时间，删除对应的到期的定时器，并调用回调函数（即定时事件）。

(注意：定时任务处理函数在主循环中调用)

### 说一下定时器的工作原理

服务器主循环为每一个连接创建一个定时器，并对每个连接进行定时。另外，利用升序时间链表容器将所有定时器串联起来，若主循环接收到定时通知，则在链表中依次执行定时任务处理函数。

> 怎么通知主循环？

利用alarm函数周期性地触发**SIGALRM信号**，信号处理函数利用管道通知主循环（注意，本项目信号处理函数仅仅发送信号通知程序主循环，将信号对应的处理逻辑放在程序主循环中，由主循环执行信号对应的逻辑代码。）

#### 双向链表删除和添加的时间复杂度还可以优化

|  位置   | 添加  | 删除  |
|  ----  | ----  | ----  |
| 刚好在头节点  | O(1) | O(1) |
| 刚好在尾节点  | O(n) | O(1) |
| 平均  | O(n) | O(1) |

备注：

a.添加的尾节点时间复杂度为O(n)，是因为本项目的逻辑是先从头遍历新定时器在链表的位置，如果位置恰好在最后，才插入在尾节点后，所以是O(n)。

b.删除的复杂度都是O(1)，因为这里的删除都是已知目标定时器在链表相应位置的删除。（看1.7可知，函数遍历升序链表容器，根据超时时间，删除对应的到期的定时器）

优化：

a.在双向链表的基础上优化：

添加在尾节点的时间复杂度可以优化：在添加新的定时器的时候，除了检测新定时器是否在小于头节点定时器的时间外，再先检测新定时器是否在大于尾节点定时器的时间，都不符合再使用常规插入。

b.不使用双向链表，使用**最小堆**结构可以进行优化。

#### 最小堆怎么优化

时间复杂度：

添加：O(lgn)

删除：O(1)

工作原理：

将所有定时器中超时时间最小的一个定时器的超时值作为alarm函数的定时值。这样，一旦定时任务处理函数tick()被调用，超时时间最小的定时器必然到期，我们就可以在tick 函数中处理该定时器。然后，再次从剩余的定时器中找出超时时间最小的一个（堆），并将这段最小时间设置为下一次alarm函数的定时值。如此反复，就实现了较为精确的定时。



## 压力测试

阅读 Webbench 源码，对 进程 加深理解

通过 Webbench 创建多个进程，每个进程通过 HTTP 连接访问服务器，完成压力测试。

可以实现 上万 并发连接
- 每秒钟相应请求数：552852 /min
- 每秒钟传输数据量：1031990 bytes/sec 
- 所有连接访问均成功

### Webbench实现的核心原理

- 进程fork若干个子进程，每个子进程在用户要求时间或默认的时间内对目标web循环发出实际访问请求;
- 父子进程通过管道进行通信，子进程通过管道写端向父进程传递在若干次请求访问完毕后记录到的总信息；
- 父进程通过管道读端读取子进程发来的相关信息，子进程在时间到后结束，父进程在所有子进程退出后统计并给用户显示最后的测试结果，然后退出。

  - 1.命令行解析，`getopt()`

  - 2.构造http请求报文 `build_request`

  - 3.压力测试：`bench` 函数

- 每个 `fork` 的子进程都调用 `benchcore` 函数，在要求时间内发送请求报文，改函数可记录请求的成功次数、失败次数、以及服务器回复的字节数。

## 压力测试 Bug 排查

使用 Webbench 对服务器进行压力测试，创建 1000 个客户端，并发访问服务器 10s ，正常情况下接近 8w 个请求访问服务器

结果显示请求 7 个请求被成功处理，0 个请求失败，服务器也没有返回错误，这时候尝试从浏览器访问服务器，发现这个请求也不能被处理和响应，必须将服务器重启，浏览器才能正常访问。

### 排查过程

通过查询服务器运行日志，通过日志观察猜想是 接受请求连接 部分发生了错误。

其中，服务器接收请求的连接步骤为：`socket --> bind --> listen --> accept`      
客户端请求连接的步骤为：`socket --> connect`


#### listen 

```cpp
#include<sys/socket.h>
int listen(int sockfd, int backlog)
```

- 函数功能，把一个未连接的套接字转换成一个被动套接字，指示内核应接受指向该套接字的连接请求。根据 TCP 状态转换图，调用 listen 导致套接字从 CLOSED 状态转换成 LISTEN 状态。

- backlog 是队列的长度，内核为任何一个给定的监听套接口维护两个队列：
  - **未完成连接队列**（incomplete connection queue），每个这样的 SYN 分节对应其中一项：已由某个客户发出并到达服务器，而服务器正在等待完成相应的 TCP 三次握手过程。这些套接口处于 SYN_RCVD 状态。
  - **已完成连接队列**（completed connection queue），每个已完成 TCP 三次握手过程的客户对应其中一项。这些套接口处于 ESTABLISHED `[ɪˈstæblɪʃt]` 状态。

#### connect

当有客户端主动连接（connect）服务器，Linux 内核就自动完成 TCP 三次握手，该项就从未完成连接队列移到已完成连接队列的队尾，将建立好的连接自动存储到队列中，如此重复。

#### accept

- 函数功能，从处于 ESTABLISHED 状态的连接队列头部取出一个已经完成的连接(三次握手之后)。

- 如果这个队列没有已经完成的连接，accept 函数就会阻塞，直到取出队列中已完成的用户连接为止。

- 如果，服务器不能及时调用 accept 取走队列中已完成的连接，队列满掉后，TCP 就绪队列中剩下的连接都得不到处理，同时新的连接也不会到来。

从上面的分析中可以看出，accept 如果没有将队列中的连接取完，就绪队列中剩下的连接都得不到处理，也不能接收新请求，这个特性与压力测试的 Bug 十分类似。


#### 定位 accept

分析代码发现，web端和服务器端建立连接，采用 Epoll 的 边缘触发模式 同时监听多个文件描述符。

#### Epoll的ET、LT

- LT水平触发模式

`epoll_wait` 检测到文件描述符有事件发生，则将其通知给应用程序，应用程序可以不立即处理该事件。

当下一次调用 `epoll_wait` 时，`epoll_wait` 还会再次向应用程序报告此事件，直至被处理。

- ET边缘触发模式

  - `epoll_wait` 检测到文件描述符有事件发生，则将其通知给应用程序，应用程序必须立即处理该事件。

  - 必须要一次性将数据读取完，使用非阻塞 I/O，读取到出现 eagain。

从上面的定位分析，问题可能是错误使用 epoll 的 ET 模式。

#### 代码分析解决

尝试将 listenfd 设置为 LT 阻塞

```cpp
for(int i=0;i<number;i++)
{
    int sockfd=events[i].data.fd;

    //处理新到的客户连接
    if(sockfd==listenfd)
    {
        struct sockaddr_in client_address;
        socklen_t client_addrlength=sizeof(client_address);

        //从listenfd中接收数据
        //这里的代码出现使用错误
        while ((connfd = accept (listenfd, (struct sockaddr *) &remote, &addrlen)) > 0){
            if(connfd<0)
            {
                printf("errno is:%d\n",errno);
                continue;
            }
            //TODO,逻辑处理
        }
    }
}
```

将代码修改后，重新进行压力测试，问题得到解决，服务器成功完成 75617 个访问请求，且没有出现任何失败的情况。压测结果如下：

#### Bug原因

established 状态的连接队列 backlog 参数，历史上被定义为已连接队列和未连接队列两个的大小之和，大多数实现默认值为5。当连接较少时，队列不会变满，即使 listenfd 设置成 ET 非阻塞，不使用 while 一次性读取完，也不会出现 Bug。

若此时 1000个 客户端同时对服务器发起连接请求，连接过多会造成 established 状态的连接队列变满。但 accept 并没有使用 while 一次性读取完，只读取一个。因此，连接过多导致 TCP 就绪队列中剩下的连接都得不到处理，同时新的连接也不会到来。

解决方案

将 listenfd 设置成LT阻塞，或者 ET 非阻塞模式下 while 包裹 accept 即可解决问题。

-----------------------------------

## 综合能力

### 使用 crc32 算法解决数据包收发过程中内容被篡改的问题

引入 CRC32 的目的是对收发的数据包进行简单校验，以确保数据包中的内容是没有被篡改的。这部分代码是借鉴过来的。

项目中主要是用到 Get_CRC 这个成员函数，这个函数的作用是：给定一段内存以及该内存的长度，可以计算出 crc32 值并返回，这段给定的内存内容或长度如果不同，返回的数字一般就会不同

当客户端将要发送一个数据包给服务器时，会提前把这个数据包的包体通过该函数计算出一个 crc32 值，放到要发送的数据包内。服务器收到一个完整的数据包之后，会根据收到的包体内容计算包体 crc32 值，与客户端发送过来的 crc32 值比较，如果 2 个 值相同，就认为这个数据包合法，否则就认为不合法丢弃。

如果一个恶意的客户端，就算破解出本项目的包格式（包头+包体），只要他破解不出服务端用的 crc32 算法，他发过来的数据包就不会被服务器认可，会被服务器丢弃。为本项目的网络服务器多了一层保障。



### 服务器突然运行很慢怎么处理

先查看后台服务器的运行状态，包括磁盘，CPU，内存的使用情况等（top，free）。如果是磁盘满了，做好备份，清理下磁盘；如果是CPU的问题，查找下占用率较高的进程，kill 掉与系统应用无关的进程。

还有一种情况可能是close_wait或者time_wait状态过多了，消耗了服务器的资源，使用netstat命令查看下网络连接的状态。



### 你的项目相比于其他项目的优点

> 丢包策略，安全方面

## Nginx 相关

### Nginx虚拟主机怎么配置

1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站

2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台

3、基于 ip 的虚拟主机。

#### 基于虚拟主机配置域名

需要建立`/data/www`,`/data/bbs`目录，`windows`本地`hosts`添加虚拟机`ip`地址对应的域名解析；对应域名网站目录下新增`index.html`文件；

```py
	#当客户端访问 www.kendall.com,监听端口号为80,直接跳转到data/www目录下文件
    server {
        listen       80;
        server_name  www.kendall.com;
        location / {
            root   data/www;
            index  index.html index.htm;
        }
    }

	#当客户端访问www.kendall.com,监听端口号为80,直接跳转到data/bbs目录下文件
	 server {
        listen       80;
        server_name  bbs.kendall.com;
        location / {
            root   data/bbs;
            index  index.html index.htm;
        }
    }

```

#### 基于端口的虚拟主机

使用端口来区分，浏览器使用域名或`ip`地址:端口号 访问

```bash
    #当客户端访问www.kendall.com,监听端口号为8080,直接跳转到data/www目录下文件
	 server {
        listen       8080;
        server_name  8080.kendall.com;
        location / {
            root   data/www;
            index  index.html index.htm;
        }
    }
	
	#当客户端访问www.kendall.com,监听端口号为80直接跳转到真实ip服务器地址 127.0.0.1:8080
	server {
        listen       80;
        server_name  www.kendall.com;
        location / {
		 	proxy_pass http://127.0.0.1:8080;
            index  index.html index.htm;
        }
	}
```


### 什么是正向代理和反向代理

正向代理就是一个人发送一个请求直接就到达了目标的服务器

反方代理就是请求统一被 Nginx 接收，nginx 反向代理服务器接收到之后，按照一定的规则分发给了后端的业务处理服务器进行处理了

> 正：请求 --- 服务器           
> 反：请求 --- Nginx --- 后端服务器

#### 使用 反向代理服务器的优点是什么?

反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。

**Nginx的优缺点**

**优点**：

- 占内存小，可实现高并发连接，处理响应快
- 可实现 http 服务器、虚拟主机、反向代理、负载均衡
- Nginx 配置简单
- 可以不暴露正式的服务器 IP 地址


**缺点**：

动态处理差：nginx 处理静态文件好,耗费内存少，但是处理动态页面则很吃力，现在一般前端用 nginx 作为反向代理抗住压力，

### nginx是如何实现高并发的

`nginx`之所以可以实现高并发，与它采用的`epoll`模型有很大的关系。`epoll`模型采用异步非阻塞的事件处理机制。这种机制可让`nginx`进程同时监控多个事件。

简单来说，就是异步非阻塞，使用了`epoll`模型和大量的底层代码优化。如果深入一点的话，就是`nginx`的特殊进程模型和事件模型的设计，才使其可以实现高并发。

#### 进程模型

它是采用一个master进程和多个worker进程的工作模式。

- 1、master进程主要负责收集、分发请求。当一个请求过来时，master拉起一个worker进程负责处理这个请求。；
- 2、master进程也要负责监控worker的状态，保证高可靠性；
- 3、worker进程议案设置为和CPU核心数一致或者其二倍。nginx的worker进程和Apache的不一样。apache的进程在同一时间只能处理一个请求，所以它会开启很多个进程，几百甚至几千个。而nginx的worker进程在同一时间可以处理的请求数只受内存限制，因此可以处理更多请求。

#### 事件模型

`nginx`是异步非阻塞的。

一个`master`进程，多个`worker`进程，每个`worker`进程可以处理多个请求。每进来一个`request`，都会有`worker`进程去处理。但不是全程的处理，那么处理到的程度就是可能发生阻塞的地方，比如向后端服务器转发`request`，并等待请求返回。那么，在等待期间，这个处理的`worker`不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是它就去休息了，此时，如果再有`request`进来，它就可以很快再按这种方式处理。而一旦后端服务器返回了，就会触发这个事件，`worker`才会来接手，这个`request`才会接着往下走。
由于`nginx`的的这个工作性质决定了每个请求大部分的生命都是在网络传输中，所以实际上花费在`nginx` 服务器上的时间并不多，这就是它几进程就能解决高并发的秘密所在


-------

# <font color="orange">课题流媒体项目</font>

## WebRTC 项目

在线教育、音视频会议这类直播属于实时互动直播，主要考虑**传输的实时性**，因此一般使用 UDP 作为底层传输协议；

而娱乐直播对实时性要求不高，更多关注的是画面的质量、音视频是否卡顿等问题，所以一般采用 TCP 作为传输协议。我们称前者为**实时互动直播**，后者为**传统直播**。

**传统直播技术使用的传输协议是 RTMP 和 HLS**。

> RTMP 协议在 PC 占有市场较大，而 HLS 是由苹果公司开发的。主要用在它的 iOS 平台，不过 Android 3 以后的平台也是默认支持 HLS 协议的。

### 传统直播基本架构

![](https://static001.geekbang.org/resource/image/65/24/65e38dc6f7d1ca989571ee685da25324.png)

传统直播架构由直播客户端、信令服务器和 CDN 网络这三部分组成。

直播客户端主要包括音视频数据的采集、编码、推流、拉流、解码与播放这几个功能。

但实际上，这几个功能并不是放在同一个客户端中实现的。为什么呢？因为作为主播来说，他不需要看到观众的视频或听到观众的声音；而作为观众来讲，他们与主播之间是通过文字进行互动的，因此也不需要分享自己的音视频。

客户端按用途可分为两类：
- 一类是主播使用的客户端，包括音视频数据采集、编码和推流功能；
- 另一类是观众使用的客户端，包括拉流、解码与渲染（播放）功能。

》对于主播客户端来说，它可以从 PC 或移动端设备的摄像头、麦克风采集数据，然后对采集到的音视频数据进行编码，最后将编码后的音视频数据按 RTMP 协议推送给 CDN 源节点（RTMP 接入服务器）。

》对于观众客户端来说，它首先从直播管理系统中获取到房间的流媒体地址，然后通过 RTMP 协议从边缘节点拉取音视频数据，并对获取到的音视频数据进行解码，最后进行视频的渲染与音频的播放。

**信令服务器**，主要用于接收信令，并根据信令处理一些和业务相关的逻辑，如创建房间、加入房间、离开房间、送礼物、文字聊天等。

> 但是在开发直播系统的信令服务器时，一定**要关注和防止消息的洪泛**。

就是聊天消息的处理。我们来举个例子，在一个有 10000 人同时在线的房间里，如果其中一个用户发送了文字消息，那么服务端收到该消息之后就要给 10000 人转发。如果主播说“请能听到我声音的人回复 1”，那这时 10000 人同时发消息，服务端要转发多少条呢？要转发 10000 * 10000 = 1 亿条消息。这对于任何一台服务器来说，都会产生灾难性的后果。所以，在开发直播系统的信令服务器时，一定**要关注和防止消息的洪泛**。

**CDN 网络**，主要用于媒体数据的分发。我们传给它的媒体数据可以很快传送给全世界每一个角落。换句话说，你在全世界各地，只要接入了 CDN 网络，你都可以快速看到你想看到的“节目”了。

### RTMP 介绍

RTMP 是实时消息协议。但它实际上并不能做到真正的实时，一般情况最少都会有几秒到几十秒的延迟，底层是基于 TCP 协议的。

需要注意的是，在使用 RTMP 协议传输数据之前，RTMP 也像 TCP 协议一样，先进行三次握手才能将连接建立起来。当 RTMP 连接建立起来后，你可以通过 RTMP 协议的控制消息为通信的双方设置传输窗口的大小（缓冲区大小）、传输数据块的大小等。


- RTMP 协议底层依赖于 TCP 协议，不会出现丢包、乱序等问题，因此音视频业务质量有很好的保障。

- 使用简单，技术成熟。有现成的 RTMP 协议库实现，如 FFmpeg 项目中的 librtmp 库，用户使用起来非常方便。而且 RTMP 协议在直播领域应用多年，技术已经相当成熟。

- 市场占有率高。在日常的工作或生活中，我们或多或少都会用到 RTMP 协议。如常用的 FLV 文件，实际上就是在 RTMP 消息数据的最前面加了 FLV 文件头。

- 相较于 HLS 协议，它的实时性要高很多。

### HLS

HLS 全称 HTTP Live Streaming，是苹果公司实现的基于 HTTP 的流媒体传输协议。它可以支持流媒体的直播和点播，主要应用在 iOS 系统和 HTML5 网页播放器中。

HLS 的基本原理非常简单，它是将多媒体文件或直接流进行切片，形成一堆的 ts 文件和 m3u8 索引文件并保存到磁盘。

当播放器获取 HLS 流时，它首先根据时间戳，通过 HTTP 服务，从 m3u8 索引文件获取最新的 ts 视频文件切片地址，然后再通过 HTTP 协议将它们下载并缓存起来。当播放器播放 HLS 流时，播放线程会从缓冲区中读出数据并进行播放。

HLS 协议的本质就是通过 HTTP 下载文件，然后将下载的切片缓存起来。由于切片文件都非常小，所以可以实现边下载边播的效果。HLS 规范规定，播放器至少下载一个 ts 切片才能播放，所以 HLS 理论上至少会有一个切片的延迟。

浏览器天然支持 HLS 协议，而 RTMP 协议需要安装 Flash 插件才能播放 RTMP 流。

### HLS 的不足

HLS 最主要的问题就是实时性差。由于 HLS 往往采用 10s 的切片，所以最小也要有 10s 的延迟，一般是 20～30s 的延迟，有时甚至更差。

HLS 之所以能达到 20～30s 的延迟，主要是由于 HLS 的实现机制造成的。HLS 使用的是 HTTP 短连接，且 HTTP 是基于 TCP 的，所以这就意味着 HLS 需要不断地与服务器建立连接。TCP 每次建立连接时都要进行三次握手，而断开连接时，也要进行四次挥手，基于以上这些复杂的原因，就造成了 HLS 延迟比较久的局面。

### 如何选择 RTMP 和 HLS

- 流媒体接入，也就是推流，应该使用 RTMP 协议。

- 流媒体系统内部分发使用 RTMP 协议。因为内网系统网络状况好，使用 RTMP 更能发挥它的高效本领。

- 在 PC 上，尽量使用 RTMP 协议，因为 PC 基本都安装了 Flash 播放器，直播效果要好很多。

- 移动端的网页播放器最好使用 HLS 协议。

- iOS 要使用 HLS 协议，因为不支持 RTMP 协议。

- 点播系统最好使用 HLS 协议。因为点播没有实时互动需求，延迟大一些是可以接受的，并且可以在浏览器上直接观看。

## HLS 直播架构

![](https://static001.geekbang.org/resource/image/c8/7a/c824a7d2fc85aa9583e10bc0dbff407a.png)

客户端采集媒体数据后，通过 RTMP 协议将音视频流推送给 CDN 网络的源节点（接入节点）。源节点收到音视频流后，再通过 Convert 服务器将 RTMP 流切割为 HLS 切片文件，即 .ts 文件。同时生成与之对应的 m3u8 文件，即 HLS 播放列表文件。

切割后的 HLS 分片文件（.ts 文件）和 HLS 列表文件（.m3u8 文件）经 CDN 网络转发后，客户端就可以从离自己最近的 CDN 边缘节点拉取 HLS 媒体流了。

在拉取 HLS 媒体流时，客户端首先通过 HLS 协议将 m3u8 索引文件下载下来，然后按索引文件中的顺序，将 .ts 文件一片一片下载下来，然后一边播放一边缓冲。此时，你就可以在 PC、手机、平板等设备上观看直播节目了。

对于使用 HLS 协议的直播系统来说，最重要的一步就是**切片**。源节点服务器收到音视频流后，先要数据缓冲起来，保证到达帧的所有分片都已收到之后，才会将它们切片成 TS 流。

可以使用 FFmpeg 生成 HLS 切片

### m3u8 格式分析

### TS 格式分析


> https://time.geekbang.org/column/article/141052


### 优化卡顿带来的累积延时

> 设计缓存策略

在理想的情况下：网络状况良好；采集推流端、流媒体服务器、播放端均吞吐正常无阻塞，可以不配置缓冲区。这时候推流端到播放端的延时将会很小，基本上就是网络传输的耗时。

但是在实际情况中，我们多多少少会遇到网络不佳或网络抖动的情况，在这种网络环境下，如果没有缓冲策略，直播将发生卡顿。为了解决卡顿，通常会根据具体情况在采集推流端、流媒体服务器、播放端增加缓冲策略，而一旦发生缓冲，就意味着推流端到播放端的延时。当卡顿情况多次出现，这样的延时就会累积。

此外，从 RTMP 协议层面上来讲，累积延时本身是它的一个特征，因为 RTMP 是基于 TCP，所以不会丢包，在网络情况不佳的情况下超时重传策略、缓冲策略等自然会带来累积延时。

所以，优化卡顿带来的累积延时首先是要优化整个直播链条的网络状况去减少卡顿。从这个角度出发，我们可以采用的策略包括：

- 使用 CDN 分发网络。
- 合理采用 CDN 边缘节点推流。
- 推流端、播放端使用 HTTP DNS 选择网络状况最好的节点接入。
- 推流端实现码率自适应策略，在网络状况不佳的情况下，降低推流码率来降低上行带宽压力。
- 流媒体服务器提供多档位直播流服务，与此同时，播放端实现直播流多档位切换策略，在网络状况不佳的情况下，切换到低档位直播流来降低下行带宽压力。

除了这些外，我们还可以优化各端的缓冲策略来降低累积延时。直播链条各端的缓冲区通常都是为了防止网络抖动以及端上性能抖动产生卡顿，但是各缓冲区的数据越多，通常也意味着累积延时越大。所以在各端上，我们还可以尝试这些策略：

- 在「卡顿」和「累积延时」这两项体验指标上寻找一个平衡点，在各端设置合适的缓冲区大小。
- 在各端实现一些丢帧策略，当缓冲区超过一定阈值时，开始丢帧。
- 在播放端的缓冲区过大时，尝试断开重连。

### http响应耗时

- 什么是http响应耗时？

http 响应耗时是指客户端发起一个 http request 请求，然后等待 http 响应的header 返回这部分耗时。直播拉流 http-flv 协议也是一个 http 请求，客服端发起请求后，服务端会先将 http 的响应头部返回，不带音视频流的数据，响应码如果是 200 ，表明视频流存在，紧接着就开始下发音视频数据。

http 响应耗时非常重要，它直接反应了 CDN 服务节点处理请求的能力。它与CDN 节点是否有缓存这条流有关，如果在请求之前有缓存这条流，节点就会直接响应客户端，这个时间一般也在 50ms 左右，最多不会超过 200ms ，如果没有缓存，节点则会回直播源站拉取直播流，耗时就会很久，至少都在 200ms 以上，大部分时间都会更长，所以它反应了这条直播流是否是冷流和热流，以及 CDN 节点的缓存命中情况。

- Http响应耗时的优化解析

目前 HTTP 响应耗时分两种情况：

如果 CDN 节点没有缓存流，CDN 收到 HTTP 请求后，就需要回源站去拉流，请求响应，并等待源站的响应结果。这个耗时就比较久了，一般是 400ms 左右，这块和 CDN 内部的架构有关，有时更久，达到几秒的情况都有，所以这种情况，一般需要推动 CDN 厂商做一些优化；

如果 CDN 节点有缓存流，CDN 收到 HTTP 请求后，会理解返回响应头部，一般是在 100ms 以内，响应很快。这块比较受限于 CDN 边缘节点分发策略，不同的 CDN 厂商的表现会有些差异，在端层面可做的东西较少，所以主要是推动多 CDN 的融合策略来提升更好的体验。













