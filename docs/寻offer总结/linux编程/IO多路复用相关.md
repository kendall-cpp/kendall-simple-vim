## Socket模型

要想客户端和服务器能在网络中通信，那必须得使用 `Socket` 编程，`Socket`可以跨主机间通信。

创建 `Socket` 的时候，可以指定网络层使用的是 `IPv4` 还是 `IPv6`，传输层使用的是 `TCP` 还是 `UDP`。

对于 TCP 的 Socket 编程来说

### 服务端的 Socket 编程过程

（1）服务端首先调用 `socket()` 函数，创建网络协议为 `IPv4`，以及传输协议为 `TCP` 的 `Socket` ，接着调用 `bind()` 函数，给这个 `Socket` 绑定一个 `IP` 地址和端口

- 绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。
- 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，只有相应的网卡收到数据后，才会发给我们；

（2）绑定完 IP 地址和端口号后，就可以调用 `listen()` 函数进行监听，这个时候如果我们要判定服务器中一个网络程序有没有启动，可以通过 `netstat` 命令查看对应的端口号是否有被监听。

（3）服务端进入了监听状态后，通过调用 `accept()` 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。

**那客户端是怎么发起连接的呢**？

（1）客户端在创建好 `Socket` 后，调用 `connect()` 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后 `TCP` 三次握手就开始了。

（2）在 `TCP` 连接的过程中，服务器的内核实际上为每个 `Socket` 维护了两个队列：

一个是TCP半连接队列，一个是TCP全连接队列。TCP半连接队列就是还没完成三次握手的链接，TCP全连接队列就是已经完成三次握手的连接。

（3）当 `TCP` 全连接队列不为空后，服务端的 `accept()` 函数，就会从内核中的 `TCP` 全连接队列里拿出一个已经完成连接的 Socket 返回给应用程序，后续数据传输都用这个 `Socket`。

连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 `read()` 和 `write()` 函数来读写数据。

当数据传输完成后就会调用`close()`函数关闭连接。

以上就是 `TCP` 协议的 `Socket` 程序的调用过程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/socket01.5fx6tkyx9lk0.png)


> 前面提到的` TCP Socket` 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。

### 服务器单机理论最大能连接多少个客户端，或者说TCP最大连接数是多少

服务器的IP地址和端口号一般是固定不变的，等待客户端的连接请求。

客户端 IP 和 端口是可变的，其理论值计算公式如下:

> 最大 TCP 连接数 = 客户端的 IP 数 * 客户端的端口数

- 对 IPv4 来说，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最 大 TCP 连接数，约为 2 的 48 次方。

- 当然，服务端最大并发 TCP 连接数远不能达到理论上限
  - 首先主要是**文件描述符限制**，Socket 都是文件，所以首先要通过 `ulimit` 配置文件描述符的数目;
  - 另一个是**内存限制**，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的。


#### 如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗

> 并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。

从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 `200KB` 的内存和 `100Kbit` 的网络带宽就可以满足并发 1 万个请求。

不过，要想真正实现 `C10K` 的服务器，要考虑的地方在于服务器的网络 `I/O` 模型，如果模型的效率低，会加重系统开销，从而会离 `C10K` 的目标越来越远。

## 什么是多进程模型

基于最原始的阻塞网络 `I/O`， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用**多进程模型**，也就是为每个客户端分配一个进程来处理请求。

服务器的主进程负责监听客户的连接，一旦与客户端连接完成，`accept()` 函数就会返回一个「已连接 `Socket`」，这时就通过 `fork()` 函数创建一个子进程，实际上就把父进程所有相关的东西都复制一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。

这两个进程刚复制完的时候，几乎一摸一样。不过，会根据返回值来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。

正因为子进程会复制父进程的文件描述符，于是就可以直接使用「已连接 `Socket` 」和客户端通信了，

其实，子进程不需要关心「监听 `Socket`」，只需要关心「已连接 `Socket`」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 `Socket`」，只需要关心「监听 `Socket`」。

下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/多进程模型.1s0nx3gpz2e8.png)

另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成**僵尸进程**，随着僵尸进程越多，会慢慢耗尽我们的系统资源。

因此，父进程要管理好自己的孩子，怎么管理呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 `wait()` 和 `waitpid()` 函数。

这种用多个进程来应付多个客户端的方式，在应对 `100` 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换会消耗大量的性能。

因为进程的切换和调度是由内核管理的，所以进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。

## 多线程模型

既然进程间上下文切换的性能开销很大，那我们就搞个比较轻量级的模型来应对多用户的请求 —— **多线程模型**。

线程是运行在进程中的一个“逻辑流”，一个进程中可以运行多个线程，同一个进程里的线程可以共享进程的部分资源的，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时是不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。

当服务器与客户端 `TCP` 完成连接后，通过 `pthread_create() `函数创建线程，然后将「已连接 `Socket`」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。

如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。

那么，我们可以使用线程池的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 `Socket` 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 `Socket` 进程处理。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/多线程模型.2doslyzk3d34.png)

需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。

上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 `C10K`，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。


## I/O 多路复用技术

 `I/O` 多路复用技术就是用一个进程来维护多个 `Socket`。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/IO复用01.4ns3czhz0sk0.png)


一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用。

比较常见的多路复用技术有 `select/poll/epoll`


### select/poll

`select` 将已连接的 `Socket` 都放到一个文件描述符集合，然后调用 `select` 函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过遍历文件描述符集合的方式，当检查到有事件产生后，将此 `Socket` 标记为可读或可写， 接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方法找到可读或可写的 `Socket`，然后再对其处理。

所以，对于 `select` 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符**集合，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

`select` 所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 `FD_SETSIZE` 限制， 默认最大值为 `1024`。

`poll` 以链表形式来组织文件描述符，解决了 `select` 的文件描述符个数限制的问题，当然还会受到系统文件描述符限制。

但是 `poll` 和 `select` 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 `Socket` 集合，因此都需要遍历文件描述符集合来找到可读或可写的 `Socket`，时间复杂度为 `O(n)`，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

### epoll

`epoll` 通过两个方面，很好解决了 `select/poll` 的问题。

- 第一点，`epoll` 在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 `socket` 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 `select/poll` 每次操作时都传入整个 `socket` 集合，只需要传入一个待检测的` socket`就可以了，减少了内核和用户空间大量的数据拷贝和内存分配。

- 第二点， `epoll` 使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当某个 `socket` 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 `select/poll` 那样轮询扫描整个` socket` 集合，大大提高了检测的效率。

从下图你可以看到 `epoll` 相关的接口作用：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epoll01.58ud4l3nxm00.png)

`epoll` 的方式即使监听的 `Socket` 数量很多的时候，效率不会大幅度降低，能够同时监听的 `Socket` 的数目也非常多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，`epoll` 被称为解决` C10K` 问题的利器。

### 边缘触发和水平触发

`epoll` 支持两种事件触发模式，分别是**边缘触发**（edge-triggered，**ET**）和**水平触发**（level-triggered，**LT**）。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- 使用**边缘触发模式**时，当被监控的 `Socket` 描述符上有可读事件发生时，服务器端只会从 `epoll_wait` 中苏醒一次，即使进程没有调用 `read` 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；

- 使用**水平触发模式**时，当被监控的 `Socket` 上有可读事件发生时，服务器端不断地从 `epoll_wait` 中苏醒，直到内核缓冲区数据被 `read` 函数读完才结束，目的是告诉我们有数据需要读取；

举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。

这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。

如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，边缘触发模式一般和非阻塞 I/O 搭配使用，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 `epoll_wait` 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

`select/poll` 只有水平触发模式，`epoll` **默认的触发模式是水平触发**，但是可以根据应用场景设置为边缘触发模式。

> 更深入理解：https://zhuanlan.zhihu.com/p/361750240