
- [一条SQL语句是如何执行的](#一条sql语句是如何执行的)
  - [连接器](#连接器)
  - [查询缓存](#查询缓存)
  - [分析器](#分析器)
  - [优化器](#优化器)
  - [执行器](#执行器)
- [更新语句是怎么执行的](#更新语句是怎么执行的)
  - [binlog 和 redo log 的区别](#binlog-和-redo-log-的区别)
  - [update的执行流程](#update的执行流程)
  - [总结](#总结)
- [事务](#事务)
  - [事务的四大特性（ACID）](#事务的四大特性acid)
  - [事务的隔离性与隔离级别](#事务的隔离性与隔离级别)
  - [事务隔离的实现](#事务隔离的实现)
    - [MCVV（多版本并发控制）实现机制](#mcvv多版本并发控制实现机制)
    - [尽量不要使用长事务](#尽量不要使用长事务)
  - [如何避免长事务对业务的影响](#如何避免长事务对业务的影响)
- [数据库的索引](#数据库的索引)
  - [索引的实现方式（索引的常见模型）](#索引的实现方式索引的常见模型)
  - [InnoDB 的索引模型](#innodb-的索引模型)
    - [为什么用 B+ 树做索引而不用哈希表做索引](#为什么用-b-树做索引而不用哈希表做索引)
    - [基于主键索引和普通索引的查询有什么区别](#基于主键索引和普通索引的查询有什么区别)
    - [B+ 树怎么维护索引有序性](#b-树怎么维护索引有序性)
  - [为什么建议使用主键自增的索引](#为什么建议使用主键自增的索引)
    - [什么情况不能使用自增索引，也就是用字段直接做主键](#什么情况不能使用自增索引也就是用字段直接做主键)
- [主键索引树搜索的过程(回表)](#主键索引树搜索的过程回表)
  - [如何优化索引，避免回表过程](#如何优化索引避免回表过程)
  - [需要每一种查询都设计一个索引么](#需要每一种查询都设计一个索引么)
  - [什么是最左前缀原则](#什么是最左前缀原则)
    - [在建立联合索引的时候，如何安排索引内的字段顺序](#在建立联合索引的时候如何安排索引内的字段顺序)
- [数据库的锁](#数据库的锁)
  - [全局锁](#全局锁)
  - [表级锁](#表级锁)
  - [行锁](#行锁)
  - [死锁和死锁检测](#死锁和死锁检测)

-------

## 一条SQL语句是如何执行的

> * 连接器：客户端先通过连接器连接到 MySQL 服务器。
> * 缓存：连接器权限验证通过之后，先查询是否有查询缓存，如果有缓存（之前执行过此语句）则直接返回缓存数据，如果没有缓存则进入分析器。
> * 分析器：分析器会对查询语句进行语法分析和词法分析，判断 SQL 语法是否正确，如果查询语法错误会直接返回给客户端错误信息，如果语法正确则进入优化器。
> * 优化器：优化器是对查询语句进行优化处理，例如一个表里面有多个索引，优化器会判别哪个索引性能更好。
> * 执行器：优化器执行完就进入执行器，执行器就开始执行语句进行查询比对了，直到查询到满足条件的所有数据，然后进行返回。

下面`sql`语句的执行过程

```sql
mysql> select * from T where ID=10；
```


MySQL 的基本架构示意图

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/MySQL逻辑架构.4b5s3h0pgu40.png)

MySQL 可以分为 **Server 层**和**存储引擎层**两部分

Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有**跨存储引擎的功能**都在这一层实现，比如存储过程、触发器、视图等。


存储引擎层负责数据的**存储**和**提取**。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

> * InnoDB ： `InnoDB`是`mysql`的默认引擎，支持事务和外键，支持容灾恢复。适合更新频繁和多并发的表  行级锁

> * MyISAM ： 插入和查询速度比较快，支持大文件，但是不支持事务，适合在`web`和数据仓库场景下使用  表级锁

> * MEMORY ： `memory`将表中的数据保存在内存里，适合数据比较小而且频繁访问的场景

### 连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接，

```sql
mysql -h$ip -P$port -u$user -p
        IP     端口   用户名   密码
```
- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖这个时候读到的权限。

连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 `show processlist` 命令中看到它。文本中这个图是 `show processlist` 的结果，其中的 `Command` 列显示为“`Sleep`”的这一行，就表示现在系统里面有一**个空闲连接**。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/MySQL连接器01.4dcjuya819y0.png)

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 `wait_timeout` 控制的，默认值是 8 小时。

如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：` Lost connection to MySQL server during query`。这时候如果你要继续，就需要重连，然后再执行请求了。

数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。

建立连接的过程通常是比较复杂的，所以一般建议**尽量使用长连接**。

但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 **MySQL 在执行过程中临时使用的内存是管理在连接对象里面的**。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。

**怎么解决这个问题呢**？

- 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

- 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 `mysql_reset_connection` 来**重新初始化连接资源**。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

> 关键词：权限验证，空闲连接，长连接「定期断开长连接 和 重新初始化连接资源」

### 查询缓存

连接建立完成后，你就可以执行 `select` 语句了。执行逻辑就会来到第二步：查询缓存。

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 `key-value` 对的形式，被直接缓存在内存中。`key` 是查询的语句，`value` 是查询的结果。如果你的查询能够直接在这个缓存中找到 `key`，那么这个 `value `就会被直接返回给客户端。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。

**但是建议尽量少使用查询缓存**，因为查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张**静态表**，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。      
MySQL 8.0 版本直接将查询缓存的整块功能删掉了

> 关键词：先查缓存「key-value」,少用缓存「频繁失效」

### 分析器

接下来就是分析器，分析器先会做“词法分析”。MySQL 需要识别出你输入的 SQL 语句的字符串分别是什么，代表什么。

做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。


> 关键词：词法分析，语法分析


### 优化器

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（`join`）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 `join`：

### 执行器

开始执行的时候，执行器要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 `precheck` 验证权限)。

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

```sql
select * from T where ID=10;
```

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：
- 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
- 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
- **执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端**。

所以到了执行的时候才会进入到数据库引擎，然后执行器也是通过调用数据库引擎的 API 来进行数据操作的。也因此数据库引擎才会是插件形式的。 

> 关键词：判断权限，引擎接口，记录集

如果表 T 中没有字段 k，而你执行了这个语句 `select * from T where k=1`, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？              
> 分析器： 分析器对语句中的表，字段是否存在进行判断

## 更新语句是怎么执行的

更新流程与查询流程不一样的是，更新流程还涉及两个重要的日志模块，`redo log`（重做日志）和 `binlog`（归档日志）

当一条记录更新的时候，`InnoDB` 引擎就会先把记录写到 `redo log`（粉板）里面，并更新内存，这个时候更新就算完成了。同时，`InnoDB` 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/SQL更新流程01.5szqyotr8jw0.png)

`write pos` 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。`checkpoint` 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 **crash-safe**。

> 关键词：`redo log`（重做日志）和 `binlog`（归档日志）， 写：`++write pos` 读：`++checkpoint`     
> `InnoDB`--> 内存 --> (空闲时写) 磁盘

### binlog 和 redo log 的区别

`redo log` 是 InnoDB 引擎特有的日志

`binlog`（归档日志) 是server 层的日志


这两种日志有以下三点不同。

- redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
- redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
- redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

> redo log 物理日志，数据做了什么修改，循环写，空间会用完， binlog 逻辑日志，比如：c + 1,追加写，不会覆盖日志


### update的执行流程

执行器和 InnoDB 引擎在执行 update 语句时的内部流程。

```sql

mysql> create table T(ID int primary key, c int);

mysql> update T set c=c+1 where ID=2;
```

- 1.执行器通过引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

- 2.执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

- 3.引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

- 4.执行器生成这个操作的 binlog，并把 binlog 写入磁盘。

- 5.执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。

> 通过引擎找到 ID=2 --> 判断数据页是否在内存 --> 在返回数据 --> 不在从磁盘读入内存再返回数据 --> c + 1 --> 写入新行 --> 更新到内存 --> 引擎写 redo log 日志 --> 执行器写 binlog 日志 --> 提交事务

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/SQL更新流程02.2iwkd0zjay80.png)


### 总结

`redo log` 用于保证 `crash-safe` 能力。`innodb_flush_log_at_trx_commit `这个参数设置成 1 的时候，表示每次事务的 `redo log` 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

`sync_binlog` 这个参数设置成 1 的时候，表示每次事务的 `binlog` 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 `binlog` 不丢失。

-------

## 事务

简单来说，事务就是要保证一组数据库操作，**要么全部成功，要么全部失败**。在 MySQL 中，**事务支持是在引擎层实现的**。MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。

### 事务的四大特性（ACID）

> 1、原子性（Atomicity）： 事务开始后的所有操作要么全部完成，要么全部不完成，不能只完成一部分。事务执行过程中发生错误，会回滚已有操作并恢复到事务开始前的状态。

> 2、一致性（Consistency）： 事务开始前和结束后，数据库的完整性没有被破坏。比如：A向B转账 1000 元，A的账户中会减少 1000 元，而B的账户中会增加1000元。

> 3、隔离性（Isolation）： 多个事务并发执行时，同一时间只允许一个事务请求同一数据，不同的事务之间不会互相干扰。如：A在从一张银行卡取款的过程中，其他人不能向这张银行卡转账。

> 4、持久性（Durability）： 事务完成之后，事务对数据库的所有更改应该保存在数据库中，不能回滚。

### 事务的隔离性与隔离级别

当数据库上有多个事务同时执行的时候，就可能出现**脏读**（dirty read）、**不可重复读**（non-repeatable read）、**幻读**（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。

但是隔离得越严实，效率就会越低，因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：**读未提交**（read uncommitted）、**读已提交**（read committed）、**可重复读**（repeatable read）和**串行化**（serializable ）

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。最低级别的隔离，不能解决以上问题
- 读已提交是指，一个事务提交之后，它做的变更才会被其他事务看到。可以避免脏读的发生
- 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。可以避免脏读和不可重复读。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。可以避免以上所有问题。

用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。

```sql
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

在实现上，数据库里面会创建一个**视图**，访问的时候以视图的逻辑结果为准。

在“读已提交”隔离级别下，这个视图是在每个 SQL 语句**开始执行的时候**创建的。

在“可重复读”隔离级别下，这个视图是在事务**启动时**创建的，整个事务存在期间都用这个视图。

这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，**没有视图概念**；

而“串行化”隔离级别下直接用**加锁**的方式来避免并行访问。

> 关键词：通过视图是实现，读已提交-->SQL语句开始执行的时候创建视图，可重复读-->事务启动时创建视图，读未提交没有视图，串行化通过加锁

我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，一定要将 MySQL 的隔离级别设置为“读提交”。

配置的方式是，将启动参数 `transaction-isolation` 的值设置成 `READ-COMMITTED`。你可以用 `show variables` 来查看当前的值。

```

mysql> show variables like 'transaction_isolation';

+-----------------------+----------------+

| Variable_name | Value |

+-----------------------+----------------+

| transaction_isolation | READ-COMMITTED |

+-----------------------+----------------+
```

“可重复读”隔离级别，事务启动时的视图可以认为是静态的，不受其他事务更新的影响。
比如：银行月末校对账户，不能受到新交易影响。

### 事务隔离的实现

在 MySQL 中，实际上每条记录在更新的时候都会同时**记录一条回滚操作**。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。

![](https://static001.geekbang.org/resource/image/d9/ee/d9c313809e5ac148fc39feff532f0fee.png)

当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的**多版本并发控制**（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。

但是回滚日志不可能一直保留，系统会判断，当没有事务需要用到这些 回滚日志 的时候，由 Purge 线程回收。

> 什么时候不需要呢？

当没有比回滚日志更早的读视图 ( read-view )「读视图在事务开启时创建」的时候，这个数据不会再有谁驱使它回滚了，这个回滚日志也就可以删除了。

> 关键词：回滚到前一个状态，后面的视图更改不影响前面的视图，回滚日志在不需要的时候会被 Purge 回收

#### MCVV（多版本并发控制）实现机制

`MCVV`是一种多版本并发控制机制，通过保存数据在某个时间点的快照来实现的。不同的存储引擎的 MCVV 实现是不同的，典型的有乐观（Optimistic）并发控制和悲观（pessimistic）并发控制。

`InnoDB` 的 `MCVV`，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的删除时间。这里时间指的并不是实际的时间值，而是系统版本号。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务版本号，用来和查询到的每行记录的版本号进行比较。在可重复读(repeatable read)隔离级别下，输入`SELECT`查找语句时，`InnoDB`只查找版本早于当前事务版本的数据行。保存这两个额外的系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作简单，性能好。不足之处就是每行记录都需要额外的存储空间，需要更多的行检查和维护工作。

> 关键词：乐观并发，悲观并发，`InnoDB`的`MCVV`保存了两列「版本号」,免了加锁

#### 尽量不要使用长事务

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。

> 关键词：老的事务视图，保留很多回滚记录 (因为老事务视图会用到)，占内存；占锁资源


> 有些客户端连接框架会默认连接成功后先执行一个 `set autocommit=0` 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。
> 
> 所以最好要总是使用 `set autocommit=1`, 通过显式语句的方式来启动事务。


### 如何避免长事务对业务的影响

**首先，从应用开发端来看**：

- 确认是否使用了 `set autocommit=0`。我们可以把 MySQL 的 `general_log` 开起来，然后随便跑一个业务逻辑，通过 `general_log` 的日志来确认是否使用了。我们最好总是使用`set autocommit=1`。

-  确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 `begin/commit` 框起来。因为有些业务并没有这个需要，但是也把好几个 `select` 语句放到了事务中。这种只读事务可以去掉。

- 业务连接数据库的时候，根据业务本身的预估，通过 `SET MAX_EXECUTION_TIME` 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。

**其次，从数据库端来看**:

- 监控 `information_schema.Innodb_trx` 表，设置长事务阈值，超过就报警 `/` 或者 `kill`；
-` Percona` 的`pt-kill` 这个工具不错，推荐使用；在业务功能测试阶段要求输出所有的 `general_log`，分析日志行为提前发现问题；
- 如果使用的是 `MySQL 5.6` 或者更新版本，把 `innodb_undo_tablespaces` 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

> 关键词：      
> - 应用开发端：
>   - 使用`set autocommit=1`       
>   - 去掉不必要可读事务
>   - 控制执行语句的最长时间
> - 数据库端：
>   - 设置长事务阈值，超过就报警
>   - 使用`pt-kill`工具分析日志行为
>   - 设置增大事务回滚段

---------

## 数据库的索引

一句话简单来说，**索引的出现其实就是为了提高数据查询的效率，就像书的目录一样**

### 索引的实现方式（索引的常见模型）

常见的有三种，分别是**哈希表**、**有序数组**和**搜索树**。

- **哈希表**

**哈希表**是一种以 键 - 值（key-value）存储数据的结构，我们只要输入待查找的键即 key，就可以找到其对应的值即 Value。

不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。也就是会发生冲突，我们可以使用**链表法**来解决冲突问题。

这种方式往后追加一个节点的速度很快，但是哈希表中的数据不是有序的，所以哈希索引做区间查询的速度是很慢的。

所以，**哈希表这种结构适用于只有等值查询的场景**，比如 Memcached 及其他一些 NoSQL 引擎。

- **有序数组**

**有序数组在等值查询和范围查询场景中的性能就都非常优秀**，仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。所以，**有序数组索引只适用于静态存储引擎**，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。

- **二叉搜索树**

**二叉搜索树**的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果你要查 查找的时间复杂度是 `O(log(N))`，

当然为了维持 `O(log(N))` 的查询复杂度，需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 `O(log(N))`。

但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。减少磁盘的访问次数。


### InnoDB 的索引模型

InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

所以每一个索引在 InnoDB 里面对应一棵 B+ 树。

#### 为什么用 B+ 树做索引而不用哈希表做索引

1、哈希表是把索引字段映射成对应的哈希码然后再存放在对应的位置，这样的话，如果我们要进行模糊查找的话，显然哈希表这种结构是不支持的，只能遍历这个表。而B+树则可以通过最左前缀原则快速找到对应的数据。

2、如果我们要进行范围查找，例如查找ID为`100 ~ 400`的人，哈希表同样不支持，只能遍历全表。

3、索引字段通过哈希映射成哈希码，如果很多字段都刚好映射到相同值的哈希码的话，那么形成的索引结构将会是一条很长的链表，这样的话，查找的时间就会大大增加。


#### 基于主键索引和普通索引的查询有什么区别

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/InnoDB索引01.5f7e3z4h6z80.png)

> 主键索引（左） 非主键做阴(右)

- 主键索引的叶子节点存的是**整行数据**。在 InnoDB 里，主键索引也被称为聚簇索引
- 非主键索引的叶子节点内容是**主键的值**。在 InnoDB 里，非主键索引也被称为二级索引

再看看他们在查询上有什么区别：

- 如果语句是 `select * from T where ID=500`，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
- 如果语句是 `select * from T where k=5`，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

#### B+ 树怎么维护索引有序性

![](https://static001.geekbang.org/resource/image/dc/8d/dcda101051f28502bd5c4402b292e38d.png)

以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。

除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。

当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。

### 为什么建议使用主键自增的索引

对于这颗主键索引的树

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/自增索引01.25xndi6itrog.png)

如果我们插入 `ID = 650` 的一行数据，那么直接在最右边插入就可以了

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/自增索引02.571rsw3p9so0.webp)


但是如果插入的是 `ID = 350` 的一行数据，由于 B+ 树是有序的，那么需要将下面的叶子节点进行移动，腾出位置来插入 `ID = 350` 的数据，这样就会比较消耗时间，如果刚好 R4 所在的数据页已经满了，需要进行页分裂操作，这样会更加糟糕。

但是，如果我们的主键是自增的，每次插入的 ID 都会比前面的大，那么我们每次只需要在后面插入就行， 不需要移动位置、分裂等操作，这样可以提高性能。也就是为什么建议使用主键自增的索引。

#### 什么情况不能使用自增索引，也就是用字段直接做主键

比如：
- 只有一个索引；
- 这个索引必须是唯一索引。

也就是典型的 KV 场景。

由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。

这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。

## 主键索引树搜索的过程(回表)

在下面这个表 T 中，如果我执行 `select * from T where k between 3 and `5，需要执行几次树的搜索操作，会扫描多少行？


![](https://static001.geekbang.org/resource/image/dc/8d/dcda101051f28502bd5c4402b292e38d.png)

这条 SQL 查询语句的执行流程：

1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；
2. 再到 ID 索引树查到 ID=300 对应的 R3；
3. 在 k 索引树取下一个值 k=5，取得 ID=500；
4. 再回到 ID 索引树查到 ID=500 对应的 R4；
5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。


在这个过程中，回到主键索引树搜索的过程，我们称为**回表**。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。

在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。


### 如何优化索引，避免回表过程

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用**覆盖索引**是一个常用的性能优化手段。

如果执行的语句是 `select ID from T where k between 3 and 5`，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为**覆盖索引**。


> 在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？

我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。

### 需要每一种查询都设计一个索引么

> 单独为一个不频繁的请求创建一个（身份证号，地址）的索引太浪费了

**B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录**。

不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

### 什么是最左前缀原则

例如对于下面这一张表

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/左前缀01.5x44tzpjya40.webp)

如果我们按照 name 字段来建立索引的话，采用B+树的结构，大概的索引结构如下

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/左前缀02.1fyfrwjljwtc.png)

如果我们要进行模糊查找，查找name 以“张"开头的所有人的ID，即 sql 语句为

```sql
select ID from table where name like '张%'
```

由于在 B+ 树结构的索引中，索引项是按照索引定义里面出现的字段顺序排序的，索引在查找的时候，可以快速定位到 ID 为 100 的张一，然后直接向右遍历所有张开头的人，直到条件不满足为止。

也就是说，我们找到第一个满足条件的人之后，直接向右遍历就可以了，由于索引是有序的，所有满足条件的人都会聚集在一起。

而这种定位到最左边，然后向右遍历寻找，就是我们所说的**最左前缀原则**。


#### 在建立联合索引的时候，如何安排索引内的字段顺序

这里需要考虑索引的复用能力

因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，**第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的**。

那么，如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。

这时候，我们要考虑的原则就是**空间**了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。


-----------

## 数据库的锁

MySQL 里面的锁大致可以分成**全局锁、表级锁和行锁**三类

### 全局锁

全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 `Flush tables with read lock (FTWRL)`。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：

全局锁的典型使用场景是，**做全库逻辑备份**。也就是把整库每个表都 select 出来存成文本。

### 表级锁

MySQL 里面表级别的锁有两种：一种是**表锁**，一种是**元数据锁**（meta data lock，MDL)。

表锁的语法是 `lock tables … read/write`。与 FTWRL 类似，可以用 `unlock tables` 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，`lock tables `语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

### 行锁

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

> 行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

### 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁

当出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。

- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑。「主动死锁检测」








