
- [项目介绍](#项目介绍)
  - [Fetcher和 DNS Resolver](#fetcher和-dns-resolver)
  - [Content Seen](#content-seen)
  - [Extractor和Url Filter](#extractor和url-filter)
  - [Url Seen](#url-seen)
  - [Url Set](#url-set)
  - [URL Frontier](#url-frontier)
- [主程序流程](#主程序流程)
    - [将程序设计成为守护进程](#将程序设计成为守护进程)
- [DNS 解析](#dns-解析)
  - [URL 处理](#url-处理)
- [处理http](#处理http)
  - [HTTP常见字段](#http常见字段)
- [使用正则表达式抽取HTML正文和URL](#使用正则表达式抽取html正文和url)
- [解析 HTML 页面](#解析-html-页面)
  - [使用 base64 传输二进制数据](#使用-base64-传输二进制数据)
  - [关于页面的存储](#关于页面的存储)
- [Epoll 相关](#epoll-相关)
- [定制规则扩展为垂直搜索](#定制规则扩展为垂直搜索)
- [线程池](#线程池)
- [遵守robots.txt](#遵守robotstxt)
- [遇到问题怎么排查，遇到什么困难。](#遇到问题怎么排查遇到什么困难)
- [epoll的触发模式是什么](#epoll的触发模式是什么)
  - [为什么要用ET模式](#为什么要用et模式)
  - [两个触发模式的应用场景](#两个触发模式的应用场景)
- [反爬](#反爬)
  - [解决页面抓取过程中`cookie`失效问题，以达到反爬攻克的目的](#解决页面抓取过程中cookie失效问题以达到反爬攻克的目的)

------

## 项目介绍

[项目概述及准备工作](https://yincheng.blog.csdn.net/article/details/38881577)

我们主要是做一个店铺选址相关的搜索系统，网络爬虫是搜索引擎抓取系统的重要组成部分。通过将互联网上的网页或与店铺选址相关的信息收集到本地，然后对这些信息创建索引，当用户输入查询请求的时，先对用户的查询请求进行分析，然后在索引库中进行匹配，最后对结果进行处理，返回结果。

[网络爬虫的结构与工作流程](https://yincheng.blog.csdn.net/article/details/38883047)

首先从一个初始的 URL 集合出发，将这些 URL 全部放入到一个有序的待提取URL队列里；然后从这个队列里按顺序取出 URL，通过 Web 上的协议，获取 URL 所指向的页面，从这些已获取的页面中分析提取出新的 URL，并将它们继续放入到待提取URL队列里，一直重复上述过程，获取更多的页面。

------- 

[子模块参考](https://www.pianshen.com/article/8423557212/)

### Fetcher和 DNS Resolver

这两个模块是两个非常简单的独立的服务：`DNS Resolver`负责域名的解析；`Fetcher`的输入是域名解析后的`url`，返回的则是该`url`对应的网页内容。对于任何一次网页的抓取，它都需要调用这两个模块。

一开始我们的设计是将这两个模块合并到一起，但是后来成了整个系统的性能的瓶颈，因为爬虫系统是一个对性能要求很高的系统。主要原因是无论是域名解析还是抓取，都是很耗时的工作。比如抓取网页，一般的延迟都在几百毫秒级别，如果遇上慢的网站，可能要几秒甚至十几秒，这导致工作线程会长时间的处于阻塞等待的状态。如果希望`Fetcher`能够达到每秒几千个网页甚至更高的下载，就需要启动大量的工作线程。

因此，后面考虑到爬虫系统的性能，我们采用`epoll`技术将两个模块改成异步机制。另外，爬虫程序还啊要对`DNS`请求做一些优化。

> 对于`DNS`的解析结果也会缓存下来，大大降低了`DNS`解析的操作。==> 项目中如何使用 `DNS`

### Content Seen

Internet 上的一些站点常常存在着镜像网站（mirror），即两个网站的内容一样但网页对应的域名不同。这样会导致对同一份网页爬虫重复抓取多次。为了避免这种情况，对于每一份抓取到的网页，它首先需要进入`Content Seen`模块。该模块会判断网页的内容是否和已下载过的某个网页的内容一致，如果一致，则该网页不会再被送去进行下一步的处理。这样的做法能够显著的降低爬虫需要下载的网页数。

至于如果判断两个网页的内容是否一致，一般的思路是这样的：并不会去直接比较两个网页的内容，而是将网页的内容经过计算生成`FingerPrint`（信息指纹），`FingerPrint`是一个固定长度的字符串，要比网页的正文短很多。如果两个网页的`FingerPrint`一样，则认为它们内容完全相同。

### Extractor和Url Filter

`Extractor`的工作是从下载的网页中将它包含的所有`url`提取出来。这是个细致的工作，你需要考虑到所有可能的`url`的样式，比如网页中常常会包含相对路径的`url`，提取的时候需要将它转换成绝对路径。

`Url Filter`则是对提取出来的`url`再进行一次筛选。不同的应用筛选的标准是不一样的，比如对于`baidu/google`的搜索，一般不进行筛选，但是对于垂直搜索或者定向抓取的应用，那么它可能只需要满足某个条件的`url`，比如不需要图片的`url`，比如只需要某个特定网站的`url`等等。`Url Filter`是一个和应用密切相关的模块。

### Url Seen

`Url Seen`用来做`url`去重。关于`url`去重之后会介绍，这里就不再详谈了。

对于一个大的爬虫系统，它可能已经有百亿或者千亿的`url`，新来一个`url`如何能快速的判断`url`是否已经出现过非常关键。因为大的爬虫系统可能一秒钟就会下载几千个网页，一个网页一般能够抽取出几十个`url`，而每个`url`都需要执行去重操作，可想每秒需要执行大量的去重操作。因此`Url Seen`是整个爬虫系统中非常有技术含量的一个部分。（`Content Seen`其实也存在这个问题）

### Url Set

当`url`经过前面的一系列处理后就会被放入到`Url Set`中等待被调度抓取。因为`url`的数量很大，所以只有一小部分可能被放在内存中，而大部分则会写入到硬盘。`Url Set`的实现就是一些文件存储。

### URL Frontier

`URL Frontier`之所以放在最后，是因为它可以说是整个爬虫系统的引擎和驱动，组织和调用其它的模块。

当爬虫启动的时候，`Froniter`内部会有一些种子`url`，它先将种子`url`送入`Fetcher`进行抓取，然后将抓取下来的网页送入`Extractor`提取新的`url`，再将新的`url`通过`url seen`去重后放入到`Url Set`中；而当`Froniter`内部的`url`都已经抓取完毕后，它又从`Url Set`中提取那些新的没有被抓取过的`url`，周而复始。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/爬虫Frontier子模块调度.5fbbwaw9kco0.png)

`Frontier`的调度实现有很多种，这里只介绍最常见的一种实现方法,也是我们采用的方法。

在此之前，需要先解释一点，尽管在介绍`Fetcher`的时候我们说，好的`Fetcher`每秒能够下载百千个网页，但是对于某个特定的目标网站，比如`www.lepu.com`，爬虫系统对它的抓取是非常慢速的，十几秒才会抓取一次，为了保证目标网站不至于被爬虫给抓垮。
`Frontier`内部对于每个域名有一个对应的`FIFO`队列，这个队列保存了该域名下的`url`。`Frontier`每次都会从某个队列中拿出一个`url`进行抓取。队列会保存上一次被`Frontier`调用的时间，如果该时间距离现在已经超过了一定值，那么该队列才可以再次被调用。

`Frontier`内部同时可能拥有成千上万个这样的队列，它会轮询的获取一个可以被调用的队列，然后从该队列中`pull`一个`url`进行抓取。而一旦所有队列中的`url`被消耗到一定程度，`Frontier`又会从`Url Set`中提取一批新的`url`放入对应的队列。

--------

## 主程序流程

[主程序流程](https://yincheng.blog.csdn.net/article/details/38887017)

1、  解析命令行参数，并根据参数跳转到相应的处理分支

2、  解析配置文件

3、  载入处理模块

4、  加载种子URL

5、  启动抓取任务

#### 将程序设计成为守护进程

> 详细参考 网络通信项目

## DNS 解析

[DNS解析](https://yincheng.blog.csdn.net/article/details/38964369)

**爬虫程序优化DNS请求的必要性**

当爬虫程序从HTML页面上提取URL时，一般情况下，应该有很多都是这个站点内部的URL。那么当这个站点里的某个URL请求过DNS以后，就应该采用某种数据结构把这个值保存起来。以后再发现这个站点内的URL后，就把域名部分的IP取出给这个URL，从而减少不必要的DNS请求。这样可以大大加快抓取页面的速度，提高效率。

**优化DNS请求的方式**

对于同一个站点内部的页面应该设一个哈希表的数据结构以保存，并且方便查找。这样，当爬虫系统得到了一个新的 URL 以后，可以快速的转换地址格式并进行连接。

### URL 处理

> https://yincheng.blog.csdn.net/article/details/38964439

> 爬虫系统要处理的URL是 HTTP 协议的 url，其端口号是 80，第三部分就是资源在服务器上的地址，也就是相对路径。一般来说，从网页上提取出来的 url 如果是站点内部的 url，那么很多都是相对地址，所以我们需要使用一些过滤技术对这些url进行编码，规范化。     
> 而且还需要对这些 url 进行调度处理，因为从页面中提出来的 url 是各种各样的，有站点内部的，站点外部的，有效的，无效的。所以我们要对这些url进行优先级设计，给他们设置级别权重。就比如哪些是DNS已经获得到可以直接建立链接的，哪些是无效或者错误需要丢弃的。另外，我们还设置合适的存储空间，就是避免把所有的url堆积到一块。我们采用的方法是：
> - 采用URL的多级存储结构。 
> - 并且给每一级结构都分配不同的调度优先级。
> - *各个级别的结构之间进行合适的数据通信*。

要实现前面 DNS 的无重复有效请求，那么在这个部分里设置一个 Nsite 类，当一个站点请求过 DNS 后，就把返回的 IP 保存到这个类里，那么再有这个站点内的 URL 出现（域名部分相同），就可以使用这个 IP，而不必重复请求。

为了从一个 URL 中很快的找到其对应的 Nsite 还应该设置一个 hash 表 ，

## 处理http

对`HTTP`头进行分析，用字符串解析出来，按行读取，找头的属性值，找到`http`头的每个属性，然后从后边把这个值提取出来，我们只要用到了两个值，分别是：**文件的类型**（`Accept：text/html`)，**状态码**，判断页面是否下载成功。其他的说忘了。

### HTTP常见字段

- Host: 客户端发送请求时，用来指定服务器的域名。有了host字段，就可以将请求发往**同一台服务器上的不同网站**了。'Host: www.AAA.com'
- Content-Length: 表示服务器返回的数据长度
- Connection: 一般用于客户段要求服务端使用 TCP 持久连接。HTTP/1.1 默认是持久连接，但是为了兼容老版本的 HTTP，需要制定 Connection 首部字段的值为 Keep-alive
- Content-Type: 告诉客户端，本次数据时什么格式，也就是数据类型。
- Content-Encoding: 表示服务器返回的数据用了什么压缩方法，比如：gzip。


## 使用正则表达式抽取HTML正文和URL

> https://yincheng.blog.csdn.net/article/details/38965253

正则表达解析部分主要是从网上找的，并结合 C 语言自带的一些正则表达式函数。

C 语言处理正则表达式常用的函数有`regcomp()`、`regexec()`、`regfree()`和`regerror()`，一般分为三个步骤，如下所示：


C语言中使用正则表达式一般分为三步：

- 编译正则表达式 regcomp()
- 匹配正则表达式 regexec()
- 释放正则表达式 regfree()

## 解析 HTML 页面

HTTP协议支持**文本**和**二进制文件**传输。最常见的 html 格式的页面即文本，图片、音乐等为二进制文件。我们要对这两类文件加以区分并分别处理。

### 使用 base64 传输二进制数据

> https://yincheng.blog.csdn.net/article/details/38965511

用`http`传输二进制的数据时，需要将二进制做一下转化，例如传输的int类型，将`int`类型之间转为`char`以后，丢失掉了长度的信息，如数字`123456`，本来只有`4`个字节，但是转化成文本的“`123456`”是有`7`个字节。在`int`类型的时候固然好办，但是一个数组的时候，经过转化以后，在转化回来就很麻烦了。

所以，只能用`base64`来解决这个问题。`base64`将二进制的内容转化成一组有意义的字符串，然后传输，`server`在`decode`。

### 关于页面的存储

在爬虫系统中数据的流量相当大，要处理的数据内容不仅包括爬虫系统的各种数据结构空间，而且包括从外部节点中得到的各种数据，比如 HTTP 请求，HTML 页面，ROBOT.TXT等等。如果对这些内容处理不当，那么不仅造成空间的**冗余浪费**，使爬虫程序效率降低，而且还可能会使系统崩溃。所以，要有合适的空间分配策略。

**空间分配与管理方案**

在内存中使用**缓冲空间**，以快速的得到、存储数据。

统一各种请求的结构，应该合理利用并在每次用完后进行回收。比如，ROBOT.TXT文件，HTTP请求头及相应的应答。这种方式，在站点数目数量相当庞大的情况下，非常有必要。

在页面的抓取部分和保存部分之间设置合适的接口，直接进行数据交换，从而使系统不必分配更多的空间来缓冲数据。数据缓冲功能由保存部分内部实现。

## Epoll 相关

> https://yincheng.blog.csdn.net/article/details/38965667

由于要实现爬虫程序的快速抓取，显然如果采用阻塞型的 I/O 方式，那么系统可能很长时间都处在等待内核响应的状态中，这样爬虫程序将大大地降低效率。然而，如果采用非阻塞 I/O ，那么就要一直调用应用进程，反复对内核进行轮询。为了实现发送出系统调用请求，而不必一直返回进行查询，最合适的方案应该是采用 epoll 函数，对系统调用实行轮询，也就是 I/O 复用模式。提高抓取的效率。

## 定制规则扩展为垂直搜索

> https://yincheng.blog.csdn.net/article/details/38966239

在垂直搜索的索引建立之前，我们需要到垂直网站上抓取资源并做一定的处理。垂直搜索与通用搜索不同之处在于，通用搜索不需要理会网站哪些资源是需要的，哪些是不需要的，一并抓取并将其文本部分做索引。而垂直搜索里，我们的目标网站往往在某一领域具有其专业性，其整体网站的结构相当规范(否则用户体验也是个灾难，想想东一篇文章西一篇文章基本没人会喜欢)，并且垂直搜索往往只需要其中一部分具有垂直性的资源，所以**垂直爬虫相比通用爬虫更加精确**。


**垂直爬虫爬取资源步骤**：

首先选定需要抓取的目标网站，输入数据库的站源表 sitelist，然后 url crawler 会读取出来存入 map，并提出对应站点的正则解析规则。

然后根据事先制定的 url 列表页正则表达式，url crawler 到列表页爬取列表并提取出来存入资源 url 表 urllist，当中涉及一些列表页分页功能，具体视每个网站分页url规则而定。

从数据库的资源 url 表读出 urls 及其资源页的爬取规则，存入一个同步的队列中(一般做法会将 url 做 md5「加密算法」 处理，用于去重，以免重复爬取相同 url，浪费资源)，多线程下的每个爬虫程序将从此队列读取 urls (若队列为空线程将进入等待)，然后爬取每个资源页并保持页面。

最后根据爬取到的页面，进行进一步的处理。


## 线程池

> https://yincheng.blog.csdn.net/article/details/39039145

**使用线程池解决多任务抓取问题**

> 一旦有一个抓取请求开始，就创建一个新的线程，由该线程执行任务，任务执行完毕之后，线程就退出。这就是"即时创建，即时销毁"的策略。尽管与创建进程相比，创建线程的时间已经大大的缩短，但是如果提交给线程的任务是执行时间较短，而且执行次数非常频繁，那么服务器就将处于一个不停的创建线程和销毁线程的状态。这笔开销是不可忽略的，尤其是线程执行的时间非常非常短的情况。

线程池就是为了解决上述问题的，**它的实现原理是这样的**：

在应用程序启动之后，就马上创建一定数量的线程，放入空闲的队列中。这些线程都是处于阻塞状态，这些线程只占一点内存，不占用`CPU`。当任务到来后，线程池将选择一个空闲的线程，将任务传入此线程中运行。当所有的线程都处在处理任务的时候，线程池将自动创建一定的数量的新线程，用于处理更多的任务。执行任务完成之后线程并不退出，而是继续在线程池中等待下一次任务。当大部分线程处于阻塞状态时，线程池将自动销毁一部分的线程，回收系统资源。

下面是通过一个线程池的实现，来解决多任务抓取问题。

**处理流程如下**：

程序启动之前，初始化线程池，启动线程池中的线程，由于还没有任务到来，线程池中的所有线程都处在阻塞状态，当一有任务到达就从线程池中取出一个空闲线程处理，如果所有的线程都处于工作状态，就添加到队列，进行排队。如果队列中的任务个数大于队列的所能容纳的最大数量，那就不能添加任务到队列中，只能等待队列不满才能添加任务到队列中。

## 遵守robots.txt

> https://yincheng.blog.csdn.net/article/details/39039625

`Robots`协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过`Robots`协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。

当一个搜索访问一个站点时，它会首先检查该站点根目录下是否存在`robots.txt`，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索将能够访问网站上所有没有被口令保护的页面。但是还参考了百度官方建议，仅当您的网站包含不希望被搜索引擎收录的内容时，才需要使用`robots.txt`文件。如果您希望搜索引擎收录网站上所有内容，请勿建立`robots.txt`文件。

**Robots 协议好处**

Robots协议用来告知搜索引擎哪些页面能被抓取，哪些页面不能被抓取；可以屏蔽一些网站中比较大的文件，如：图片，音乐，视频等，节省服务器带宽；可以屏蔽站点的一些死链接。方便搜索引擎抓取网站内容；设置网站地图连接，方便引导蜘蛛爬取页面。

------

## 遇到问题怎么排查，遇到什么困难。

其实困难还是很多的。当时因为很多不会经常都是自己加班到十二点来解决的。

- 遇到问题先是查看日志吧。输出log永远是最简单快捷的调试方式，可以快速定位`bug`，通过设置日志级别控制日志的输出详略程度，结合一些文本分析工具`awk/sed/grep`可以快速在大量日志中找到错误信息。

- 还有的话可以借助一些命令，比如说当时用到，`pstack`,用来跟踪进程栈，比如我们发现一个服务一直处于work状态（如假死状态，好似死循环），使用这个命令就能轻松定位问题所在；可以在一段时间内，多执行几次`pstack`，若发现代码栈总是停在同一个位置，那个位置就需要重点关注，很可能就是出问题的地方；

- 还有`gdb`,经典的调试工具，功能很强大，注意此时编译的时候应该使用`-g`选项，并用`-Og`进行优化。多线程下可以`attach`到进程去来调试。

- 还有经常使用的就是`memcheck`工具，在`c++`中指针的使用，一不留神就会产生异常，就可以利用`memcheck`进行检查。个人一般用--`track-origins=yes`来定位未初始化变量的位置。
- 还有`tcpdump`,抓包用的，在开发网络应用的时候很给力,结合`awk/sed/grep`可以快速查找网络数据包。
- 对了当时还发现一个网站，`stackoverflow`: 这个网站是个程序设计领域的问答网站，基本碰到的问题都能在这里面找到答案！ 技术氛围很强，从中能学到很多东西。

## epoll的触发模式是什么

ET模式，边缘触发模式。

### 为什么要用ET模式

要求请求被一次处理，不再进行通知。减少事件遍历个数。

### 两个触发模式的应用场景

- 我觉得LT模式适合请求不能被忽略或者错过的场景，并发量高但是处理能力较低的场景。因为LT会重复通知请求事件。
- ET的话适用于处理能力较强的场景。

请求量大但简单的场景适合用 **ET模式**。


## 反爬

反爬策略

1.通过 UA 限制或者其他头信息限制

解决方案：构建用户代理池，或其他头信息（爬虫糗事百科）

2.通过访问者IP限制

解决方案：构建 IP 代理池

3.通过验证码限制

解决方案：手工打码、验证码接口自动识别或者通过机器学习自动识别

4.通过数据的异步加载限制

解决方案：抓包分析或者使用PhantomJS(如淘宝爬虫)

5.通过 Cookie 限制

解决方案：进行 Cookie 处理（爬知乎）

6.通过 JS 限制（如请求的数据通过JS随机生成等）

解决方案：分析 JS 解密或者使用 PhantomJS (爬裁判文书网、腾讯动漫)

### 解决页面抓取过程中`cookie`失效问题，以达到反爬攻克的目的

之前使用爬虫时，最让我头疼的就是cookie失效的问题了。由于有的cookie存在时效问题。一段时间后，cookies会失效。失效后，爬虫碰到的页面就基本上是重新登录的页面了。这时候就需要cookie自动的更新了。通常怎样自动更新cookie呢？这里会用到 **selenium**。

步骤1、 采用 selenium 自动登录获取cookie，保存到文件;

步骤2、 读取cookie，比较cookie的有效期，若过期则再次执行步骤1；

步骤3、 在请求其他网页时，填入cookie，实现登录状态的保持。


> 参考：https://blog.csdn.net/cheyuan4575/article/details/100720308










