

- [项目介绍](#项目介绍)
- [设置进程名称](#设置进程名称)
  - [怎么修改进程名称](#怎么修改进程名称)
- [日志打印实现](#日志打印实现)
  - [printf 的实现](#printf-的实现)
  - [日志等级划分](#日志等级划分)
  - [日志输出时遇到了问题 (行缓存造成日志输出混乱问题)](#日志输出时遇到了问题-行缓存造成日志输出混乱问题)
  - [日志写入混乱问题 和 掉电导致数据丢失问题](#日志写入混乱问题-和-掉电导致数据丢失问题)
  - [怎么解决掉电导致数据丢失问题](#怎么解决掉电导致数据丢失问题)
    - [fwrite 和 write 有什么区别](#fwrite-和-write-有什么区别)
    - [fwrite 实现原理](#fwrite-实现原理)
- [守护进程以及信号处理](#守护进程以及信号处理)
    - [守护进程的实现](#守护进程的实现)
    - [避免子进程变成僵尸进程](#避免子进程变成僵尸进程)
    - [守护进程不会收到的信号](#守护进程不会收到的信号)
    - [守护进程 和 后台进程的区别](#守护进程-和-后台进程的区别)
- [两个小思考](#两个小思考)
- [SYN 攻击](#syn-攻击)
  - [模拟TCP 半连接溢出，制造 SYN 攻击](#模拟tcp-半连接溢出制造-syn-攻击)
  - [TCP 半连接队列的最大值是如何决定的](#tcp-半连接队列的最大值是如何决定的)
  - [如何防御 SYN 攻击](#如何防御-syn-攻击)
- [Epoll技术简介](#epoll技术简介)
  - [epoll_create 函数](#epoll_create-函数)
  - [epoll_ctl 函数](#epoll_ctl-函数)
  - [epoll_wait 函数](#epoll_wait-函数)
    - [关于 epitem 节点](#关于-epitem-节点)
- [向内核双链表增加节点](#向内核双链表增加节点)
- [使用 epoll 函数来实现数据的收发](#使用-epoll-函数来实现数据的收发)
  - [创建连接池的目的](#创建连接池的目的)
  - [ET 和 LT 模式](#et-和-lt-模式)
    - [LT 模式-水平触发](#lt-模式-水平触发)
    - [ET 模式-边缘触发](#et-模式-边缘触发)
  - [事件驱动](#事件驱动)
  - [腾讯面试题](#腾讯面试题)
  - [深入理解ET LT](#深入理解et-lt)
  - [Epoll 中 ET 和 LT 模式的处理编码不同](#epoll-中-et-和-lt-模式的处理编码不同)
  - [TCP 粘包和缺包](#tcp-粘包和缺包)
    - [客户端粘包](#客户端粘包)
    - [服务器粘包](#服务器粘包)
    - [缺包](#缺包)
    - [粘包和缺包问题解决](#粘包和缺包问题解决)
  - [收包分析](#收包分析)
    - [字节对齐问题](#字节对齐问题)
    - [怎么处理数据包过期问题](#怎么处理数据包过期问题)
    - [服务端怎么识别客户端断线问题](#服务端怎么识别客户端断线问题)
  - [收包流程](#收包流程)
- [多线程相关](#多线程相关)
- [SYN 攻击和压力测试](#syn-攻击和压力测试)

-------

## 项目介绍

本科时候发了一篇 《即时通信设计与实现》论文，但是当时完成这篇论文发现项目服务器方面有很多问题没有解决，然后我目前的课题是左环境监控这一块的项目嘛，所以需要一个支持 tcp 长连接的服务器，于是在读研阶段就本着完善的目的做了这个**网络通信服务器项目**，这个项目大部分是参考 Nginx 的思想来实现的。期间也是边学边实现吧，看了很多服务器相关的书和博客。

这个项目主要是面向客户端和服务端之间保持 TCP 长连接，并且客户端和服务端之间不算太频繁的进行数据收发的场景，比如网络游戏，视频监控等等。

服务器后端的处理方式是 socket 通信，利用多路 IO 复用，可以同时处理多个请求，请求的解析使用预先准备好的线程池，使用模拟 proactor 模式，主线程负责监听，监听到有事件之后，从 socket 中循环读取数据，然后将读取到的数据封装成一个请求对象插入请求队列。睡眠在请求队列上的工作线程被唤醒进行处理。

- 日志模块 分为同步日志和异步日志，异步日志利用阻塞队列，先将日志放入阻塞队列中，然后利用条件变量将日志添加到对应文件中。采用单例模式。

- 用定时器处理非活动链接，定时器容器利用升序链表进行设计。

- 在项目开发过程中也遇到了很多问题，比如在一开始的时候为了让进程 CMD 名称显示更加规范，直接通过命令传入参数作为名称，导致了环境变量被覆盖问题

- 在打印日志的时候，日志混乱等问题。

因此从中学到了很多独立分析和解决问题的方法。



------

## 设置进程名称

> [进程名称 参数被覆盖问题](/Cpp项目/通讯实战项目/note?id=设置进程名称)

进程名称实际上是保存在 `argc[0]` 所指向的内存中。CMD 会把 argv 所指向的命令参数全部显示出来，因为我们设置的进程名称是 `./nginx` 是保存在 `argv[0]`中，所以 `argv[0]`改变，进程名也就改变了。

> 在这里遇到了个问题，一旦设置的进程名称的长度大于字符串 `./nginx`的长度，就可能导致设置的进程名称覆盖其他参数。

由于环境变量信息也是保存在内存中的，并且**保存的位置紧紧邻 argv 所指向的内存**。所以若果设置的进程名称太长，不但会覆盖掉命令行参数，而且很可能覆盖掉环境变量所指向的内容。

为此，借助了 nginx 的源码，想到了一个解决方案，就是将环境变量搬家，大致思路是：

- **重新分配一块内存**：足够容纳新的 environ 所指向的内容，把 environ 内容搬到这块内存中来。

- 将以往 `argv[0]` 指向的内容替换成实际要修改的新进程名称

### 怎么修改进程名称

大致逻辑：

- 计算进程名称的长度

- 计算命令行参数所占内存与环境变量所占内存的总和

- 设置新的进程名称

------

## 日志打印实现

> [日志打印实现](/Cpp项目/通讯实战项目/note?id=日志打印实现)

打印输出相关函数借鉴了 nginx 的实现，并做一些改动，这部分主要是 学习 printf,vprintf 这类函数的内部实现。

### printf 的实现

C 语言中`printf`函数的**参数是可变**的，是通过**栈**实现参数的传递。`printf`至少有一个参数，就是字符串指针，如果还有其他参数，比如

`printf("a=%d b=%d",i,j); `会**从右往左**依次把参数压入栈内，先压`j`然后压`i`，然后压这个串`"a=%d b=%d"`的首地址也压入栈。在进行解析的时候是后入先出，按照`%d`去寻找后面的参数。这样其实`printf`并不知道一共有几个参数,完全按照`%d`或者其他格式的类型去处理。这就有个问题就是要特别注意变量定义的类型一定要与格式控制符表示的格式一致,不一致会出现读取错误.当然`printf`**在转换参数时，对栈只读不写**，不会造成栈错误。

### 日志等级划分

参考 nginx 的日志等级划分，nginx 日志分成 8 个等级，级别从高到低，数字最小的级别最高，数字最大的级别最低，nginx 中有专门的日志处理模块处理日志(*很复杂，没看*)

```
#define NGX_LOG_STDERR            0    //控制台错误【stderr】：最高级别日志，日志的内容不再写入log参数指定的文件，而是会直接将日志输出到标准错误设备比如控制台屏幕
#define NGX_LOG_EMERG             1    //紧急 【emerg】
#define NGX_LOG_ALERT             2    //警戒 【alert】
#define NGX_LOG_CRIT              3    //严重 【crit】
#define NGX_LOG_ERR               4    //错误 【error】：属于常用级别
#define NGX_LOG_WARN              5    //警告 【warn】：属于常用级别
#define NGX_LOG_NOTICE            6    //注意 【notice】
#define NGX_LOG_INFO              7    //信息 【info】
#define NGX_LOG_DEBUG             8    //调试 【debug】：最低级别
```

### 日志输出时遇到了问题 (行缓存造成日志输出混乱问题)

`printf` 函数不加“`\n`”无法及时输出，就是说，我们在实现`ngx_vslprintf`函数 [打印日志] 测试的时候，等待了好几秒，发现屏幕上迟迟没有日志输出的结果，然后突然之间，在屏幕上出现一大堆输出结果。

后来查了 `printf()`底层实现后发现，这是 **行缓存（输出缓冲区)的问题**，标准输入输出函数都是带有缓存的，一般是行缓存（还发现 window 系统上没有这个问题，但是 Unix 系统就有），就是把需要输出的数据先缓存到某个地方，等待 **行刷新标志** 或者 **缓存已满** 的情况下，才会把缓存的数据显示出来。

“`\n`” 可以认为是刷新标志，也可以通过调用 `fflush(stdout)` 函数刷新缓冲区，将结果显示出来。

### 日志写入混乱问题 和 掉电导致数据丢失问题

本项目是 1 个 master 进程， 4 个 worker 进程，假如 5 个进程间同时不停地调用，同时向日志文件中写日志，就会造成日志文件混乱问题

首先先在 master 进程和 worker 进程的功能函数中添加一条日志输出，返发现并没有出现混乱现象，所以初步确定 写日志代码在应对多个进程向同一个日志文件中写日志的时候是没有问题的。

当时还专门去研究了 write 函数内部是怎么解决这个问题的？然后参考这个方案在项目中实现。

> **write 内部是如何解决多进程同时写一个文件不出现混乱问题的呢**？

- 在日志初始化函数中调用了 open 函数，open 函数中的 O_APPEND 标志可以保证多个进程操作同一个文件的时候不会相互覆盖，如果不加这个标记，某些情况下就会出现数据彼此覆盖的问题。

- write 函数在写入文件的时候是原子操作，2 个进程同时写入是竞争关系，最终只会由某个进程写入数据。

- 父进程 fork 出子进程，在父进程都会执行的公共代码就已经调用了 open 函数打开了日志文件，然后才通过 fork 创建出子进程，这种父子进程之间会共享文件表项，文件表项里有当前文件偏移量，子进程用 write 原子操作写了一个日志，文件偏移量会移动到文件末尾，父进程的当前文件偏移量也会移动到文件末尾，因为是共享文件表项，所以父进程 write 是接着子进程写的内容末尾开始写，因此不会混乱。

> 所以本项目中利用这种机制解决了写日志时日志混乱的问题

### 怎么解决掉电导致数据丢失问题

> 这里就需要涉及到 write 内部的写数据过程

内核可以在任何时候写磁盘，但并不是所有的 write 操作都会导致内核的写操作，内核会把待写数据暂存在缓冲区，积累到一定数量后再一次性写入磁盘，如果出现意外，断电，计算机崩溃等，内核还没来得及把内核缓冲区的数据写入磁盘，这些数据就会丢失。

为了确保内核缓冲区中的数据被及时写入磁盘，内核缓冲区中设立了一个时间上限，达到时间上限后，内核会把所有内核缓冲区中的“脏数据”直接写到磁盘。

> 怎么解决掉电导致 write 写入的数据丢失


- 直接 `I/O`,直接访问物理磁盘，但是这样效率会降低。

本项目的 `ngx_log.cxx` 中的 `ngx_log_init` 函数

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT,0644);  
```

如果 open 参数增加 O_DIRECT 就会绕过缓冲区

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_CREAT|O_DIRECT,0644);  
```

- 设置 open 文件时的 O_SYNC 选项

O_SYNC 选项也叫**同步选项**，只针对 write 函数有效，使每次 write 操作等待物理 `I/O` 操作完成。也就是说将写入缓冲区的数据立即写入磁盘，而不用等到时间上限，这样将计算机崩溃或者断电时造成的数据丢失减到最小。

也是通过更改 open 第二个参数实现

```cpp
ngx_log.fd = open((const char *)plogname,O_WRONLY|O_APPEND|O_SYNC,0644);  
```

但是直接向磁盘写数据的效率不高，因此磁盘是按 页 或者 扇区 来写数据的，而且还要进行磁盘寻道，也就是说要找到写的位置，这些都需要花时间。

所以使用 O_SYNC 标记写数据时要批量写，不要每次只写几个字节。

- 缓冲同步

这是最推荐的方法，项目中也是使用这种方法

这里涉及到 3 个函数：`sync`,`fsync`,`fdatasync`

(1) sync(void): 将所有修改过的块缓冲区排入写队列，然后立即返回，不等待实际写磁盘操作。但是数据是否写入磁盘并没有保障

(2) fsync(int fd): 将 fd 对应文件的缓冲区理解写入磁盘，并将等待实际写磁盘操作结束后返回。可用于数据库这样的应用程序，因为这种应用程序需要确保修改过的数据理解写到磁盘上。

(3) fdatasync(int fd): 类似于 fsync，但只影响文件的数据部分。除数据外，fsync 还会同步更新文件属性（如文件大小，文件访问时间等。文件属性和文件内容是分开存储的，写磁盘会涉及 2 次寻道）。所以 `fdatasync`比 `fsync` 速度更快。

我们采取的方法是：

调用多次 write 函数，在调用 1 次 fsync 函数，因为频繁调用 fsync 函数效率会很低。

如果文件很大，就都写完，然后调用 1 次 fsync 函数

还有如果整个文件需要调用 write 函数 10 次才能写完，那么每写 1 次，就调用 fsync 函数 1 次意义就不大， 所以应该写 10 次后，再调用 fsync 函数 1 次。

> 本项目中写日志使用 write 系统调用，工作没有问题，当时还尝试了使用 fwrite 来写日志，就会出现日志混乱问题。

#### fwrite 和 write 有什么区别

read write 这类函数时属于 **系统调用**，

而 fwrite printf 属于标准 IO 库里面的函数，**内部实现有缓冲区的，此时写日志可能就要用到锁机制**。

#### fwrite 实现原理

当调用 fwrite 函数的时候，写入的内容会被放入一个系统的 CLib 缓冲区中（可以理解成 stdio 这个库里面提供的缓冲区）。当 CLib 缓冲区满之后，会将内容移至内核缓冲区。所以这里相当于在应用程序和内核之间加了一层。所以 IO 库函数相当于一层用于缓存，最终还是调用底层 IO，也就是系统调用来实现相关功能。

所谓的缓存就是内存，用于在输入输出设备和 CPU 之间临时保存数据，使低速输入输出设备和高速输入输出设备能够协调工作，避免低速的输入输出设备占用 CPU，解放出 CPU，使其能够高效工作。

## 守护进程以及信号处理

#### 守护进程的实现

`ngx_deamon.cxx` 文件实现守护进程。`ngx_deamon()` 是核心函数，

读取配置文件，拿到配置文件中是否按守护进程方式启动的选项。

**守护进程是实现逻辑**：

- fork 出了一个子进程，核心目的是让这个子进程作为 master 进程，旧进程需要退出

- 通过`setsid()`让子进程脱离终端，
  
- 设置 `umask(0)`,不要让它来限制文件权限，以免引起混乱

- 以读写方式打开黑洞设备,`open("/dev/null", O_RDWR);`

**守护进程的调用时机**：

我们在创建 worker 子进程之前（也就是在调用`ngx_master_process_cycle()`函数之前）、日志初始化之后调用`ngx_deamon()`函数创建守护进程。

#### 避免子进程变成僵尸进程

> 使用 `kill -9` 命令杀死一个 worker 进程后，再次使用 ps 命令查看发现这个 worker 进程并没有消失，而是变成了僵尸进程。

**解决方案**

在父进程中接收 SIGCHILD 信号并在其中调用 waitpid 来解决这个问题

信号处理函数：`ngx_signal_handler()`

大致的逻辑是

- 在函数中，通过一个 for 语句 `for (sig = signals; sig->signo != 0; sig++)` 遍历信号数组，寻找收到的信号
- 针对 master 进程，在收到 SIGCHLD 信号(子进程退出，父进程就会收到这个信号)后，会将一个全局标记（全局变量）`ngx_reap`设置为 1，目前本项目中并没有用到这个标记，但是将来可能会用到，（如果 worker 子进程运行崩溃或者被杀掉后，master 进程就可以通过该标记重新 fork 出一个子进程）
- 真正处理僵尸进程是通过调用 `ngx_process_get_status` 函数并在该函数中调用了系统函数 waitpid 来进行的。当时参考了 nginx 的源码，在这个函数中引入了一个 for 无限循环的写法，for 无限循环会执行 2 次，第一次循环执行 waitpid 时返回大于 0，所以再次循环执行 waitpid 时返回结果等于 0 并直接返回。

这样就实现了子进程一旦死掉之后，父进程可以通过调用 waitpid,防止子进程变成僵尸进程。

#### 守护进程不会收到的信号

- 守护进程不会收到来自内核的 SIGHUP「连接（终端）断开」 信号，也就是说如果收到了 SIGHUP 信号肯定是前台进程发来的。

- 守护进程也不会收到来自内核的 SIGINT「终端终端， Ctrl+C」 信号，也就是说如果守护进程收到了 SIGINT 信号那就是前台进程发送嘎来的。

- 还有 SIGWINNCH 信号「改变终端窗口大小」。

#### 守护进程 和 后台进程的区别

- 守护进程和终端不挂钩，不向终端输出内容，后台进程是能向终端输出内容的（比如 `printf()` )
- 守护进程在关闭终端的时候不会受到影响，而后台进程将终端关掉之后就会自动退出

## 两个小思考

(1)如果两个队列之后【已完成连接队列，和未完成连接队列】达到了`listen()`所指定的第二参数，也就是说队列满了；此时，再有一个客户发送 syn 请求，服务器怎么反应？

实际上服务器会忽略这个 syn，不给回应； 客户端这边，发现 syn 没回应，过一会会重发 syn 包；

(2)三次握手完成，连接放到已完成连接队列中，等着 accept 函数从已完成连接队列中把连接取走

当 accept 还没来得急取走这个连接的时候，因为三次握手已经建立了，客户端如果此时发送数据过来，这个数据就会被保存在已经连接的套接字的接受缓冲区中，这个接受缓冲区的大小就是能接受的最大数据量。


## SYN 攻击

如果一个黑客通过一些特殊的手段，伪造自己的 IP 地址和端口号，不停地给服务器发送 SYN 包（也就是三次握手的第 1 次握手包），就会导致服务器未完成连接队列的条目越来越多，当 未完成连接队列 和 已完成连接队列 已满了，服务器就会忽略掉后续再来的 SYN 包，无法建立正常的 TCP 连接了。因为正常 TCP 连接的三次握手的第 1 次握手包（ SYN 包）被忽略了。

> blocklog 指定内核套接字（服务器监听套接字）上内核为其排队的最大已完成连接数（也就是已完成连接队列中允许存放的最大条数）

> 什么是半连接队列？    
> 服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立连接。服务器会把这种状态下请求连接放在一个队列里，我们把这种队列称之为**半连接队列**。     
> **全连接队列**，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。

> 全连接队列的大小，也就是当前已完成三次握手并等待服务端 `accept()` 的 TCP 连接;


> https://blog.csdn.net/qq_34827674/article/details/106448326

### 模拟TCP 半连接溢出，制造 SYN 攻击

实际上就是对服务端一直发送 TCP SYN 包，但是不回第三次握手 ACK，这样就会使得服务端有大量的处于 SYN_RECV 状态的 TCP 连接。

这其实也就是类似 SYN 攻击


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击.1fapi2iway5c.png)

实验环境：

- 客户端和服务端都是 CentOS 6.5 ，Linux 内核版本 2.6.32
- 服务端 IP 192.168.3.200，客户端 IP 192.168.3.100
- 服务端是 Nginx 服务(自己写的服务器），端口为 8088

注意：本次模拟实验是**没有开启** tcp_syncookies，关于 tcp_syncookies 的作用

> 开启 tcp_syncookies 是缓解 SYN 攻击其中一个手段。

**本次实验使用 hping3 工具模拟 SYN 攻击**：

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS83ODYxZWQxNy01YTBkLTQ4YjctYjI3ZS04OTA3NGQ4MTM0NDAucG5n?x-oss-process=image/format,png)

当服务端受到 SYN 攻击后，连接服务端 ssh 就会断开了，无法再连上。只能在服务端主机上执行查看当前 TCP 半连接队列大小：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击02.d58f965ft40.png)

同时，还可以通过 `netstat -s` 观察半连接队列溢出的情况：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击03.43qxy62xfq20.png)

上面输出的数值是累计值，表示共有多少个 TCP 连接因为半连接队列溢出而被丢弃。隔几秒执行几次，如果有上升的趋势，说明当前存在半连接队列溢出的现象。

上面模拟 SYN 攻击场景时，服务端的 `tcp_max_syn_backlog` 的默认值如下

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击04.2dgfb8h08g5c.png)

> 测试的时候发现，使用 netstat -natp 查看 TCP 半连接队列大小的时候，发现是 256，但是查看 `tcp_max_syn_backlog` 的值却是 512，当时还查看了 linux 内核的源码分析了 TCP 半连接队列的最大值是如何决定的。

但是在测试的时候发现，服务端最多只有 256 个半连接队列，而不是 512，所以半连接队列的最大长度不一定由 `tcp_max_syn_backlog` 值决定的。

### TCP 半连接队列的最大值是如何决定的

> 当时还透过 Linux 内核的源码，来分析 TCP 半连接队列的最大值是如何决定的

- 如果半连接队列满了，并且没有开启 tcp_syncookies ,则忽丢弃连接
- 如果全连接队列满了，并且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃连接
- 如果没有开启 tcp_syncookies，并且 max_syn_backlog 减去 当前半连接队列长度小于 `max_syn_backlog >> 2`，则会丢弃连接；


半连接队列最大值不是单单由 `max_syn_backlog` 决定，还跟 `somaxconn` 和 `backlog` 有关系。

>  - 当 max_syn_backlog > min(somaxconn, backlog) 时， 半连接队列最大值 max_qlen_log = min(somaxconn, backlog) * 2;
> - 当 max_syn_backlog < min(somaxconn, backlog) 时， 半连接队列最大值 max_qlen_log = max_syn_backlog * 2;

全连接队列的最大值是` sk_max_ack_backlog` 变量，`sk_max_ack_backlog` 实际上是在 `listen() `源码里指定的，也就是 `min(somaxconn, backlog)`；


> TCP 全连接队列足最大值取决于 somaxconn 和 backlog 之间的最小值，也就是 `min(somaxconn, backlog)`。    
> - somaxconn 是 Linux 内核的参数，默认值是 128，可以通过 `/proc/sys/net/core/somaxconn` 来设置其值；
>- backlog 是 `listen(int sockfd, int backlog)` 函数中的 backlog 大小，Nginx 默认值是 511，可以通过修改配置文件设置其长度；

### 如何防御 SYN 攻击

- 增大半连接队列；
- 开启 tcp_syncookies 功能
- 减少 SYN+ACK 重传次数

**方式一：增大半连接队列**

要想增大半连接队列，我们得知不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大全连接队列。否则，只单纯增大 tcp_max_syn_backlog 是无效的。

增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击05.hoz9lpbx9f4.png)

增大 backlog 的方式，每个 Web 服务都不同，比如 Nginx 增大 backlog 的方法如下：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击06.29esp09qzpq8.png)

最后，改变了如上这些参数后，要重启 Nginx 服务，因为半连接队列和全连接队列都是在 `listen()` 初始化的。

**方式二：开启 tcp_syncookies 功能**

开启 tcp_syncookies 功能的方式也很简单，修改 Linux 内核参数：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击07.5whon2v6pok0.png)

**方式三：减少 SYN+ACK 重传次数**

当服务端受到 SYN 攻击时，就会有大量处于 SYN_REVC 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。

那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结02/syn攻击08.1hvr8wld1r8g.png)



## Epoll技术简介

以了一位网友【[王博靖](https://github.com/wangbojing/NtyTcp)】自己写的一套 Epoll 源码为入口，通过阅读源码学习了 Epoll 函数内部的实现原理。


**Epoll 就是一种在 Linux 上使用的 IO 多路复用并支持高并发的典型技术**。

比如说有 10 万个并发连接（也就是同一时刻有 10 万个客户端保持和服务器的连接），这 10 万个连接通常也不可能同一时刻都在收发数据，一般在**同一时刻通常只有其中几十个或者几百个连接在收发数据，其他连接可能处于只连接而没有收发数据的状态**。

如果以 100ms 为间隔判断一次，可能这 100ms 内只有 100 个活跃连接（就是有数据收发的连接），把这 100 个活跃连接的数据放在一个专门的地方，后续到这个专门的地方来，只需要处理 100 条数据，处理起来的压力就没那么大了。

这也就是 Epoll 的处理方式。而 select 和 poll 是依次判断这 10w 个连接有没有收发数据（可能实际上有数据的只有 100 个连接），有数据就处理。所以不难看出每次检查 10w 个连接与每次检查 100 个连接相比，浪费了巨大的资源和时间。

> 实际上，`epoll` 在内核里使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的 `socket` 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 `select/poll` 每次操作时都传入整个 `socket` 集合，只需要传入一个待检测的 `socket` 就可以了，减少了内核和用户空间大量的数据拷贝和内存分配。


此外 Epoll 采用了 **事件驱动机制**，只在单独的进程或者线程里收集和处理各种事件，没有进程或线程之间上下文切换的开销。

> 也就是说，在内核中维护了一个「链表」来记录就绪事件，当某个 `socket` 有事件发生时候，通过回调函数，内核会将这个 事件 加入到 就绪事件 列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件的 socket 文件描述符，不需要像 `select/poll` 那样轮询扫描整个` socket` 集合，大大提高了检测的效率。

`epoll` 通过两个方面，很好解决了 `select/poll` 的问题。

从下图你可以看到 `epoll` 相关的接口作用：

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epoll01.58ud4l3nxm00.png)


源码中 `nty_epoll_rb.c` 和 `nty_epoll_inner.h` 这 2 个文件是 Epoll 相关的 3 个函数的实现文件。

### epoll_create 函数

- **格式**

```c
int epoll_create(int size);  // size 必须 > 0
```

- **功能**：创建一个 Epoll 对象，返回一个对象文件描述符来表示这个 Epoll 对象，后续通过操作这个描述符来进行数据的收发。

这个对象最终要用 close 关闭，因为它是个描述符，或者说是个句柄，总是要关闭的，

- **原理**

执行 `struct eventpoll *ep` 生成一个 `eventpoll` 对象

```c
struct eventpoll *ep = (struct eventpoll*)calloc(1, sizeof(struct eventpoll)); 
```

`eventpoll` 的结构如下。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create01.ik3e4xapf3k.png)

`eventpoll` 的结构中有两个比较重要的成员

(1) `rbr`,可以理解成代表一颗红黑树的根节点（的指针）。

红黑树是一种高效的数据结构，用于保存数据，一般都是存“键值对（`key-value`）”，红黑树的特点是能够快速地根据给的 key 找到并取出 value ，这里的 key 一般是一个数字，而 value 代表的可能是一批数据。**红黑树查找的时间复杂度**是：`O(logn)`

一开始的时候红黑树还是空的，也就是 rbr 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create02.5zdre53dgpo0.png)

(2) `rdlist`，可以理解成代表一个双向链表的表头指针

双向链表能快速顺序地访问里面的节点。

一开始的时候双向链表也是空的，`rdlist` 指向 NULL，还没有节点。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_create03.5zueg4dj5yg0.png)

- **总结**：
  - 创建一个 eventpoll 结构的对象，被系统保存起来
  - 对象中的 rbr 成员被初始化成指向一颗红黑树的根节点，
  - 对象中的 rdlist 成员被初始化成指向一个双向链表的头结点。

### epoll_ctl 函数

- **格式**：

```c
int epoll_ctl (int efpd,int op,int sockid,struct epoll_event *event);
```

- **功能**：

把一个 socket 以及 socket 相关的事件添加到 epoll 对象描述符中，以通过这个 epoll 对象监视该 socket（也就是这个 tcp 连接）上数据的来往情况，当有数据来往时，系统会通知程序。

我们可以通过 `epoll_ctl` 函数吧程序中需要关注的事件添加到 epoll 对象描述符中，当有数据来往时，系统会通知程序。

**epoll_ctl 函数中参数的介绍**：

- `efpd`：`epoll_create()`返回的`epoll`对象描述符
- `op`：一个操作类型，添加/删除/修改 ，对应数字是`1,2,3`. 分别对应： `EPOLL_CTL_ADD`（添加事件）, `EPOLL_CTL_DEL`（删除事件）， `,EPOLL_CTL_MOD`（修改事件）
- `sockid`：表示一个 TCP 连接，添加事件（也就是往红黑树中添加节点）时，就是用 sockid 作为 key 往红黑树中增加节点的。
- `event`: 向 `epoll_ctl` 函数传递信息，比如要增加一些书剑，就可以通过 `event` 参数将具体事件传递进 `epoll_ctl` 函数。

- **原理**：

假如传递进来的是一个 `EPOLL_CTL_MOD` ,首先使用 `RB_FIND` 来查找红黑树上是否已经有了这个节点，如果有了，程序就直接返回，如果没有，程序流程就继续往下走。

>  **EPOLL_CTL_ADD 怎么往红黑树你增加节点**

**确定红黑树没有该节点**的情况下，会生成一个 epitem 对象。

通过执行下面代码创建 `epitem` 对象

```cpp
epi = (struct epitem*)calloc(1, sizeof(struct epitem));
```

这个对象就是后续增加到红黑树中的一个节点，该节点的 key 保存在 sockfd 中，要增加的事件保存在 event 中，然后使用 `RB_INSERT` 宏将该节点插入红黑树中，对于红黑树来说,每个节点都要记录自己的左子树、右子树和父节点，这里是通过 rbn 成员，指向父节点和子节点的。如果将来多个用户连入服务器，需要向这颗红黑树加入很多节点，这些节点彼此也要连接起来。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/epol_ctl01.7d2r4etg4d40.png)

> EPOLL_CTL_ADD：等价于往红黑树中增加节点

> EPOLL_CTL_DEL：等价于从红黑树中删除节点

> EPOLL_CTL_MOD：等价于修改已有的红黑树的节点

**每一个连入客户端都应该调用 `epoll_ctl` 向红黑树增加一个红黑树节点**，如果有 100w 个并发连接，红黑树上就会有个 100w 个节点

### epoll_wait 函数

- **格式**：

```c
int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout);
```

- **功能**：

阻塞一小段时间并等待事件发生，返回事件集合，也就是获取内核的事件通知；

其实就是遍历这个双向链表，把这个双向链表里边的节点数据拷贝出去，拷贝完毕的就从双向链表里移除；因为所有数据的 socket（ TCP 连接）都在双链表里记着。

- 参数`epfd`：是`epoll_create()`返回的`epoll`对象描述符

- 参数`events`：是内存，也是数组，长度 是`maxevents`，表示此次`epoll_wait`调用可以收集到的`maxevents`个已经就绪【已经准备好的】的读写事件；换句话说返回的是有事件发生的 TCP 连接数目

- 参数`timeout`：阻塞等待的时长；

> 总的来说，epoll_wait 函数就是到双链表中去，把此刻同时连入的连接中有事件发生的连接拿出来，后续 read，write，或者 send，secv 之类的函数调用收到数据，某个 socket 只要在双链表中，这个 socket 上一定发生了 某个/某些 事件，也就是说，只有发生了某个/某些 事件的 socket 才会在双向链表中实现。

> 这也就是 epoll 高效的原因，因为 epoll 每次值遍历发生事件的一小部分 socket 连接（这些 socket 都在这个双向链表中），而不用到全部 socket 连接中逐个遍历以判断事件是否到来。

#### 关于 epitem 节点

【【关于 epitem 结构 晚点在补充】】



## 向内核双链表增加节点

epoll_wait 函数实际上就是去双向链表，那么，**操作系统什么时候向双向链表中插入节点呢**？

- 客户端完成三次握手时，操作系统会向双向链表插入节点，这时服务器往往要调用 accept 函数把该连接从已完成连接队列中取走

- 当客户端发送来数据时，操作系统会向双向链表插入节点，这时服务器也要调用 close 关闭对应的 socket

- 当客户端发送数据时，操作系统会向双向链表插入节点，这时服务器要调用 read 或者 recv 来收数据

- 当可以发送数据时，操作系统会向双向链表插入节点，这时服务器可以调用 send 或者 write 向客户端发送数据。可以这样理解：如果客户端接收话剧慢，服务器发送数据快，那么服务器就得等客户端收完一批数据后才能再发下一批。

## 使用 epoll 函数来实现数据的收发

nginx 源码中的 `ngx_c_socket.cc` 中的 `CSocekt::ngx_epoll_init()`

对 epoll 功能初始化，在子进程中进行 ，这个函数被`ngx_worker_process_init()`所调用.

**实现逻辑**

- 首先调用 epoll_create 函数创建一个 epoll 对象，也就是创建了一个红黑树，还创建了一个双向链表，直接以 epoll 连接的最大项数为参数

```cpp
m_epollhandle = epoll_create(m_worker_connections); 
```

- 接着创建一个连接池

```cpp
 m_pconnections = new ngx_connection_t[m_connection_n]  //m_connection_n 连接池的大小
```
### 创建连接池的目的

目前项目有 2 个监听套接字，以后客户端连入后，每个用户还会产生 1 个套接字，套接字本身只是一个数字，但往往需要保存很多与这个数字相关的信息，这就需要把套接字数字本身与一块内存捆绑起来。所以，引入连接池的目的就是把套接字与连接池中的某个元素捆绑起来，将来就可以通过套接字取得连接池中的元素（内存），一遍读写其中的数据。

### ET 和 LT 模式

#### LT 模式-水平触发

epoll 默认采用的是 LT 模式，只有使用 EPOLLET 参数才会使用 ET（边缘触发）

发生一个事件，如果程序不处理，那么这个事件就一直被触发，具体地说，就是一个新用户连入后，如果程序不调用 accept4 或者 accept 函数将这个用户接入（从已完成连接队列中取出来），使用 epoll_wait 函数获取事件时，就每次都能获取到用户连入的事件通知，也就是 EPOLLIN 事件，显然，这种触发方式效率不高。

#### ET 模式-边缘触发

这种触发只是对非阻塞 socket 有用，因为项目中用的都是非阻塞 socket，所以可以使用边缘触发模式，发生一个事件，内核只会通知程序 1 次，如果一个新用户连入，内核通知程序 1 次，程序必须使用 accept4 或者 accept 将这个新用户接入，如果这次没接进来就麻烦了，因为内核不会再次通知程序。因为边缘触发这种模式减少了通知的次数，所以效率更高。

目前的代码中，这几个监听套接字，在调用 epoll_ctl 增加事件的时候用的都是默认的 LT 模式，这样就能保证不丢失客户端的连接，因为内核会反复通知程序。

对于接入的 socket 连接（accept4 或者 accept 返回的 scoket 连接），程序中用了 ET 模式，从而提高程序工作效率。

### 事件驱动

事件驱动架构，就比如说客户端连入，三次握手完成，只要服务器注册了获取读事件，内核就会通知服务器，这就产生了一个事件，这里的事件发生源是客户端，通过事件收集器来收集和分发事件（这里的事件收集就是 epoll_wait 函数）。然后比如`CSocket::ngx_event_accept`这些函数就是事件处理函数，服务器准备用这些函数来处理或者消费事件。

> 注意：每个事件消费者（处理函数）都不能有阻塞行为，否则整个执行通道就会堵塞了。

### 腾讯面试题

> 使用 linux epoll 模型，水平触发模式，当 socket 可写时，会不停地触发 socket 可写事件，如何处理？

- 第一种方式

需要向 socket 写数据的时候才把 socket 可写事件通知加入 epoll 的红黑树节点，等待可写事件。当程序接受到来自系统的可写事件通知后，调用 write 或者 send 发送数据。所有数据都发送完毕后，把 socket 可写事件通知从 epoll 的红黑树节点中移除（移除的是可写事件通知，而不是红黑树节点）

这种方式的缺点：即使发送很少的数据，也要把 socket 可写事件通知加入 epoll 红黑树节点，写完后再把可写事件通知从 epoll 红黑树节点中删除，有一点的操作代价。

- 第二种方式

开始不把 socket 可写通知事件加入 epoll 的红黑树节点，需要发送数据时，直接调用 write 或者 send 发送，如果 write 或者 send 返回 EAGIN（缓冲区满了，需要等待可写事件才能继续往发送缓冲区写数据），再把 socket 的写事件通知加入 epoll 的红黑树节点。这就变成了在 epoll 的驱动下发送数据，全部数据发送完毕后，再把可写事件通知从 epoll 红黑树节点中删除

这种方式的优点是：数据不多的时候避免 epoll 的红黑树节点中针对写事件通知的增删，提高了程序执行效率。

### 深入理解ET LT

- LT 是水平触发，属于低速模式，如果事件还没处理完，就会被一直触发
- ET 是边缘触发，属于高速模式，这个事件的通知只会出现一次

### Epoll 中 ET 和 LT 模式的处理编码不同

如果发送来了数据，一个读事件就会被内核放到双向链表，如果我们不使用 recv 来接受数据或者只使用 recv 接受了部分数据，也就是说 TCP 连接的接受缓冲区中还有数据没有接受完

在 LT 模式下，内核就不会把这个读事件的节点从双向链表中删除，这样每次程序调用 epoll_wait 都能获取通知。

ET 模式不一样，不管我们是否调用 recv 来接受数据，一旦从双向链表中把读事件对应的节点取走，内核肯定把这个节点从双向链表中删除了，所以下次用 epoll_wait 去取事件时取不到的，除非后续客户端又发来了数据，内核会再次向这个双向链表中添加一个读事件的节点，程序使用 epoll_wait 才能再次收到读事件。


一般来讲，本项目的服务器程序，如果收发的数据包后固定格式，都建议采用 LT 模式--编程简单，清晰，写好了效率上估计也不会很差。

如果收发数据包没有固定格式，可以考虑采用 ET 模式，反复收数据，收完为止，编程难度较大，但是效率会高一些。再浏览器反问一个 web 服务器页面时，发送的数据就可能没有固定格式，浏览器一次可能向 web 服务器发送一大批数据，然后等 web 服务器回应。所以 nginx 采用的是 ET 模式。

### TCP 粘包和缺包

#### 客户端粘包

客户端采用了 Nagle 优化算法（参考网络实现），即使客户端 3 次调用 write 来发送数据包，Nagle 优化算法也很可能把这 3 次 write 调用打包成 1 个数据包直接发送给服务器，当然这个可以调用某个函数关闭 Nagle 优化算法，关闭后，可能这 3 次 write 调用就分成了 3 个数据包发送给服务器，客户端的数据包粘包问题就解决了。

但是代价就是本来 1 个包可以发送全部内容，现在分成 3 个包，每个包都要带 TCP 头，IP 头，以太网帧头等。多了这些头，效率显然要差一些。

#### 服务器粘包

就算客户端不粘包，服务器也存在粘包问题，服务器不可能随时都在 recv（接收）数据，可能 recv 完一次之后要做一些其他操作，需要时间，假设这期间客户端发送的 3 个包到了服务器， 保存在服务器针对这个 socket 连接（TCP连接）的接收缓冲区中，也就是说 abcdefgij 都在服务器的接收缓冲区中了，等服务器完成请他操作，调用 recv 接受数据，还是会一次把 abcdefgij 全部收到，这就叫做服务端的粘包。

#### 缺包

服务器调用 recv 收包时，收到多少数据都有可能，可能数据包小，一次就收完几个数据包，也可能因为网络迟延或数据包过大，几次才收完一个完整的包（缺包）

#### 粘包和缺包问题解决

要解决粘包问题就要把这几个包逐个拆出来，主要服务器能够正确区分出来每个包，那么粘包问题就解决了。

首先我们要给收发的数据包定义一个统一的格式，服务器端以及客户端都要按照这个格式来收发数据。

> 那么数据包的格式是怎样的呢？

就是包头 + 包体 的格式，收发的任何一个数据包，都要遵循这种包头+ 包体 的格式，其中包头是**固定长度**的。包头其实是一个结构，在包头中有一个成员，用于记录整个包的长度（包头+包体的长度），因为包头长度固定，并且能从包头中获取整个包的长度，用整个包的长度减去包头的长度，就可以得到包体的长度，从而能完整收到包体。

接受一个数据包的过程

- 先收固定长度的包头

- 根据包头中的内容，计算出包体长度

- 再收包体长度这么多的数据

这样就收到一个完整的数据包，于是，粘包问题就解决了。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/tcp数据包.78v7tmsz1700.png)


### 收包分析

- 每个包虽然都是包头 + 包体形式，但是在本项目中规定，一个包的总长度不超过 30 000 字节。如果有人恶意伪造数据包包头，告诉服务器这个数据包长度是 300 亿字节，服务器程序不能收 300 亿字节才认为收包结束，那会把服务器程序内存消耗光的。所以这个规定主要是防止恶意伪造数据包或通信双方无意中写错数据包长度信息而将服务器置于危险境地。

#### 字节对齐问题

在 C/C++ 语言中，结构有 字节对齐 的问题，也就是说不同操作系统、不同编译器对于一个相同的结构，其 sizeof 值可能不同。包体也是一种结构，在客户端，sizeof 计算是 10 字节，发送出去，如果服务器相同的结构，sizeof 却是 12 个字节，如果服务端按照 10 字节收取，那整个收到的包就乱套了。所以，必须保证客户端和服务端对于同一个结构的 sizeof 值相同，项目中采用 **1字节对齐**的方式来定义结构，

也就是说，结构体成员内存之间不需要添加额外字节，一个挨着一个，这样就可以保证筒一个结构，在客户端和服务器，sizeof 值必然不同。这正是本项目所需要的。

注意，一个结构，如果在定义这个结构的程序内部使用，是否字节对齐无关紧要，但是如果要通过网络进行传输，就必须制定 1 字节对齐。

实现方式是在要指定 1 字节对齐的结构之前加一条语句：

```cpp
# pragma pack(1)  //对齐方式，1 字节对齐（结构之间不做任何字节对齐，紧密排列子啊一起）
```

然后如果要还原默认的字节对齐方式

```cpp
# pragma pack() //取消指定对齐，恢复默认对齐
```

#### 怎么处理数据包过期问题

#### 服务端怎么识别客户端断线问题


```cpp
//消息头，引入的目的是当收到数据包时，额外记录一些内容以备将来使用
typedef struct _STRUC_MSG_HEADER
{
	lpngx_connection_t pConn;         //记录对应的链接，注意这是个指针
	uint64_t           iCurrsequence; //收到数据包时记录对应连接的序号，将来能用于比较是否连接已经作废用
	//......其他以后扩展	
}STRUC_MSG_HEADER,*LPSTRUC_MSG_HEADER;
```


使用**消息头**来处理，也就是对于服务端，一个完整的数据包应该是 **消息头+包头+包体**，其中包头+包体来自客户端，消息头是服务器后增加上去的。

如果收到客户端发送来的一个数据包，服务器处理这个数据包对应的业务用了 3 秒，处理完成并向客户端发送结果时客户端可能已经断线了，所以需要消息头来识别客户端是否断线。观察消息头结构`_STRUC_MSG_HEADER`成员，其中 pConn 用于记录 socket 连接对应的连接池中连接的内存地址，`iCurrsequence`是个序号，从连接池中获取一个连接（CSocket::ngx_get_connection 函数）时，这个序号值会再次 +1 。

当收到一个客户端发送来的数据包时，就把这个序号记录在所收到包的**消息头**中，处理完数据包并向客户端回应时，在比较消息头中的序号与这时候连接池中这个连接的序号，如果两者不相等，表示该客户端已经断线了（断线的客户端会因为调用了 `CSocket::ngx_free_connection`函数使对应的连接池中连接的序号 + 1），就不需要向客户端回应数据包了。

此外，我们考虑到消息头和包头的 sizeof 值会经常使用，所以在 CSocket 类中引入 2 个成员变量，专门用于保存消息头和包头的 sizeof 值。

```cpp
	//一些和网络通讯有关的成员变量
	size_t  m_iLenPkgHeader;                      	
	size_t  m_iLenMsgHeader;    
```

### 收包流程

`CSocket::ngx_write_request_handler()` 函数中

- 首先调用`CSocket::ngx_write_request_handler()`函数将收包初始化以及分配内存

- 利用`CSocket::recvproc`函数来收包（其实开始收的就是包头）
  - 内部其实是调用系统函数 recv 来收包的

- 刚开始是收包头，如果包头收完整了，就调用`CSocket::ngx_wait_request_handler_proc_p1`函数处理该包头，如果包头没收完整，这进入`_PKG_HD_RECVING`状态并继续收包头中剩余的字节。总之，只要包头接收完整，就调用`CSocket::ngx_wait_request_handler_proc_p1`函数来处理。

- 接着开始**处理收到的数据包**
  - 取出整个包的长度放在 e_pkgLen 变量中，根据包的长度判断是否是恶意包并做相应处理，注意，收到一个合法的包头之后，分配足以保存消息头+包头+包体的内存，把消息头、包头率先保存进去。然后继续为后续包体做准备（如果有包体）。如果有一个恶意用户，向服务器只发包头不发包体（甚至发完包头后直接关闭 socket 连接），服务器如何释放刚刚分配的这块内存呢？

  - 项目中在 ngx_connection_t 的结构体中引入了 `ifnewrecvMem`成员，标记新建了一块内存，并用一个指针 `pnewMemPointer`成员指向这块内存。一旦数据包没接收完整，客户端就关闭 `socket`连接，服务器端也能及时回收这块分配出去的内存，以防内存泄漏。

- 包体没收完整，就设置收包状态为 `_PKG_BD_RECVING`并继续收包体；如果包体也收完整了，就调用`CSocket::ngx_wait_request_handler_proc_plast`函数处理整个包。

  - 这里把消息体放进消息队列中，然后重新设置收包状态
  - 为了防止不断向消息队列中放数据导致内存耗费严重，需要适当清理数据，比如当消息数量超过 1000 条时就做清理操作。

到目前为止整个收包流程就完成了。

## 多线程相关


## SYN 攻击和压力测试

