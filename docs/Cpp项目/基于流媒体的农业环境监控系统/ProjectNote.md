## 1.项目介绍
本项目是我们学校大数据学院、机电学院和植保学院一起合作的课题项目，将摄像头安置在机器人身上让机器人在农田你行走，从而实现对农田环境的实时监控。我主要负责的是流媒传输和视频解码部分。因为前期只能是本地模拟，所以我就将移动设备来模拟摄像头，`PC`端作为接受设备。为了方便实验分析所以使用`Android` 调试桥 `adb` 来实现移动设备与`PC`之间的通信交互。这个项目实现的大致流程是：移动设备实时采集视频流，然后编码为`H264`,通过安卓端的`Client`套接字，借助于`adb reverse`搭建的反向代理传送到`PC server`端，然后`PC server`收到视频流之后通过解码器模块，把`H264`视频解码为`YUV`,然后再将`YUV`传给渲染器模块，渲染器负责将`YUV`显示到屏幕,这就是整个视频实时同步的流程。	

![](./img/项目流程.jpg)

在视频解码方面使用的是`ffmpeg`,整个项目使用的是`C++`编写，流媒体数据实时传输采用的是`RTMP`协议

启动流程：
1. 将`scrcpy-server`推送到手机（这里可以借用`adb push`命令来实现）
2. 启动反向代理（`adb reverse`），
3. `pc`端启动一个`socket server`来监听反向代理的端口
4. 启动安卓端`scrcpy-server`（	`adb shell app_process`），之后会建立一个`socket client`去连接`adb reverse`建立的端口，
5. `PC`接收`scrcpy-server`的连接，并与安卓端进行`socket`通信

**遇到的技术难点**：

收到移动端的视频要立即解码显示，避免任何不必要的缓冲，这就需要自己来编写视频的解码和显示，已有的轮子无法满足我们的需求（`VLC`等）。

此外，因为考虑到农田环境大部分都是在偏远野外，很大可能会出现网络不稳定的情况，为此我在动态网络中流媒体数据实时传输方面做了一定的研究，并在研一上半年完成发表了一篇北大核心论文，题目是《面向农业监视的流媒体传输机制研究》。


然后在项目中实现了低延迟传输，我们测试了`720P`的视频流传输平均时延在`300ms`以。为了保证流媒体数据传输的实时性，我们决定采用`RTMP`协议。所以目前我在课题组中的主要工作是针对`RTMP`协议的研究。

在完成这个项目过程中，掌握音视频相关知识，比如`ffmpeg`的数据结构，音视频同步处理，RTMP流媒体协议，还有音视频的解码流程。提高了`C++`的编程能力，同时锻炼独立解决问题的能力。

## 2.abd 相关

### 2.1 什么是 `ADB`

`ADB`的全称为Android Debug Bridge，就是**安卓调试桥接**，简单点说，它是`Android`系统 	提供的一套工具，通过它，我们可以在电脑上建立一个连接到手机的通道，然后可以在电脑上向手机发送一些指令，完成一些我们需要做的工作。

### 2.2 `adb` 的工作方式

启动一个 `adb` 客户端时，这个客户端首先检查是否有已运行的服务器进程。如果没有，它将启动服务器进程。当服务器启动时，它与本地 `TCP` 端口 `5037` 绑定，并侦听从 `adb` 客户端发送的命令——所有 `adb` 客户端均使用端口 `5037` 与 `adb` 服务器通信。

### 2.3 什么是反向代理

- **正向代理**：客户端想要访问一个服务器，但是它可能无法直接访问这台服务器，这时候这可找一台可以访问目标服务器的另外一台服务器，而这台服务器就被当做是代理人的角色 ，称之为代理服务器，于是客户端把请求发给代理服务器，由代理服务器获得目标服务器的数据并返回给客户端。客户端是清楚目标服务器的地址的，而目标服务器是不清楚来自客户端，它只知道来自哪个代理服务器，所以正向代理可以屏蔽或隐藏客户端的信息。

- **反向代理**：从上面的正向代理，你会大概知道代理服务器是为客户端作代理人，它是站在客户端这边的。其实反向代理就是代理服务器为服务器作代理人，站在服务器这边，它就是对外屏蔽了服务器的信息，常用的场景就是多台服务器分布式部署，像一些大的网站，由于访问人数很多，就需要多台服务器来解决人数多的问题，这时这些服务器就由一个反向代理服务器来代理，客户端发来请求，先由反向代理服务器，然后按一定的规则分发到明确的服务器，而客户端不知道是哪台服务器。

## 3.Qt相关

### 3.1 QT 信号槽机制的优缺点 

#### Qt信号槽机制的优势

- （1）**类型安全**。需要关联的信号和槽的签名必须是等同的，也就是信号的参数类型和参数个数和接收该信号的槽的参数类型和参数个数相同。不过，一个槽的参数个数是可以少于信号的参数个数的，但缺少的参数必须是信号参数的最后一个或几个参数。如果信号和槽的签名不符，编译器就会报错。
- （2）**松散耦合**。信号和槽机制减弱了Qt对象的耦合度。发送信号的Qt对象不需要知道是哪个对象的哪个槽需要接收它发出的信号，也不关心它的信号有没有被接收到，同样的，对象的槽也不知道是哪些信号关联了自己。而一旦关联信号和槽，Qt就保证了适合的槽得到了调用。即使关联的对象在运行时被删除，应用程序也不会崩溃。
- （3）**信号和槽机制增强了对象间通信的灵活性**。一个信号可以关联多个槽，也可以多个信号关联一个槽。

#### Qt信号槽机制的不足

相比回调函数，信号和槽机制运行速度有些慢。通过传递一个信号来调用槽函数将会比直接调用非虚函数运行速度慢10倍。原因如下：
- （1）需要定位接收信号的对象；
- （2）如果一个信号关联多个槽的情况下，需要遍历所有的关联
- （3）需要编组/解组传递的参数；
- （4）多线程的时候，信号可能需要排队等待。

然而，与使用`new`创建对象和使用`delete`删除对象相比，信号和槽相对来说运行代价就会小很多。信号和槽机制导致的这点性能的损耗，对实时应用程序是可以忽略的

### 3.2 多线程情况下, Qt 中的信号槽分别在什么线程中执行, 如何控制? 

可以通过`connect`函数的第五个参数来控制, 信号槽执行时所在的线程 

- 1)自动连接(AutoConnection)，默认的连接方式，如果信号与槽，也就是发送者与接受者在同一线程，等同于直接连接；如果发送者与接受者处在不同线程，等同于队列连接。
- 2)直接连接(DirectConnection)，当信号发送时，槽函数立即直接调用。无论槽函数所属对象在哪个线程，槽函数总在发送者所在线程执行，即**槽函数和信号发送者在同一线程**
- 3)队列连接(QueuedConnection)，当控制权回到接受者所在线程的事件循环时，槽函数被调用。槽函数在接受者所在线程执行，即**槽函数与信号接受者在同一线程**


## 4音视频相关

### 4.1视频播放的流程

视频播放的流程需要经过几个步骤：解协议，解封装，解码音视频，音视频同步。如果播放的是本地文件就不需要解协议。他们的过程如下图所示：

![](./img/project/视频播放流程.png)

- **解协议**：<u>就是将流媒体协议的数据，解析为标准的相应的**封装格式数据**，</u>音视频在网络上传播的时候，常常采用各种流媒体协议，比如：HTTP，RTMP，或者MMS等。这些协议在传输音视频数据的同时，也会传输一些信令数据，这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等等，解协议的过程中会除去信令数据而只是保留音视频数据，<u>例如采用`RTMP`协议传输的数据，经过解协议操作后，输出`FLV`格式的数据。</u>

- **解封装**：<u>就是将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。封装格式的种类很多，例如` MP4，TS，AVI`等等，它的作用是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。例如：`FLV`格式的数据经过解封装后，输出`H264`编码的视频码流和`AAC`编码的音频码流。</u>

- **解码**：<u>就是将视频/音频压缩编码数据，解码称为非压缩的视频/音频原始数据。</u>音频的压缩编码标准包含`AAC,MP3,AC-3`等等，视频的压缩编码标准包含`H264,MPEG2,VC-1`等等。解码是整个系统中最重要也是最复杂的一个环节。<u>通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如`YUV420P,RGB`等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如`PCM`数据。</u>

- **音视频同步**：<u>就是根据解封装模块处理过程中获取的参数信息，同步解码出来的音频和视频数据，并将音视频数据送至系统的显卡和声卡播放出来。</u>

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音视频播放流程.574tor5xrrc0.png)

### 怎么处理音视频同步

对于一个播放器，一般来说，其基本构成均可划分为以下几部分：
**数据接收（网络/本地）->解复用->音视频解码->音视频同步->音视频输出**。

媒体数据经过解复用流程后，音频/视频解码便是独立的，也是独立播放的。而在音频流和视频流中，其播放速度都是有相关信息指定的：

<u>实现音视频同步，在播放时，需要选定一个参考时钟，读取帧上的时间戳，同时根据的参考时钟来动态调节播放。现在已经知道时间戳就是PTS，那么参考时钟的选择一般来说有以下三种：</u>

- 将视频同步到音频上：就是以音频的播放速度为基准来同步视频。
- 将音频同步到视频上：就是以视频的播放速度为基准来同步音频。
- 将视频和音频同步外部的时钟上：选择一个外部时钟为基准，视频和音频的播放速度都以该时钟为标准。


<u>当播放源比参考时钟慢，则加快其播放速度，或者丢弃；快了，则延迟播放。</u>

<u>这三种是最基本的策略，考虑到人对声音的敏感度要强于视频，频繁调节音频会带来较差的观感体验，且音频的播放时钟为线性增长，所以**一般会以音频时钟为参考时钟**，视频同步到音频上。</u>

[音视频同步原理及实现](https://blog.csdn.net/myvest/article/details/97416415)

[使用rtcp实现音视频同步](https://blog.csdn.net/fdsafwagdagadg6576/article/details/108717668)

## FFmpeg数据结构

FFmpeg的数据结构有很多，大致分成四种：
- (1)解协议：`AVIOContext`，`URLProtocol`，`URLContext`主要存储视音频使用的协议的类型以及状态。

- (2)解封装：`AVFormatContext`主要存储视音频封装格式中包含的信息

- (3)解码：`AVStream`，`AVCodecContext`，`AVCodec`。`AVStream`就是存储音视频流，然后`AVCodecContext`就根据哪种音视频流找到对应的解码器`AVCodec`。

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/ffmpeg数据结构02.2wmury82usk0.png)

- (4)存数据：`AVPacket`存储解码前数据，`AVFrame`存储解码后数据


### FFmpeg编解码模块流程

#### 音频解码

比如说音频解码，就是音频数据，比如 AAC格式通过解码器解码成 原始的音频数据，比如 PCM。


大致流程

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码01.4mvaz31l8xe0.png)


解码需要理解四个结构体`AVStream`、 `AVPacket` 和 `AVFrame` 以及 `AVCodecContext`， 其中`AVPacket` 是存放是编码格式的帧数据， `AVFrame` 存放的是解码后的帧数据。 解码的过程其实就是从`AVCodecContext` 取出一个`AVPacket` 解码成 `AVFrame`的过程。

解码过程需要调用一些函数：


![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/寻offer总结/音频解码02.3d4aphvcp5y0.jpg)


关键函数说明：

- `avcodec_register_all()`：注册所有的编解码器。
- `avcodec_find_decoder`：根据指定的`AVCodecID`查找注册的解码器。
- `av_parser_init`：初始化`AVCodecParserContext`。返回的是`AVCodecParserContext`
- `avcodec_alloc_context3`：为`AVCodecContext`分配内存。
- `avcodec_open2`：打开解码器。
- `av_parser_parse2`：解析获得一个`Packet`。
- `avcodec_send_packet`：将`AVPacket`压缩数据给解码器。
- `avcodec_receive_frame`：获取到解码后的`AVFrame`数据。
- `av_get_bytes_per_sample`: 获取每个`sample`中的字节数。

#### 视频解码

视频解码过程如下图所示：

⼀般解出来的是420p

![](./img/project/视频解码流程.jpg) 

**FFmpeg流程**

![](./img/project/视频解码流程01.jpg) 

**关键函数说明**：

- avcodec_find_decoder：根据指定的AVCodecID查找注册的解码器。
- av_parser_init：初始化AVCodecParserContext。
- avcodec_alloc_context3：为AVCodecContext分配内存。
- avcodec_open2：打开解码器。
- av_parser_parse2：解析获得⼀个Packet。
- avcodec_send_packet：将AVPacket压缩数据给解码器。
- avcodec_receive_frame：获取到解码后的AVFrame数据。
- av_get_bytes_per_sample: 获取每个sample中的字节数。

**关键数据结构说明**：

`AVCodecParser`：⽤于解析输⼊的数据流并把它分成⼀帧⼀帧的压缩编码数据。⽐较形象的说法就是把⻓⻓的⼀段连续的数据“切割”成⼀段段的数据。

⽐如`H264 aac_parse`
```cpp
 AVCodecParser ff_h264_parser = {
 	.codec_ids = { AV_CODEC_ID_H264 },
 	.priv_data_size = sizeof(H264ParseContext),
 	.parser_init = init,
 	.parser_parse = h264_parse,
 	.parser_close = h264_close,
 	.split = h264_split,
 };
 ```
从`AVCodecParser`结构的实例化我们可以看出来，不同编码类型的`parser`是和`CODE_ID`进⾏绑定的。所以也就可以解释

```cpp
parser = av_parser_init(AV_CODEC_ID_H264);
```
可以通过`CODE_ID`查找到对应的码流 `parser`




### 介绍一下`RTMP`协议

`RTMP`是应用层协议，底层依靠的是`TCP`协议。因为多媒体传输是一个持续的过程，所以一般需要更为可靠的握手来保证连接的可靠性。`RTMP`协议在`TCP`三次握手的基础上，自己定义了**六次握手**，如下图：

![](./img/project/rtmp-01.jpg)

`RTMP`协议中规定了握手过程就是图上这样的：

- 客户端发送版本号`C0`和生成的随机字符串`C1`

- 服务端收到`C0`后，如果支持客户端的版本，则发送自己支持的版本号`S0`，否则不发送

- 服务端收到`C1`后则发送自己生成的随机字符串`S1`

- 客户端收到`S1`后，则发送`S1`的拷贝`C2`

- 服务端收到`C1`后，则发送`C1`的拷贝`S2`

- 客户端收到`S2`后，进行校验，通过后才发送控制信息和真实音视频等数据

- 服务端收到`C2`后，进行校验，通过后才发送控制信息和真实音视频等数据

[RTMP抓包分析过程](https://zhuanlan.zhihu.com/p/157429042)


[RTMP规范简单分析](https://blog.csdn.net/leixiaohua1020/article/details/11694129)

[RTMP流媒体播放过程](https://blog.csdn.net/leixiaohua1020/article/details/11704355)





### H264 格式

H264内部结构
在H.264/AVC视频编码标准中，整个系统框架被分为了两个层面：

1.VCL (VideoCoding Layer，视频编码层)。核心算法引擎，块，宏块及片的语法级别的定义，负责高效的视频内容表示，通俗的讲就是编码器直接编码之后的数据，这部分数据还不能直接用于保存和网络传输，否则在解析上存在困难。

2.NAL(NetworkAbstraction Layer，网络抽象层)。负责格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输，通俗的讲NAL就是将上面的VCL加了一些头部信息封装了一下。

现实中的传输系统是多样化的，其可靠性，服务质量，封装方式等特征各不相同，NAL这一概念的提出提供了一个视频编码器和传输系统的友好接口，使得编码后的视频数据能够有效地在各种不同的网络环境中传输。

### YUV 格式

https://blog.csdn.net/iva_brother/article/details/84036877

YUV是指亮度参量和色度参量分开表示的像素格式，其中“Y”表示明亮度（Luminance或Luma），也就是灰度值；而“U”和“V”表示的则是色度（Chrominance或Chroma），作用是描述影像色彩及饱和度，用于指定像素的颜色。

**YUV的格式有两大类：`planar`和`packed`**。

对于planar的YUV格式，先连续存储所有像素点的Y，紧接着存储所有像素点的U，随后是所有像素点的V。

对于`packed`的`YUV`格式，每个像素点的Y、U、V都是连续交叉存储的。



### AAC 格式

AAC⾳频格式：Advanced Audio Coding (⾼级⾳频解码)，是⼀种由 MPEG-4 标准定义的有损⾳频压缩格式。它有以下两种格式：

- ADIF：Audio Data Interchange Format ⾳频数据交换格式。这种格式的特征是可以确定的找到这个⾳频数据的开始，不需进⾏在⾳频数据流中间开始的解码，即它的解码必须在明确定义的开始处进⾏。故这种格式常⽤在磁盘⽂件中。

- ADTS 的全称是 Audio Data Transport Stream 。是 AAC ⾳频的 传输流格式 AAC ⾳频格式在 MPEG-4 ISO-13318-7 2003）中有定义。 AAC 后来⼜被采⽤到 MPEG-4 标准中。这种格式的特征是它是⼀个有同步字的⽐特流，解码可以在这个流中任何位置开始。它的特征类似于mp3数据流格式。这种格式可以用于广播电视。

简单的说，**ADTS 可以在任意帧解码，也就是说每一帧都有头信息**，ADIF 只有一个统一的头，所以必须得到所有的数据后才解码。

且这两种的 header 的格式也是不同的，⽬前⼀般编码后的和抽取出的都是 ADTS 格式的⾳频流。两者具体的组织结构如下所示:



[AAC_ADTS格式分析](/音视频开发/ffmpeg/04AAC_ADTS格式分析.md)

-----


### 迟延怎么处理

延时会产生在：

- 音视频数据的前处理；

- 音视频数据的编解码；

- 音视频数据的网络传输；

- 为了防止抖动业务代码中的缓冲区，包括推流服务、转码服务、播放器的缓存等；

- 音视频的渲染播放；


当然上面会产生延时的地方对于最终的延时影响权重是不一样的，其中数据的前处理、编解码、渲染对于延时影响比较小，而网络传输和业务代码的缓存对于延时影响非常大。所以优化也要结合你的业务有重点进行。

- 优化思路1：调整推流端和播放端的缓冲区大小，对于25fps的视频流，如果我们缓存25帧的数据，就会在播放时产生1s的延时。所以我们要动态调整我们的缓冲区，对于推流上行区我们如果带宽不够就会产生网络阻塞，这时发送端的数据就会积累，最终延时不断累加，导致延时变大。我们此时就需要有一套机制来能够预测带宽，降低发送码率，减低当前发送数据量，减少网络阻塞，等网络好的时候再继续增大数据发送量，增大码率。

对于播放端的缓存，当网络不好产生的延时比较大时，我们需要通过丢帧和加速播放方式快速消耗掉播放缓冲区的数据，从而消除累计的延时。

- 优化思路2：优化网络传输，如果实时性要求很高的场景，你如果选用基于TCP承载的网络传输协议，无论你怎么优化，也很难降低延时。因为TCP会进行三次握手，而且它会对每一次发送的数据进行确认，还要对丢包进行重传，所以这些限制很不适合降低延时。我们要优化传输协议，我们可以将基于TCP的RTMP、HLS协议切换到基于UDP的RTP、QUIC协议上，或者自己开发基于UDP的私有协议栈，这样我们就可以对一些TCP延时大的功能进行裁剪和修改，对于一些不关重要的数据进行丢弃，优先保障重要数据的传输。其中国内B站、虎牙直播，在线k12教育等都进行了类似的处理；

- 优化思路3：选择优质的CDN加速服务，保障传输的线路带宽和线路资源，一般都会提供测速选线、动态监测、智能路由等功能。

- 优化思路4：如果感觉自己的编解码，前期处理等花费时间比较多，我们就需要选择合适的音视频编解码器，进行算法调优降低延时，比如我们在播放端能支持硬解的优先选择硬解否则才选择软解。

前段时间我们课题做了个项目就是采用了自动切换网络传输协议的措施来降低延时，摄像头的视频一般要推送到云服务器上，然后才能进行大规模的转发和分发。这是因为摄像头毕竟是嵌入式设备，并发量非常有限，能同时推送的视频路数也就一两路，如何想无限制进行分发和允许多客户端同时观看，就需要先让摄像头的视频上云到服务端的流媒体，再进行大规模的分发和转发，这也是视频监控的基本玩法。但是我们摄像头以前只支持TCP长链接方式向服务器推流，这样当网络不好就会丢包重传，延时也逐渐积累增大。甚至网络非常不好时，延时会达到几十秒，用户体验很不好。

措施：

我们流媒体服务端会收集播放器的延时数据和丢包，然后当达到一定条件，我们通过信令服务器进行传输协议切换，重新让摄像头推流。将TCP推流改成UDP推流，我们在流媒体服务器端重新实现组包和增加丢帧策略，降低播放端延时，效果最后也得到了客户的满意。
> https://blog.csdn.net/Anne033/article/details/108094889









