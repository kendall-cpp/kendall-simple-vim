
<!-- TOC -->

- [WebRTC 处理过程](#webrtc-处理过程)
	- [WebRTC 进行音视频通话的大体过程。](#webrtc-进行音视频通话的大体过程)
	- [音视频采集](#音视频采集)
	- [选用什么协议](#选用什么协议)
		- [RTP 协议](#rtp-协议)
		- [RTCP 协议](#rtcp-协议)
	- [SDP](#sdp)

<!-- /TOC -->

-------


## WebRTC 处理过程


![](https://static001.geekbang.org/resource/image/c5/a0/c536a1dd0ed50008d2ada594e052d6a0.png)

假如说有两个 WebRTC 终端（上图中的两个大方框）、一个 Signal（信令）服务器和一个 STUN/TURN 服务器。

- WebRTC 终端：负责音视频采集，编解码，NAT 穿越、音视频数据传输
- Signal 服务器，负责信令处理，如加入房间、离开房间、媒体协商消息的传递等。
- STUN/TURN 服务器，负责获取 WebRTC 终端在公网的 IP 地址，以及 NAT 穿越失败后的数据中转。

### WebRTC 进行音视频通话的大体过程。

当一端（WebRTC 终端）进入房间之前，它首先会检测自己的设备是否可用。如果此时设备可用，则进行音视频数据采集

采集到的数据一方面可以做预览，也就是让自己可以看到自己的视频；另一方面，可以将其录制下来保存成文件，等到视频通话结束后，上传到服务器让用户回看之前的内容。

在获取音视频数据就绪后，WebRTC 终端要发送 “加入” 信令到 Signal 服务器。Signal 服务器收到该消息后会创建房间。在另外一端，也要做同样的事情，只不过它不是创建房间，而是加入房间了。待第二个终端成功加入房间后，第一个用户会收到 “另一个用户已经加入成功” 的消息。

此时，第一个终端将创建 “媒体连接” 对象，即 RTCPeerConnection ,并将采集到的音视频数据通过 RTCPeerConnection 对象进行编码，最终通过 P2P 传送给对端。

当然，在进行 P2P 穿越时很有可能失败。所以，当 P2P 穿越失败时，为了保障音视频数据仍然可以互通，则需要通过 TURN 服务器（TURN 服务会在后面文章中专门介绍）进行音视频数据中转。

这样，当音视频数据来到对端后，对端首先将收到的音视频数据进行解码，最后再将其展示出来，这样就完成了一端到另一端的单通。如果双方要互通，那么，两方都要通过 RTCPeerConnection 对象传输自己一端的数据，并从另一端接收数据。

### 音视频采集

在浏览器打开摄像头

首先执行 `getUserMedia()` 方法，这个方法会去访问 摄像头，经过用户允许后，调用 `gotLocalMediaStream` 方法。

在 gotLocalMediaStream 方法中，其输入参数为 MediaStream 对象，该对象中存放着 getUserMedia 方法采集到的**音视频轨**。

我们将它作为视频源赋值给 HTML5 的 video 标签的 srcObject 属性。这样在 HTML 页面加载之后，就可以在该页面中看到摄像头采集到的视频数据了。

### 选用什么协议

实现一套实时互动直播系统，在选择网络传输协议时，你会选择使用 UDP 协议还是 TCP 协议呢？

必须使用 UDP

为什么一定要使用 UDP 呢？关于这个问题，你可以反向思考下，假如使用 TCP 会怎样呢？在极端网络情况下，TCP 为了传输的可靠性，它是如何做的呢？简单总结起来就是“**发送 -> 确认；超时 -> 重发**”的反复过程。

TCP 的超时重传会带来较大的迟延

> 所以实现实时互动直播系统的时候你必须使用 UDP 协议

一般情况下，在实时互动直播系统传输音视频数据流时，我们并不直接将音视频数据流交给 UDP 传输，而是先给音视频数据加个 RTP 头，然后再交给 UDP 进行传输。

我们以视频帧为例，一个 I 帧的数据量是非常大的，最少也要几十 K ，而以太网的最大传输单元是多少呢？ 1.5K，所以要传输一个 I 帧需要几十个包。并且这几十个包传到对端后，还要重新组装成 I 帧，这样才能进行解码还原出一幅幅的图像。如果是我们自己实现的话，要完成这样的过程，至少需要以下几个标识。

- 序号：用于标识传输包的序号，这样就可以知道这个包是第几个分片了。
- 起始标记：记录分帧的第一个 UDP 包。
- 结束标记：记录分帧的最后一个 UDP 包。

有了上面这几个标识字段，我们就可以在发送端进行拆包，在接收端将视频帧重新再组装起来了。

#### RTP 协议

![](https://static001.geekbang.org/resource/image/ae/89/aec03cf4e1b76296c3e21ebbc54a2289.png)

- sequence number：序号，用于记录包的顺序。这与上面我们自己实现拆包、组包是同样的道理。

- timestamp：时间戳，同一个帧的不同分片的时间戳是相同的。这样就省去了前面所讲的起始标记和结束标记。一定要记住，不同帧的时间戳肯定是不一样的。

- PT：Payload Type，数据的负载类型。音频流的 PT 值与视频的 PT 值是不同的，通过它就可以知道这个包存放的是什么类型的数据。

![](https://static001.geekbang.org/resource/image/e2/8f/e21ea8be9c0d13638a6af38423640d8f.png)

> https://blog.csdn.net/qq_26602023/article/details/101024815

#### RTCP 协议


在使用 RTP 包传输数据时，难免会发生丢包、乱序、抖动等问题，

RTCP 有两个最重要的报文：RR（Reciever Report）和 SR(Sender Report)。通过这两个报文的交换，各端就知道自己的网络质量到底如何了。


![](https://static001.geekbang.org/resource/image/ae/f3/ae1b83a0255d05dd70285f0a26fb23f3.png)

SR 报文分成三部分：Header、Sender info 和 Report block。在 NTP 时间戳之上的部分为 SR 报文的 Header 部分，SSRC_1 字段之上到 Header 之间的部分为 Sender info 部分，剩下的就是一个一个的 Report Block 了。那这每一部分是用于干什么的呢？

- Header 部分用于标识该报文的类型，比如是 SR 还是 RR。
- Sender info 部分用于指明作为发送方，到底发了多少包。
- Report block 部分指明发送方作为接收方时，它从各个 SSRC 接收包的情况。

![](https://static001.geekbang.org/resource/image/1e/04/1e772dd266c0899799dad777339adc04.png)

 SR 报文并不仅是指发送方发了多少数据，它还报告了作为接收方，它接收到的数据的情况。当发送端收到对端的接收报告时，它就可以根据接收报告来评估它与对端之间的网络质量了，随后再根据网络质量做传输策略的调整。

 SR 报文与 RR 报文无疑是 RTCP 协议中最重要的两个报文

还有其他报文


比如，RTCP 类型为 206、子类型为 4 的 FIR 报文，其含义是 Full Intra Request (FIR) Command，即完整帧请求命令。它起什么作用？又在什么时候使用呢？

该报文也是一个特别关键的报文，我为什么这么说呢？试想一下，在一个房间里有 3 个人进行音视频聊天，然后又有一个人加入到房间里，这时如果不做任何处理的话，那么第四个人进入到房间后，在一段时间内很难直接看到其他三个人的视频画面了，这是为什么呢？

原因就在于解码器在解码时有一个上下文。在该上下文中，必须先拿到一个 IDR 帧之后才能将其后面的 P 帧、B 帧进行解码。也就是说，在没有 IDR 帧的情况下，对于收到的 P 帧、B 帧解码器只能干瞪眼了。

如何解决这个问题呢？这就引出了 FIR 报文。当第四个人加入到房间后，它首先发送 FIR 报文，当其他端收到该报文后，便立即产生各自的 IDR 帧发送给新加入的人，这样当新加入的人拿到房间中其他的 IDR 帧后，它的解码器就会解码成功，于是其他人的画面也就一下子全部展示出来了。


> 使用UDP的确可以解决网络不好重传导致的延时问题，RTP协议解决了数据包组装的问题，而RTCP只是用于了解发送端和接收端的网络质量，那么UDP导致的丢包问题是如何解决的呢? 

> https://blog.csdn.net/dxpqxb/article/details/102487715

> https://blog.csdn.net/qq_41681715/article/details/109821871?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.control&spm=1001.2101.3001.4242

### SDP

SDP（Session Description Protocal）说直白点就是用文本描述的各端（PC 端、Mac 端、Android 端、iOS 端等）的能力。这里的能力指的是各端所支持的音频编解码器是什么，这些编解码器设定的参数是什么，使用的传输协议是什么，以及包括的音视频媒体是什么等等。

两个客户端 / 浏览器进行 1 对 1 通话时，首先要进行信令交互，而交互的一个重要信息就是 SDP 的交换。	

交换 SDP 的目的是为了让对方知道彼此具有哪些能力，然后根据双方各自的能力进行协商，协商出大家认可的音视频编解码器、编解码器相关的参数（如音频通道数，采样率等）、传输协议等信息。

举个例子，A 与 B 进行通讯，它们先各自在 SDP 中记录自己支持的音频参数、视频参数、传输协议等信息，然后再将自己的 SDP 信息通过信令服务器发送给对方。当一方收到对端传来的 SDP 信息后，它会将接收到的 SDP 与自己的 SDP 进行比较，并取出它们之间的交集，这个交集就是它们协商的结果，也就是它们最终使用的音视频参数及传输协议了。

标准 SDP 规范主要包括 SDP 描述格式和 SDP 结构，而 SDP 结构由会话描述和媒体信息描述两个部分组成。

其中，媒体信息描述是整个 SDP 规范中最重要的知识，它又包括了：

- 媒体类型
- 媒体格式
- 传输协议
- 传输的 IP 和端口

----------



- 使用 RTCPeerConnection 实现各端的数据传输
- 使用 Medooze 实现多方视频会议
- 使用 Canvas 绘制统计图表进行数据统计。
- 使用 SFU 架构实现多人音视频实时通话





- 采用 HLS 协议构建流媒体服务器，并使用 RTMP 协议实现流媒体系统内部分发
- 使用 flv.js 和 video.js 实现播放多媒体文件
- 缓冲 Buffer 的控制，解决首屏耗时、迟延优化的问题，使用 RTCP 解决音视频数据实时传输问题
- 利用 Medooze 搭建的流媒体服务器与 WebRTC 客户端，基于 SFU 方案实现多人音视频会议




成功为公司完成 57 个 cpp 文件近 24w 行代码的 debug , 提升了研发团队的工作效率 