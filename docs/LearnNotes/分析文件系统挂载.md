# mount 

mount 文件系统的入口通过这个函数

```c
SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,
		char __user *, type, unsigned long, flags, void __user *, data)
{
	int ret;
	char *kernel_type;
	char *kernel_dev;
	void *options;

	kernel_type = copy_mount_string(type);
	ret = PTR_ERR(kernel_type);
	if (IS_ERR(kernel_type))
		goto out_type;

	kernel_dev = copy_mount_string(dev_name);
	ret = PTR_ERR(kernel_dev);
	if (IS_ERR(kernel_dev))
		goto out_dev;

	options = copy_mount_options(data);
	ret = PTR_ERR(options);
	if (IS_ERR(options))
		goto out_data;

        // kernel_dev = /dev/mapper/system dir_name = /system.ro kernel_type = squashfs flags = 1029 options = (null) 
        // 这个函数在  mount squashfs /dev/mapper/system /system.ro ro nodev noatime 会调用
	ret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);

	kfree(options);
out_data:
	kfree(kernel_dev);
out_dev:
	kfree(kernel_type);
out_type:
	return ret;
}
```

- copy_mount_string 的实现,

```c
static char *copy_mount_string(const void __user *data)
{
        //返回 strndup_user 从用户空间中获取到的字符串
	return data ? strndup_user(data, PATH_MAX) : NULL;
}

// 从用户空间复制现有字符串
char *strndup_user(const char __user *s, long n)
{
	char *p;
	long length;

	length = strnlen_user(s, n);

	if (!length)
		return ERR_PTR(-EFAULT);

	if (length > n)
		return ERR_PTR(-EINVAL);

	p = memdup_user(s, length);

	if (IS_ERR(p))
		return p;

	p[length - 1] = '\0';

	return p;
}
```


https://www.cnblogs.com/jesse123/archive/2011/11/13/2247382.html


# squashfs 文件系统

> 文中所有内容均来自网络自由收集并按照自己理解进行总结，仅供学习参考。		
> kernel 源码版本：5.15		
> 硬件：Amlogic A113L 芯片

## 流程分析

### init_squashfs_fs

主要是初始化 squashfs 文件系统, 将 squashfs_fs_type 结构体注册到 kernel 的文件系统链表中，当 mount 的时候就会去调用 squashfs_fs_type 结构体中的回调函数。

但是在注册文件系统之前需要获取 inode 缓存，主要通过 init_inodecache 来实现。

```c
static int __init init_squashfs_fs(void)
{
	int err = init_inodecache();
	printk("lsken00 %s: %d ==== \n", __func__, __LINE__);
	if (err)
		return err;

	err = register_filesystem(&squashfs_fs_type);
	if (err) {
		destroy_inodecache();
		return err;
	}

	pr_info("version 4.0 (2009/01/31) Phillip Lougher\n");

	return 0;
}
```

### init_inodecache

```c
static int __init init_inodecache(void)
{
	squashfs_inode_cachep = kmem_cache_create("squashfs_inode_cache",
		sizeof(struct squashfs_inode_info), 0,
		SLAB_HWCACHE_ALIGN|SLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT,
		init_once);

	return squashfs_inode_cachep ? 0 : -ENOMEM;
}
```

#### kmem_cache_create

kmem_cache_create 函数是用来创建一个新的 slab 缓存，这通常是在内核初始化时就会被执行，或者在首次加载内核模块的时候执行，这里就是首次加载 squashfs 文件系统的时候需要创建一个新的 slab 缓存。

- kmem_cache_create输入参数说明
  - name： 该参数指缓存名称，proc文件系统（在/proc/slabinfo中）使用它标识一个缓存。
  - size： 该参数指定了为这个缓存创建的对象的大小，它是以字节为单位的。
  - align： 该参数定义了每个对象的对齐方式。
  - flags： 该参数指定了分配缓存时的选项，这些选项标志如表所示。
  - ctor： 参数定义了一个可选的对象构造器，构造器是用户提供的回调函数。当从缓存中分配新对象时，可以通过构造器进行初始化。

**kmem_cache_create 就是 kmem_cache_create_usercopy 的一个封装；**

```c
struct kmem_cache *
kmem_cache_create(const char *name, unsigned int size, unsigned int align,
		slab_flags_t flags, void (*ctor)(void *))
{
	return kmem_cache_create_usercopy(name, size, align, flags, 0, 0,
					  ctor);
}

// 这里 ctor 对应的就是 init_once 函数
// 所以 init_inodecache 中会回调用户提供的 init_once 函数，主要是将这个 innode 插入到 kernel 的文件节点链表中。
static void init_once(void *foo)
{
	struct squashfs_inode_info *ei = foo;

	inode_init_once(&ei->vfs_inode);   // 操作链表的操作
}
```

这里创建一个名为 “squashfs_inode_cache” 的 kmem_cache 作为分配 inode 的 slub 缓存，通过传入的参数可以知道，其对象大小就是 inode 的结构体大小，也就是说，通过这个 inode_cache 缓存可以用于高效分配 inode 、还包括 slab flag 、以及对象构造函数 init_once .

```c
squashfs_inode_cachep = kmem_cache_create("squashfs_inode_cache",
		sizeof(struct squashfs_inode_info), 0,
		SLAB_HWCACHE_ALIGN|SLAB_RECLAIM_ACCOUNT|SLAB_ACCOUNT,
		init_once);
```

### squashfs_init_fs_context

初始化为 inode 节点， 当 mount squashfs 的时候就会调用 squashfs_fs_type 中的初始化函数 squashfs_init_fs_context

```c
static int squashfs_init_fs_context(struct fs_context *fc)
{
	struct squashfs_mount_opts *opts;

	printk("lsken00 %s: %d ==== \n", __func__, __LINE__);

	opts = kzalloc(sizeof(*opts), GFP_KERNEL);
	if (!opts)
		return -ENOMEM;

	fc->fs_private = opts;
	fc->ops = &squashfs_context_ops;
	return 0;
}
```

#### fs_context

在连接这个函数之前有必要先了解一下 fs_context 结构体，因为 squashfs_fs_type 所有的回调函数都是以这个结构体为传入参数。fs_context 这个结构体是在 mount 流程中的 do_new_mount  分配的，

该结构体用于保存超级块的信息的 root，而超级块本身包含了该实际文件系统的信息，因此通过该结构体，我们可以**使该文件系统和 mount 结构体建立联系**。在旧版的 Linux 源码中，我们需要使用 mount_fs()  函数去获取对应的root，而新版则可以通过 fs_context 直接获取。

```c
struct fs_context {
	//这是一个文件系统上下文实例持续期间，提供给文件系统上下文使用的众多方法。一般由特定文件系统类型的 init_fs_context (squashfs_init_fs_context) 方法来对其进行设置。
	const struct fs_context_operations *ops;   // 操作改结构体的回调函数
	struct mutex            uapi_mutex;     /* Userspace access mutex */
	// 这里我们需要挂载哥 squashfs 文件系统类型指针
	struct file_system_type *fs_type;
	//指向文件系统私有数据的指针，常用于存储需要特定文件系统来解析的选项。
	void                    *fs_private;    /* The filesystem's context */
	void                    *sget_key;
	// 这是一个指向一个可挂载文件系统的根节点的指针，同时它也和文件系统的superblock关联。这个域一般通过vfs_get_tree调用特定文件系统的->get_tree函数来构建。同时，为了构建SB，也会调用特定文件系统的fill_super函数。
	struct dentry           *root;          /* The root and superblock */
	struct user_namespace   *user_ns;       /* The user namespace for this mount */
	struct net              *net_ns;        /* The network namespace for this mount */
	const struct cred       *cred;          /* The mounter's credentials */
	struct p_log            log;            /* Logging buffer */
	// 这个代表挂载操作的源对象，一般就是一个设备（上面含有文件系统）。
	// 如 mount -t squashfs /dev/block/mtdblock8  /system.ro 中，"/dev/block/mtdblock8"就是这个 source 的名称。
	const char              *source;        /* The source name (eg. dev path) */
	void                    *security;      /* Linux S&M options */
	void                    *s_fs_info;     /* Proposed s_fs_info */
	unsigned int            sb_flags;       /* Proposed superblock flags (SB_*) */
	unsigned int            sb_flags_mask;  /* Superblock flags that were changed */
	unsigned int            s_iflags;       /* OR'd with sb->s_iflags */
	unsigned int            lsm_flags;      /* Information flags from the fs to the LSM */
	enum fs_context_purpose purpose:8;                                                                                                                                                                                                                  
	enum fs_context_phase   phase:8;        /* The phase the context is in */
	bool                    need_free:1;    /* Need to call ops->free() */
	bool                    global:1;       /* Goes into &init_user_ns */
	bool                    oldapi:1;       /* Coming from mount(2) */
};
```

### squashfs_context_ops

```c
static const struct fs_context_operations squashfs_context_ops = {
	.get_tree	= squashfs_get_tree, // 申请super_block结构体
	.free		= squashfs_free_fs_context,
	.parse_param	= squashfs_parse_param,  // 解析文件系统配置参数, 也就是 mount 传进来的参数
	.reconfigure	= squashfs_reconfigure,
};
```

**执行顺序**

- squashfs_parse_param

解析 mount 挂载文件系统传进来的参数

- squashfs_get_tree

### squashfs_get_tree

squashfs_get_tree 函数其实就是 get_tree_bdev 的封装。

```c
 static int squashfs_get_tree(struct fs_context *fc)
{
	return get_tree_bdev(fc, squashfs_fill_super);
}   
```

#### get_tree_bdev

get_tree_bdev 函数中会申请 super_block 结构体，主要流程如下：

- blkdev_get_by_path ：找到 `fc->source` 。

比如 `mount -t squashfs /dev/block/mtdblock8  /system.ro` 中找到 `/dev/block/mtdblock8 ` 


- 调用传入的 squashfs_fill_super 函数， 执行文件系统自定义的操作。 这里一般做的是去解析文件系统的元数据， 并填充到文件系统的私有结构体中。

### squashfs_fill_super

squashfs_fill_super 这个函数非常重要，不同文件系统的 fill_super 实现都不同，这就导致了不同文件系统在不同应用场景或者硬件平台上性能的差异。

```c
static int squashfs_fill_super(struct super_block *sb, struct fs_context *fc)
{
	struct squashfs_mount_opts *opts = fc->fs_private;
	struct squashfs_sb_info *msblk;
	struct squashfs_super_block *sblk = NULL;
	struct inode *root;
	long long root_inode;
	unsigned short flags;
	unsigned int fragments;
	u64 lookup_table_start, xattr_id_table_start, next_table;
	int err;
	/*
	 * squashfs provides 'backing_dev_info' in order to disable read-ahead. For
	 * squashfs, I/O is not deferred, it is done immediately in readpage,
	 * which means the user would always have to wait their own I/O. So the effect
	 * of readahead is very weak for squashfs. squashfs_bdi_init will set
	 * sb->s_bdi->ra_pages and sb->s_bdi->io_pages to 0 and close readahead for
	 * squashfs.
	 */
	err = squashfs_bdi_init(sb);
	if (err) {
		errorf(fc, "squashfs init bdi failed");
		return err;
	}

	sb->s_fs_info = kzalloc(sizeof(*msblk), GFP_KERNEL);
	if (sb->s_fs_info == NULL) {
		ERROR("Failed to allocate squashfs_sb_info\n");
		return -ENOMEM;
	}
	msblk = sb->s_fs_info;

	msblk->panic_on_errors = (opts->errors == Opt_errors_panic);

	msblk->devblksize = sb_min_blocksize(sb, SQUASHFS_DEVBLK_SIZE);
	msblk->devblksize_log2 = ffz(~msblk->devblksize);

	mutex_init(&msblk->meta_index_mutex);

	/*
	 * msblk->bytes_used is checked in squashfs_read_table to ensure reads
	 * are not beyond filesystem end.  But as we're using
	 * squashfs_read_table here to read the superblock (including the value
	 * of bytes_used) we need to set it to an initial sensible dummy value
	 */
	msblk->bytes_used = sizeof(*sblk);  // 防止 squashfs_read_table 越界
	sblk = squashfs_read_table(sb, SQUASHFS_START, sizeof(*sblk));

	if (IS_ERR(sblk)) {
		errorf(fc, "unable to read squashfs_super_block");
		err = PTR_ERR(sblk);
		sblk = NULL;
		goto failed_mount;
	}

	err = -EINVAL;

	//检查是否是 squashfs 类型的 super_block
	/* Check it is a SQUASHFS superblock */
	sb->s_magic = le32_to_cpu(sblk->s_magic);
	if (sb->s_magic != SQUASHFS_MAGIC) {
		if (!(fc->sb_flags & SB_SILENT))
			errorf(fc, "Can't find a SQUASHFS superblock on %pg",
			       sb->s_bdev);
		goto failed_mount;
	}

	// 检查 MAJOR & MINOR 版本和查找压缩类型
	/* Check the MAJOR & MINOR versions and lookup compression type */
	msblk->decompressor = supported_squashfs_filesystem(
			fc,
			le16_to_cpu(sblk->s_major),
			le16_to_cpu(sblk->s_minor),
			le16_to_cpu(sblk->compression));
	if (msblk->decompressor == NULL)
		goto failed_mount;

	//检查文件系统没有超出块设备的末尾
	/* Check the filesystem does not extend beyond the end of the
	   block device */
	msblk->bytes_used = le64_to_cpu(sblk->bytes_used);
	if (msblk->bytes_used < 0 || msblk->bytes_used >
			i_size_read(sb->s_bdev->bd_inode))
		goto failed_mount;

	//检查块大小的完整性
	/* Check block size for sanity */
	msblk->block_size = le32_to_cpu(sblk->block_size);
	if (msblk->block_size > SQUASHFS_FILE_MAX_SIZE)
		goto insanity;

	/*
	 * Check the system page size is not larger than the filesystem
	 * block size (by default 128K).  This is currently not supported.
	 */
	if (PAGE_SIZE > msblk->block_size) {
		errorf(fc, "Page size > filesystem block size (%d).  This is "
		       "currently not supported!", msblk->block_size);
		goto failed_mount;
	}

	//检查块日志的完整性
	/* Check block log for sanity */
	msblk->block_log = le16_to_cpu(sblk->block_log);
	if (msblk->block_log > SQUASHFS_FILE_MAX_LOG)
		goto failed_mount;

	/* Check that block_size and block_log match */
	if (msblk->block_size != (1 << msblk->block_log))
		goto insanity;

	/* Check the root inode for sanity */
	root_inode = le64_to_cpu(sblk->root_inode);
	if (SQUASHFS_INODE_OFFSET(root_inode) > SQUASHFS_METADATA_SIZE)
		goto insanity;

	msblk->inode_table = le64_to_cpu(sblk->inode_table_start);
	msblk->directory_table = le64_to_cpu(sblk->directory_table_start);
	msblk->inodes = le32_to_cpu(sblk->inodes);
	msblk->fragments = le32_to_cpu(sblk->fragments);
	msblk->ids = le16_to_cpu(sblk->no_ids);
	flags = le16_to_cpu(sblk->flags);


	sb->s_maxbytes = MAX_LFS_FILESIZE;
	sb->s_time_min = 0;
	sb->s_time_max = U32_MAX;
	sb->s_flags |= SB_RDONLY;
	sb->s_op = &squashfs_super_ops;

	err = -ENOMEM;

	msblk->block_cache = squashfs_cache_init("metadata",
			SQUASHFS_CACHED_BLKS, SQUASHFS_METADATA_SIZE);
	if (msblk->block_cache == NULL)
		goto failed_mount;

	/* Allocate read_page block */
	msblk->read_page = squashfs_cache_init("data",
		squashfs_max_decompressors(), msblk->block_size);
	if (msblk->read_page == NULL) {
		errorf(fc, "Failed to allocate read_page block");
		goto failed_mount;
	}

	msblk->stream = squashfs_decompressor_setup(sb, flags);
	if (IS_ERR(msblk->stream)) {
		err = PTR_ERR(msblk->stream);
		msblk->stream = NULL;
		goto insanity;
	}

	/* Handle xattrs */
	sb->s_xattr = squashfs_xattr_handlers;
	xattr_id_table_start = le64_to_cpu(sblk->xattr_id_table_start);
	if (xattr_id_table_start == SQUASHFS_INVALID_BLK) {
		next_table = msblk->bytes_used;
		goto allocate_id_index_table;
	}

	/* Allocate and read xattr id lookup table */
	msblk->xattr_id_table = squashfs_read_xattr_id_table(sb,
		xattr_id_table_start, &msblk->xattr_table, &msblk->xattr_ids);
	if (IS_ERR(msblk->xattr_id_table)) {
		errorf(fc, "unable to read xattr id index table");
		err = PTR_ERR(msblk->xattr_id_table);
		msblk->xattr_id_table = NULL;
		if (err != -ENOTSUPP)
			goto failed_mount;
	}
	next_table = msblk->xattr_table;

allocate_id_index_table:
	/* Allocate and read id index table */
	msblk->id_table = squashfs_read_id_index_table(sb,
		le64_to_cpu(sblk->id_table_start), next_table, msblk->ids);
	if (IS_ERR(msblk->id_table)) {
		errorf(fc, "unable to read id index table");
		err = PTR_ERR(msblk->id_table);
		msblk->id_table = NULL;
		goto failed_mount;
	}
	next_table = le64_to_cpu(msblk->id_table[0]);

	/* Handle inode lookup table */
	lookup_table_start = le64_to_cpu(sblk->lookup_table_start);
	if (lookup_table_start == SQUASHFS_INVALID_BLK)
		goto handle_fragments;

	/* Allocate and read inode lookup table */
	msblk->inode_lookup_table = squashfs_read_inode_lookup_table(sb,
		lookup_table_start, next_table, msblk->inodes);
	if (IS_ERR(msblk->inode_lookup_table)) {
		errorf(fc, "unable to read inode lookup table");
		err = PTR_ERR(msblk->inode_lookup_table);
		msblk->inode_lookup_table = NULL;
		goto failed_mount;
	}
	next_table = le64_to_cpu(msblk->inode_lookup_table[0]);

	sb->s_export_op = &squashfs_export_ops;

handle_fragments:
	fragments = msblk->fragments;
	if (fragments == 0)
		goto check_directory_table;

	msblk->fragment_cache = squashfs_cache_init("fragment",
		SQUASHFS_CACHED_FRAGMENTS, msblk->block_size);
	if (msblk->fragment_cache == NULL) {
		err = -ENOMEM;
		goto failed_mount;
	}

	/* Allocate and read fragment index table */
	msblk->fragment_index = squashfs_read_fragment_index_table(sb,
		le64_to_cpu(sblk->fragment_table_start), next_table, fragments);
	if (IS_ERR(msblk->fragment_index)) {
		errorf(fc, "unable to read fragment index table");
		err = PTR_ERR(msblk->fragment_index);
		msblk->fragment_index = NULL;
		goto failed_mount;
	}
	next_table = le64_to_cpu(msblk->fragment_index[0]);

check_directory_table:
	/* Sanity check directory_table */
	if (msblk->directory_table > next_table) {
		err = -EINVAL;
		goto insanity;
	}

	/* Sanity check inode_table */
	if (msblk->inode_table >= msblk->directory_table) {
		err = -EINVAL;
		goto insanity;
	}

	/* allocate root */
	root = new_inode(sb);
	if (!root) {
		err = -ENOMEM;
		goto failed_mount;
	}

	err = squashfs_read_inode(root, root_inode);
	if (err) {
		make_bad_inode(root);
		iput(root);
		goto failed_mount;
	}
	insert_inode_hash(root);

	sb->s_root = d_make_root(root);
	if (sb->s_root == NULL) {
		ERROR("Root inode create failed\n");
		err = -ENOMEM;
		goto failed_mount;
	}

	TRACE("Leaving squashfs_fill_super\n");
	kfree(sblk);
	return 0;

insanity:
	errorf(fc, "squashfs image failed sanity check");
failed_mount:
	squashfs_cache_delete(msblk->block_cache);
	squashfs_cache_delete(msblk->fragment_cache);
	squashfs_cache_delete(msblk->read_page);
	squashfs_decompressor_destroy(msblk);
	kfree(msblk->inode_lookup_table);
	kfree(msblk->fragment_index);
	kfree(msblk->id_table);
	kfree(msblk->xattr_id_table);
	kfree(sb->s_fs_info);
	sb->s_fs_info = NULL;
	kfree(sblk);
	return err;
}
```


