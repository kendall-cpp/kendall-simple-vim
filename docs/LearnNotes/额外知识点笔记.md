# linux 学习笔记

----

# 安装和编译自己的内核 (TODO)

> **ARM64 版本**

参考网址：https://www.jianshu.com/p/a0d166bfe21f

## 安装相关支持库

```sh
apt-get install libpixman-1-dev
 
sudo apt-get install zlib1g-dev
sudo apt-get install libglib2.0-0
sudo apt-get install libglib2.0-dev
```

## 安装qemu 

从qemu官网下载源码文件 qemu-6.2.0.tar.xz

```sh
tar -xvf qemu-6.2.0.tar.xz

# 编 译
export PATH="/home/book/kenspace/linux-kernel/toolchain/gcc-linaro-7.5.0-2019.12-x86_64_arm-linux-gnueabihf/bin:${PATH}"
./configure --prefix=/home/book/kenspace/linux-kernel/qemu   # 指定需要安装的路径

./configure --target-list=x86_64-softmmu,x86_64-linux-user,arm-softmmu,arm-linux-user,aarch64-softmmu,aarch64-linux-user --enable-kvm --prefix=/home/book/kenspace/linux-kernel/qemu  
```

如果出现编译错误，参考这里解决。 https://blog.csdn.net/birencs/article/details/126666827


make 

最后将 `/home/book/kenspace/linux-kernel/qemu/bin` 添加到环境变量中

qemu安装完成。

## 安装交叉编译器

```
sudo apt-get install gcc-arm-linux-gnueabi
```

## 配置脚本目录

参考我的目录关系

```sh
book@kendall:~/kenspace/linux-kernel$ ls
buildroot-2022.02  build-vexpressa9  qemu  buildroot-dl

book@kendall:~/kenspace/linux-kernel/build-vexpressa9$ ls
build.sh    output-vexpress-v2p-ca9 
```

cd build-vexpressa9

在该目录下建立编译建立 build.sh ，输入如下内容

```sh
#! /bin/bash

TPWD=$(pwd)
echo $TPWD
cd $TPWD/../buildroot-2022.02/
#mkdir -p $TPWD/output-vexpress-v2p-ca9
make O=$TPWD/output-vexpress-v2p-ca9 ARCH=arm $1
```

## 配置 buildroot

> 我这里使用的 buildroot 版本是 buildroot-2022.02

- make defconfig

这里的 defconfig 是根据不同型号的板子自行确定的，对应于 buildroot/configs 目录下的配置文件，我们这里仿真的是vexpress，qemu_arm_vexpress_defconfig 。

- 配置

```sh
cd build-vexpressa9/
./build.sh qemu_arm_vexpress_defconfig
```

可以看到 build-vexpressa9/output-vexpress-v2p-ca9 文件夹下生成了 .config 文件

注意： 这一步之后就不要再 ./build.sh qemu_arm_vexpress_defconfig 了，否则会重新初始化 .config 

- 配置 buildroot 相关参数

安装 menuconfig 的依赖库文件

```sh
sudo apt-get install build-essential 
sudo apt-get install libncurses5 
sudo apt-get install libncurses5-dev 
```

```sh
cd build-vexpressa9/
./build.sh menuconfig
```

- Target options主要是和架构有关的配置，一般我们使用ARCH=arm或者其他架构后，一般不需要做调整

- Build options主要是设置和buildroot相关的参数，比如说下载目录、主机环境的配置地址等等

```sh
/home/book/kenspace/linux-kernel/buildroot-2022.02/configs/qemu_arm_vexpress_defconfig) Location to save buildroot config
($(TOPDIR)/../buildroot-dl) Download dir  # 这里我们需要配置Build options，主要是更改Download dir，即buildroot下载文件的存放目录，包括 kernel 源码
($(BASE_DIR)/host) Host dir 
# 选择下载的网址
    Mirrors and Download locations  --->
()  Primary download site 
(http://sources.buildroot.net) Backup download site
(https://cdn.kernel.org/pub) Kernel.org mirror 
(http://ftpmirror.gnu.org) GNU Software mirror 
(http://rocks.moonscript.org) LuaRocks mirror
(http://cpan.metacpan.org) CPAN mirror (Perl packages)
```

- Toolchain配置工具链相关的参数，可以使用外部自己的，也可以网上下载的，又或者直接使用buildroot帮忙编译的

```sh
# 我这里选择 buildroot 自己下载 toolchain , 因为选择额外的会出错
    Toolchain type (Buildroot toolchain)  --->  
     (X) External toolchain 
     Toolchain (Custom toolchain)  ---> 
     Toolchain origin (Pre-installed toolchain)  --->
   (/home/book/kenspace/linux-kernel/toolchain/gcc-linaro-7.5.0-2019.12-x86_64_arm-linux-gnueabihf) Toolchain path  
  ($(ARCH)-linux-gnueabihf) Toolchain prefix 
     External toolchain gcc version (7.x)  --->
     External toolchain kernel headers series (4.10.x)  ---> 
     External toolchain C library (glibc/eglibc)  ---> 
  [*] Toolchain has SSP support? (NEW) 
  [*]   Toolchain has SSP strong support? (NEW) 
  [*] Toolchain has RPC support? (NEW) 
```

- System configuration配置文件系统相关的参数

- Kernel配置内核相关信息，比如源码位置、生成文件、加载地址等等

```
[*] Linux Kernel 
   Kernel version (Custom tarball)  --->  
  (https://mirror.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.15.tar.xz) URL of custom kernel tarball 
(vexpress) Defconfig name 
      Kernel compression format (gzip compression)  --->
[*]   Build a Device Tree Blob (DTB)
 (vexpress-v2p-ca9) In-tree Device Tree Source file name
```

- Target packages是buildroot配置应用包的地方，后面需要用到的很多应用都可以直接配置，包括像opencv这样的库，当然也可以自己添加配置应用包

- Filesystem images主要设置的是文件系统的镜像格式，可以根据需要使用yaffs2、initial RAM等等格式

···
 [*] ext2/3/4 root filesystem  
       ext2/3/4 variant (ext2 (rev1))  ---> 
···

- Bootloaders 主要配置的启动引导方式

## 配置kernel参数

我们可以提前去网上下载 kernel 到 buildroot-dl/linux/linux-5.15.tar.xz  这样编译的时候会方便很多

## 编译 

```sh
build-vexpressa9$ ./build.sh -j4
```

- 如果报错： multiple (or no) load addresses: 
 
```sh
 ./build.sh LOADADDR=0x80008000 -j4
```

- fakeroot: preload library `libfakeroot.so' not found, aborting.

sudo apt-get install fakeroot

如果还不行，尝试安装：

sudo apt-get install cramfsprogs

之后只需要修改  build-vexpressa9/output-vexpress-v2p-ca9/build/linux-custom 这里的 kernel 源码即可

然后再 ./build.sh linux-rebuild

可能需要移除掉之前安装多余的 gcc ,根据提示操作

```
sudo apt autoremove cpp-7-aarch64-linux-gnu 
sudo apt autoremove cpp-aarch64-linux-gnu 
sudo apt autoremove gcc-7-aarch64-linux-gnu 
sudo apt autoremove gcc-7-aarch64-linux-gnu-base 
sudo apt autoremove libasan4-arm64-cross 
sudo apt autoremove libatomic1-arm64-cross 
sudo apt autoremove libc6-arm64-cross libc6-dev-arm64-cross
sudo apt autoremove libgcc-7-dev-arm64-cross 
sudo apt autoremove libgcc1-arm64-cross 
sudo apt autoremove libgomp1-arm64-cross 
sudo apt autoremove libitm1-arm64-cross 
sudo apt autoremove liblsan0-arm64-cross 
sudo apt autoremove libstdc++6-arm64-cross 
sudo apt autoremove libtsan0-arm64-cross 
sudo apt autoremove libubsan0-arm64-cross 
sudo apt autoremove linux-hwe-5.4-headers-5.4.0-139
sudo apt autoremove linux-libc-dev-arm64-cross
```


再不行可能需要删除原来编译的 rootfs 

```
build-vexpressa9/output-vexpress-v2p-ca9$ mv images images-bak
```

## 运行 qemu

 cd output-vexpress-v2p-ca9/images
 vi kernel-qemu.sh


```sh
#!/bin/sh
IMAGE_DIR="${0%/*}/"
BUILD_ROOTDIR=`realpath ../`
echo $BUILD_ROOTDIR
cp $BUILD_ROOTDIR/build/linux-custom/arch/arm/boot/zImage .
#cp $BUILD_ROOTDIR/build/linux-custom/arch/arm/boot/uImage .
cp $BUILD_ROOTDIR/build/linux-custom/arch/arm/boot/dts/vexpress-v2p-ca9.dtb .

if [ "${1}" = "only" ]; then
    EXTRA_ARGS='-nographic'
else
    EXTRA_ARGS='-serial stdio'
fi

export PATH="/home/vencol/code/vexpressa9/output-vexpress-v2p-ca9/host/bin:${PATH}"
exec   qemu-system-arm -M vexpress-a9 -smp 1 -m 256 -kernel ${IMA1GE_DIR}/zImage -dtb ${IMAGE_DIR}/vexpress-v2p-ca9.dtb -drive file=${IMAGE_DIR}/rootfs.ext2,if=sd,format=raw -append "console=ttyAMA0,115200 rootwait root=/dev/mmcblk0"  -net nic,model=lan9118 -net user  ${EXTRA_ARGS}
```

```
chmod +x kernel-qemu.sh

./kernel-qemu.sh only
```


---

# 进程原理和系统调用 (TODO)

## 进程概述

> **什么是进程？**

进程: 直观的说，就是保存在硬盘中的程序在运行以后，那么这个运行起来的执行程序就是进程了。另外，操作系统会以进程为单位进行分配资源，比如说CPU时间片，内存等资源。进程是资源分配的最小单位。

> **进程的生命周期**

系统中的每个进程能够分时复用 CPU 的时间片，所以操作系统必须要设计有效的进程调策略实现多任务并行执行。进程在被 CPU 调度运行时有不同的状态。

- 创建状态
- 就绪状态：获取到了运行资源和运行条件
- 执行状态：进程正在 CPU 中执行操作
- 阻塞状态：进程因资源被占用而释放 CPU
- 终止状态：进程终止

在 linux 内核中提供 API 函数来设置进程的状态

- TASK_RUNNING：可运行状态，进程要么在CPU上执行，要么准备执行。
- TASK_INTERRUPTIBLE：可中断的等待状态，进程被挂起(睡眠)，直到某个条件为真，产生一个硬中断、释放进程正等待的系统资源、或传递一个信号都是可以唤醒进程的条件。
- TASK_UNINTERRUPTIBLE：不可中断的等待状态，与可中断等待状态类似，只是不能被信号唤醒。在一些特殊情况下会使用，例如：当进程打开一个设备文件，设备驱动会开始探测相应的硬件时会用到这种状态。
- TASK_STOPED：暂停状态，当进程接收到SIGSTOP、SIGTSTP、SIGTTIN或SIGTTOU信号后进入。
- TASK_TRACED：跟踪状态，进程执行由debugger程序暂停，当一个进程被另一个进程监控时，任何信号都可以把这个进程置于TASK_TRACED状态。

还有两个状态是既可以存放在进程描述符的 state 字段中，也可以存放在 exit_state 字段中。从这两个字段可以看出，只有当进程执行被终止时，进程的状态才会为这两种状态中的一种：

- EXIT_ZOMBIE：僵死状态，进程将被终止，但父进程还没有发布 wait4() 或者 waitpid() 系统调用来返回关于死亡进程的信息。发布 wait() 类系统调用之前，内核不能丢弃包含在死进程描述符中的数据，因为父进程可能还需要它。(一般出现这种状态的原因都是父进程没有响应子进程的死亡信号,可能父进程处于 TASK_INTERRUPTIBLE 状态或者 TASK_UNINTERRUPTIBLE 状态)
- EXIT_DEAD：僵死撤销状态，进程被终止后的最终状态，父进程发布 wait4() 或者 waitpid() 系统调用后，内核删除此进程描述符。

![](https://raw.githubusercontent.com/kendall-cpp/blogPic/main/blog-01/%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81.png)

> 我们使用一个简单地例子说明这种状态的转变，我们有个程序A，它的工作就是做一些计算，然后把计算结构写入磁盘文件中。我们在 shell 中运行它，起初它就是 TASK_RUNNING 状态，也就是运行态，CPU 会不停地分配时间片供我们的进程 A 运行，每次时间片耗尽后，进程 A 都会转变到就绪态(实际上还是 TASK_RUNNING 状态，只是此时在等待 CPU 分配时间片，暂时不在 CPU 上运行)。当进程 A使 用 fwrite 或 write 将数据写入磁盘文件时，就会进入阻塞态( TASK_INTERRUPTIBLE 状态)，而磁盘将数据写入完毕后，会通过一个中断告知内核，内核此时会将进程A的状态由阻塞态( TASK_INTERRUPTIBLE )转变为就绪态( TASK_RUNNING )等待CPU分配时间片运行。而最后当进程 A 需要退出时，内核先会将其设置为僵死状态( EXIT_ZOMBIE )，这时候它所使用的内存已经被释放，只保留了一个进程描述符供父进程使用，最后当父进程(也就是我们起初启动它的 shell )通过 wait() 类系统调用通知内核后，内后会将进程A设置为僵死撤销状态( EXIT_DEAD )，并释放其进程描述符。到这里进程 A 的整个运行周期完整结束。

> **Linux 中进程表示**

为了描述控制进程的运行，系统中存放进程的管理和控制信息的数据结构（`task_struct`）称为**进程的控制块** PCB（Process Control Block），它是进程实体的一部分，是操作系统中重要的数据结构。也就是说一个`task_struct`  就是一个 PCB ，我们在调用 `fork()` 的时候，系统就会产生一个 stsk_struct 结构，然后从父进程那你继承一些数据，并把新创建的进程插入到进程数中 。

> task_struct 在文件 include/linux/sched.h 中，成员介绍可以参考：https://www.cnblogs.com/JohnABC/p/9084750.html



## 进程优先级

通过 ps -le 命令

```sh
$ ps -le | head
F S   UID    PID   PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD
4 S     0      1      0  0  80   0 - 56554 -      ?        00:00:36 systemd
1 S     0      2      0  0  80   0 -     0 -      ?        00:00:00 kthreadd
1 I     0      3      2  0  60 -20 -     0 -      ?        00:00:00 rcu_gp
1 I     0      4      2  0  60 -20 -     0 -      ?        00:00:00 rcu_par_gp
1 I     0      6      2  0  60 -20 -     0 -      ?        00:00:00 kworker/0:0H-kb
1 I     0      9      2  0  60 -20 -     0 -      ?        00:00:00 mm_percpu_wq
1 S     0     10      2  0  80   0 -     0 -      ?        00:00:04 ksoftirqd/0
1 I     0     11      2  0  80   0 -     0 -      ?        00:00:21 rcu_sched
1 S     0     12      2  0 -40   - -     0 -      ?        00:00:00 migration/0
```

上面的输出中，PRI 表示 Priority ， NI 表示 Nice ， 这两个值表示优先级，数字越小代表这个进程优先级越高，也就是越优先被 CPU 处理，不过 PRI 值是由内核动态调整的，用户不能直接修改，所以用户层只能通过修改 NI 值来影响 PRI 值，间接地调整进程优先级。

PRI 和 NI 的关系如下：

PRI (最终值) = PRI (原始值) + NI

> 其实，大家只需要记得，在用户态，我们修改 NI 的值就可以改变进程的优先级。NI 值越小，进程的 PRI 就会降低，该进程就越优先被 CPU 处理；反之，NI 值越大，进程的 PRI 值就会増加，该进程就越靠后被 CPU 处理。

**修改 NI 值时有几个注意事项：**

- NI 范围是 -20~19 。
- 普通用户调整 NI 值的范围是 0~19，而且只能调整自己的进程。
- 普通用户只能**调高** NI 值，而**不能降低**。如原本 NI 值为 0，则只能调整为大于 0。
- 只有 root 用户才能设定进程 NI 值为负值，而且可以调整任何用户的进程。


> **task_struct 中描述进程优先级的成员如下：**

```c
int                             prio; 
int                             static_prio;
int                             normal_prio;
unsigned int                    rt_priority;
```


<table>
	<tr>
	    <th >优先级</th>
	    <th>限期进程</th>
	    <th>实时进程</th>  
      <th>普通进程</th>  
	</tr >
	<tr>
      <th>prio</th>
      <td colspan="3">大多数情况下， `prio = normal_prio` ，特殊情况下，如果进程 A 占用实时互斥锁，进程 B 正在等待锁，进程 B 的优先级就比进程 A 优先级低，那么把进程 A 的优先级临时提高到 A 进程 的优先级，那么进程 A 的 prio 就等于进程 B 的 prio</td>
	</tr >
	<tr>
	    <th>static_prio</th>
	    <td>没有意义 0</td>
	    <td>没有意义 0</td>
	    <td>120*ni, 数值越小，优先级越高</td>
	</tr >
	<tr>
	    <th>normal_prio</th>
	    <td>-1</td>
	    <td>99 - rt_priority</td>
	    <td>static_prio</td>
	</tr >
	<tr>
	    <th>rt_priority</th>
	    <td>没有意义 0</td>
	    <td>实时进程的优先级，范围1-9， 数值越大，优先级越高</td>
	    <td>没有意义 0</td>
	</tr >
</table>



# ARM 中的中断(TODO)

> **什么是中断？**

从本质上来讲，中断是一种电信号，当设备有某种事件发生时，它就会产生中断，通过总线把电信号发送给中断控制器。中断控制器就把电信号发送给处理器的某个特定引脚。处理器于是立即停止自己正在做的事，跳到中断处理程序的入口点，进行中断处理。

<strong><font color="orange" size="4">
外设 --（电信号）--> 中断控制器 --（中断线是否激活？）--> CPU 的引脚 ---> 停止当前的任务 ---> 处理中断
</font></strong>


从软件方面，在 CPU 正常运行期间，由于内部或者外部事件引起 CPU 暂时停止正在运行的程序，转去这个内部或者外部事件程序中去执行，服务完这个事件程序之后再返回继续运行被暂停的程序。

### 硬中断 和 软中断

中断可分为：**硬件中断** 和 **软中断**

- **硬件中断**（hardware interrunpt）: 由与系统相连的**外设**(比如网卡、硬盘)自动产生的。主**要是用来通知操作系统系统外设状态的变化**。比如当网卡收到数据包的时候，就会发出一个中断。我们通常所说的中断指的是硬中断(hardirq)。

- **软中断**（SoftIRQ）: 为了满足实时系统的要求，中断处理应该是越快越好。linux为了实现这个特点，当中断发生的时候，硬中断处理那些短时间就可以完成的工作，而将那些处理事件比较长的工作，放到中断之后来完成，也就是软中断(softirq)来完成。
  - 软中断处理硬中断未完成的工作，是一种推后执行的机制，属于**下半部**

另外需要注意的是，Linux 下硬中断是可以嵌套的，但是没有优先级的概念，也就是说任何一个新的中断都可以打断正在执行的中断，但同种中断除外。软中断不能嵌套，但相同类型的软中断可以在不同 CPU 上并行执行。


### 不可屏蔽中断 和 可屏蔽中断

硬件中断可分为 **不可屏蔽中断 NMI** 和 **可屏蔽中断 INTR**，NMI 用于紧急情况的故障处理，如R AM 奇偶校验错等，INTR 则用于外部依靠中断来工作的硬件设备。网卡使用的就是 INTR，

> 不可屏蔽中断源一旦提出请求，cpu必须无条件响应，而对于可屏蔽中断源的请求，cpu可以响应，也可以不响应。 

CPU 一般会设置 2 根中断请求输入线


- 可屏蔽中断 INTR(Interrupt Require)

除了受本身的屏蔽位的控制外，还都要受一个总的控制，即CPU标志寄存器中的中断允许标志位IF(Interrupt Flag)的控制，IF位为1，可以得到CPU的响应，否则，得不到响应。IF位可以有用户控制，指令STI或Turbo c的Enable()函数，将IF位置1(开中断)，指令CLI或Turbo_c 的Disable()函数，将IF位清0(关中断)。典型的非屏蔽中断源的例子是电源掉电，一旦出现，必须立即无条件地响应，否则进行其他任何工作都是没有意义的。

- 不可屏蔽中断请求 NMI(Nonmaskable Interrupt)

典型的可屏蔽中断源的例子是打印机中断，CPU 对打印机中断请求的响应可以快一些，也可以慢一些，因为让打印机等待儿是完全可以的。

### 相关问题

> 对于软中断，I/O操作是否是由内核中的I/O设备驱动程序完成？

答：对于I/O请求，内核会将这项工作分派给合适的内核驱动程序，这个程序会对I/O进行队列化，以可以稍后处理（通常是磁盘I/O），或如果可能可以立即执行它。通常，当对硬中断进行回应的时候，这个队列会被驱动所处理。当一个I/O请求完成的时候，下一个在队列中的I/O请求就会发送到这个设备上。

> 软中断所经过的操作流程是比硬中断的少吗？换句话说，对于软中断就是：进程 ->内核中的设备驱动程序；对于硬中断：硬件->CPU->内核中的设备驱动程序？

答：是的，软中断比硬中断少了一个硬件发送信号的步骤。产生软中断的进程一定是当前正在运行的进程，因此它们不会中断CPU。但是它们会中断调用代码的流程。

### linux 源码中的软中断和硬中断

https://zhuanlan.zhihu.com/p/85597791

## 中断向量表

https://blog.csdn.net/honour2sword/article/details/40213417?spm=a2c6h.12873639.article-detail.5.7ff61d90Ok7ugZ

## 中断控制器是及中断域

https://blog.csdn.net/u013836909/category_10181523.html

GIC 架构规范， 

## 中断处理过程

每个中断都维护一个状态机，支持Inactive、Pending、Active、Active and pending。 中断处理的状态机如下图：

![](https://doc.embedfire.com/linux/rk356x/driver/zh/latest/_images/interr05.png)


- Inactive：无中断状态，即没有 Pending 也没有 Active。

- Pending：硬件或软件触发了中断，该中断事件已经通过硬件信号通知到 GIC，等待 GIC分配的那CPU进行处理，在电平触发模式下，产生中断的同时保持Pending状态。

- Active：CPU已经应答该中断请求，并且正在处理中。

- Active and pending：当一个中断源处于Active状态的时候，同一中断源又触发了中断，进入pending状态，挂起状态。

**一个简单的中断处理过程是**：外设发起中断，发送给Distributor ，Distributor并基于它们的中断特性(优先级、是否使能等等)对中断进行分发处理，分发给合适的Redistributor， Redistributor 将中断信息，发送给 CPU interface，CPU interface产生合适的中断异常给处理器，处理器接收该异常，最后软件处理该中断。

##  Linux 系统对中断处理的方式

### softirq（性能好）

### tasklet（易用）

### workqueue

### threaded irq

----



# 内存管理

## 引导内存分配器

引导内存分配器是指在操作系统启动时，也就是引导阶段所使用的内存分配器，在这个阶段，因为操作系统还没有完全加载和运行，因此不能使用伙伴系统等高级内存管理算法，也不能依赖于已经存在的内存分配器。


引导内存分配器的主要任务是在启动时从操作系统获取一部分可用的物理内存，然后将其划分成多个大小不同的内存块，以便后续分配给应用程序使用。这个过程通常称为**内存映射**（Memory Mapping）。

在内存映射完成后，引导内存分配器维护一个内存块列表，记录每个内存块的大小和状态（已分配或未分配）。当应用程序需要内存时，它会向引导内存分配器发出请求，请求一定大小的内存块。引导内存分配器会在内存块列表中查找一个合适的未分配内存块，并将其标记为已分配状态，然后返回该内存块的起始虚拟地址给应用程序。如果找不到合适的未分配内存块，则会通过其他方式获取更多的内存。

![](https://raw.githubusercontent.com/kendall-cpp/blogPic/main/blog-01/%E5%BC%95%E5%AF%BC%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8.png)

> 引导内存分配器为了避免出现内存碎片和内存浪费等问题，它通常采用各种高效的算法和策略来管理内存。例如，在分配大内存块时，它可能会使用内存池（Memory Pool）技术，预先将一些固定大小的内存块缓存起来，以便快速分配；在释放内存块时，它可能会将相邻的未分配内存块合并成一个更大的内存块，以充分利用可用空间。此外，引导内存分配器还会考虑性能和安全性等方面的因素，确保系统运行稳定、高效和安全。

### bootmem 内存分配器

> linux-4.12

- 数据结构

```c
typedef struct bootmem_data {
	unsigned long node_min_pfn;  //其实的物理页号
	unsigned long node_low_pfn;  // 结束物理页号
	void *node_bootmem_map;  // 指向一个位图，每个物理页对应一位，如果物理页被分配，把对应的位设置为 1
	unsigned long last_end_off;  // 上次分配的内存块的结束位置后面一个字节的偏移
	unsigned long hint_idx;  // 下一次将分配物理页索引
	struct list_head list;  // 内核链表
} bootmem_data_t;
```

每个内存节点都有一个 bootmem_data 实例。

```c
struct bootmem_data;
typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;  // 该节点包含内存域数量
#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
	struct page *node_mem_map;
#ifdef CONFIG_PAGE_EXTENSION
	struct page_ext *node_page_ext;
#endif
#endif
#ifndef CONFIG_NO_BOOTMEM
	struct bootmem_data *bdata;  // 引导bootmem分配器
#endif
```

#### bootmem 分配器的算法

bootmem 分配器是一种用于在 Linux 内核启动阶段分配物理内存的算法。它的基本思想是将系统可用的物理内存按照固定大小的块进行划分，并采用**位图**来管理每个块的分配情况。

具体的算法流程如下：

- 在内存的起始位置建立一个 数据结构，用于记录系统可用的内存块信息。

- 遍历系统的内存空间，将其按照指定的块大小进行分割，记录每个块的大小和地址，并将其标记为未分配状态。

- 使用位图来表示每个块的分配情况，其中每一位代表一个内存块的分配状态。如果位图中的某一位为 0，则表示对应的内存块尚未被分配；如果为 1，则表示该内存块已经被分配。

- 当需要分配内存时，遍历位图，找到第一个连续的未分配块，并将其标记为已分配状态。然后返回该块的起始地址作为分配的内存地址。

- 当释放内存时，将对应内存块的位图从 1 改为 0，表示该内存块变为了未分配状态。

需要注意的是，bootmem 分配器只能在内核启动阶段使用，因为它依赖于系统的物理内存布局以及启动过程中的一些数据结构。在内核启动完成后，就需要使用其他更高级的内存分配算法

> **arm64 架构内核不使用 bootmem 分配器，但是其他的处理器架构还在使用 bootmem 分配器**

在arm64架构的Linux内核中，arm64 使用的是 memblock 分配器。

### memblock 分配器

数据结构

- **memblock**

```c
//include/linux/memblock.h
struct memblock {
	bool bottom_up;  // 表示分配内存的方式，true -- 向上分配，flase -- 向下分配
	phys_addr_t current_limit; // 可分配的最大物理地址
	struct memblock_type memory; // 内存类型（已分配和未分配的内存）
	struct memblock_type reserved; // 预留类型（已经分配的内存）
#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP
	struct memblock_type physmem;  // 物理内存类型
#endif
};
```

> **物理内存类型和内存类型的区别 ？**

- 内存类型是物理内存类型的子集，在引导内核时可以使用内核参数 `mem=nn[KMG}` ，指定可用内存的大小，导致内核不能看见所有的内存，只能看见它能使用的内存。

- 物理内存类型包含所有的内存。内存类型只包含内核参数 “mem=” 指定的那部分内存。

> 在 Linux 内核源码中，`mem=nn[KMG]` 是用来指定系统可用的物理内存大小的一个启动参数。这个参数可以在系统启动时通过 boot loader 的命令行选项来设置。
>
> 具体来说，这个参数的作用是告诉内核只使用指定大小的物理内存，而忽略剩余的内存。其中，nn 表示内存大小，K、M、G 分别表示千字节、兆字节、吉字节。例如，mem=512M 表示只使用 512MB 的物理内存。
>
> 这个参数通常用于测试或调试目的，在某些情况下也可以用来解决系统启动失败或崩溃的问题。需要注意的是，如果设置了这个参数，可能会导致系统无法使用全部的物理内存，因此应该谨慎使用，并根据实际情况进行配置。

- **memblock_type**

```c
//include/linux/memblock.h
struct memblock_type {
	unsigned long cnt;	//该内存块类型的数量
	unsigned long max;	// 内存区域的最大个数
	phys_addr_t total_size;	// 所有内存区域的总长度
	struct memblock_region *regions; // 执行内存区域结构指针
	char *name; 
};
// struct memblock_type 结构体在新版本的内核中仍然扮演着管理物理内存块的重要角色，并且在原有的基础上进行了一些微小的改进，以提高其在内核中的实用性。
```


```c
//include/linux/memblock.h
struct memblock_region {
	phys_addr_t base;  //起始物理地址
	phys_addr_t size;  // 内存块类型所占据的大小
	unsigned long flags;
#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
	int nid;  // 节点编号
#endif
};

enum {
	// 没有特殊要求区域
	MEMBLOCK_NONE		= 0x0,	/* No special request */
	// 可以热插拔的区域，也就是系统运行过程中可以拔出或者插入的物理内存
	MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
	// 镜像区域
	MEMBLOCK_MIRROR		= 0x2,	/* mirrored region */
	// 不添加到内核直接映射区域（也就是线性映射区域）
	MEMBLOCK_NOMAP		= 0x4,	/* don't add to kernel direct mapping */
};
```

![](https://cdn.staticaly.com/gh/kendall-cpp/blogPic@main/blog-01/image.6m2bn9503ms0.webp)

#### ARM64 内核初始化 memblock 分配器过程

主要实现是这个函数

```c
// arch/arm64/mm/init.c 
void __init arm64_memblock_init(void)
{

}
// 29-1.png

// mm/memblock.c
// 插入一块可用的物理内存
// base 指向要添加内存块的起始物理地址
// size 指向要添加内存块的大小
int __init_memblock memblock_add(phys_addr_t base, phys_addr_t size)
{
	phys_addr_t end = base + size - 1;

	memblock_dbg("memblock_add: [%pa-%pa] %pF\n",
		     &base, &end, (void *)_RET_IP_);

	// 将内存块添加到 memblock.memory
	return memblock_add_range(&memblock.memory, base, size, MAX_NUMNODES, 0);
}

/**
type: 指向内存区，也就是预留内存
base: 指向新加入的内存块的基础地址
size：指向新加入的内核块的长度
nid : 指向新加入内存块对应的 flags
*/
int __init_memblock memblock_add_range(struct memblock_type *type,
				phys_addr_t base, phys_addr_t size,
				int nid, unsigned long flags)
{
	// 遍历这个内存区的所有内存块，每遍历到一个内存块，函数就会将新的内存块和这个内存块进行比较
	for_each_memblock_type(type, rgn) {
		phys_addr_t rbase = rgn->base;
		phys_addr_t rend = rbase + rgn->size;

		...
			if (insert)
				memblock_insert_region(type, idx++, base,
						       rbase - base, nid,
						       flags);
		}
		/* area below @rend is dealt with, forget about it */
		base = min(rend, end);
	}

	// 没有新的内存区块需要加入到内存区块链表
	if (!nr_new)
		return 0;
}
//30-1.png
```

- memblock_add : 添加新的内存块到 memblock.memory 中。
- memblock_renove : 删除内存块区域
- memblock_alloc : 分配内存
- memblock_free : 释放内存

**memblock 内存分配器的原理：**

主要是维护两种内存：

- 第一种内存是系统可用的物理内存，也就是系统实际含有的物理内存，这个值是从 dts 中进行配置的， 通过 uboot 实际探测之后再插入到内核中。

- 第二种内存是内核预留给操作系统的内存，这部分内存作为特殊功能使用，不能作为共享内存使用。

## linux常见物理内存的管理


在 Linux 中，内存分配的基本单元是“页框”（page frame），其大小通常为 4KB。但是，Linux 支持通过配置来使用不同大小的页框，最大可支持 2MB 大小的页框。

> 为什么呢？

采用 4KB 作为页框大小是出于多方面的考虑。首先，这个大小足够小以满足大多数应用程序的内存管理需求，同时也不会占用过多的内存。其次，较小的页框大小可以提高内存空间的利用率，并减少对磁盘 I/O 的访问次数，从而提高系统性能。此外，4KB 页框大小也便于与硬件平台集成，因为这是目前主流处理器架构的标准大小。在某些场景下，如大规模内存数据库或缓存服务器等应用，可能需要使用更大的页面来提高系统性能，但这通常需要进行额外的配置和调优。

### linux常见物理内存的管理方法有哪些

- Buddy 系统（伙伴系统）：将可用物理内存块分成大小相等的块，并使用二叉树结构来跟踪每个块的可用性和状态。
  - 通常用于分配**大块内存**。它的优势是简单高效，适用于对**内存碎片化要求较低**的场景。
  - 例如数组、图像、视频或者音频文件。这些对象通常需要长期存储，并且在整个程序运行期间都可能被多次访问

- Slab分配器：是一种基于对象缓存的内存分配器，用于分配小块内存。它通过缓存已经分配过的对象来提高性能，并具有更好的内存碎片管理能力。它的优势是可以减少内存碎片化，适合频繁分配和释放**小块内存**的场景。
  - 例如，在编写循环或函数时，可能需要使用小块内存来存储迭代器或局部变量。通常用于存储数据结构、变量、指针等较小的数据对象

#### Buddy系统原理

Buddy系统是一种内存管理算法，用于动态分配和释放内存。它的原理是将可用内存空间划分为大小相等且为2的幂次方的块，每个块可以被分配或释放。

- 当需要分配内存时，Buddy系统会搜索可用块列表，直到找到一个大小合适的块来满足需求。如果系统需要存储一个大块数据但没有足够大的空闲空间可用，则系统会把一个更大的空闲空间划分成两个较小的部分来存储这个大块数据。这两个较小的块就称为 buddy ，其中一个块提供所需的内存，另一个块则继续被分割，直到找到足够小的块。

- 当需要释放内存时，Buddy 系统会检查该块的“伙伴”是否也处于空闲状态。如果伙伴块为空闲，则两者将合并成一个更大的块。该块然后与其它同样大小的块结合在一起，形成一个更大的可用块，以便于将来使用。

这种方式可以减少内存碎片化，提高内存利用率，并允许高效地分配和释放内存。

#### slab 分配器的原理

当操作系统需要分配一块内存时，通常会使用 slab 分配器来管理这些内存块。Slab 分配器的基本原理是将一块大内存空间按照固定大小的块划分成若干个 slabs（或称对象池）。每个 slab 包含多个同样大小的内存块，当需要分配内存时，可以从一个未满的 slab 中分配一个内存块来使用。

为了更好地管理内存，slab 分配器还会维护三个链表，分别是**空闲链表**、**局部缓存链表**和**全局缓存链表**。其中，

- 空闲链表用于保存所有未被分配的内存块；
- 局部缓存链表用于保存当前正在被使用的 slab；
- 全局缓存链表用于保存未满的 slab。

>  未满的 slab 是什么?		
> 已经被分配但未被完全使用的内存块。这些内存块仍然被 Slab 分配器所持有，并且可以被快速重用，以避免频繁的内存分配和释放操作。

当需要分配内存时，slab 分配器会首先检查局部缓存链表中是否有可用的 slab，如果存在，则直接从该 slab 中分配内存块；如果不存在，则会从全局缓存链表中获取一个未满的 slab，并将其移到局部缓存链表中，再从该 slab 中分配内存块。如果全局缓存链表也没有可用的 slab，则会重新申请一块大内存空间，并将其划分成多个 slabs，然后从其中一个新的 slab 中分配内存块。

使用 slab 分配器能够有效地防止内存碎片和频繁的系统调用，提高系统性能和稳定性。

