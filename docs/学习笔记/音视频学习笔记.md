
---

## ffmpeg 编程入门

​本小结只是对 ffmpeg 编程的一些概念做一个简单的介绍，并不会对某个细节或者过程深入讲解，后期会有单独的篇章详细介绍各个部分，大家先对这些概念有个了解先，有助于后期音视频开发的学习。

### 音视频常见术语

- 容器／文件（Conainer/File）：即特定格式的多媒体文件，比如mp4、flv、mkv等。
- 媒体流（Stream）：表示时间轴上的一段连续数据，如一段声音数据、一段视频数据或一段字幕数据，可以是压缩的，也可以是非压缩的，压缩的数据需要关联特定的编解码器（有些码流音频是纯PCM）。
- 数据帧／数据包（Frame/Packet）：通常，一个媒体流是由大量的数据帧组成的，对于压缩数据，帧对应着编解码器的最小处理单元，分别属于不同媒体流的数据帧交错存储于容器之中。
- 编解码器：编解码器是以帧为单位实现压缩数据和原始数据之间的相互转换的。

### 编解码器

- **视频编解码器**

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/视频编解码器.5m9tmtgam9s0.webp)

- **音频编解码器**

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/音频编解码器.hzm3oi4i3cw.webp)

### ffmpeg 函数简介

- `av_register_all()`：注册所有组件, **4.0 已经弃用**
- `avdevice_register_all()` 对设备进行注册，比如 V4L2 等。
- `avformat_network_init()` 初始化网络库以及网络加密协议相关的库（比如 openssl）
  
#### 封装格式相关函数

|  函数原型   | 介绍 |
|  ----  | ----  |
| `avformat_alloc_context`  | 分配一个`AVFormatContext`,并进行简单初始化 (调用者不一定需非调用) |
| `avformat_free_context`  | 释放掉`AVFormatContext`和其里面的流数据 |
|  `avformat_close_input`   | 关闭解复用器，关闭后就不再需要使用`avformat_free_context `进行释放 |
|  `avformat_open_input`   | 打开输入流 (音视频文件) |
|  `avformat_find_stream_info`   | 读取媒体文件并获取流信息 |
|  `av_read_frame`   | 读取音视频包 (文件中的内容),但不进行验证这些码流哪些帧是有效帧 |
|  `avformat_seek_file`   | 定位文件，比如播放器中的拖动定位播放 |
|  `av_seek_file`   |  寻找关键帧 |

> **下面来看看解复用的流程**

- (1)分配解复用器上下文（AVFormatContext）<非必须>
- (2)打开文件(或者网络流）、获取封装信息上下文`AVFormatContext`
- (3)获取媒体文件音视频信息，这一步会将`AVFormatContext`内部变量填充
- (4)读取码流信息：循环处理
  
  * 4.1 从文件中读取数据包`av_read_frame`
  * 4.2 定位文件 `avformat_seek_file` 或 `av_seek_frame`

- (5)关闭解复用器

![](https://cdn.jsdelivr.net/gh/kendall-cpp/blogPic@main/blog-img-01/ffmpeg解封装02.60xe5klq6iw0.png)

##### AVFormatContex

> **先来认识一下 AVFormatContext**		
> 
> `AVFormatContext`主要存储视音频封装格式中包含的信息

因为在使用 ffmpeg 进行开发的时候，时刻都会使用到 AVFormatContext 这个数据结构，因为很多函数的参数都会以它为参数，所以有必要先了解一下这个结构体。

AVFormatContext 描述一个媒体文件或媒体流的构成和基本信息。

源码中的注释非常详细，可以参考源码或者点击 [雷神的博客](https://blog.csdn.net/leixiaohua1020/article/details/14214705)

AVFormatContext 中比较重要的几个字段

```c
struct AVInputFormat *iformat：输入数据的封装格式
AVIOContext *pb：输入数据的缓存
unsigned int nb_streams：  音视频流的个数
AVStream **streams：       视频流
char filename[1024]：    文件名
int64_t duration：   时长（单位：微秒us，转换为秒需要除以1000000）
int bit_rate：       比特率（单位bps，转换为kbps需要除以1000）
AVDictionary *metadata：元数据
```


#### 解码器相关函数

- `avcodec_alloc_context3()`: 分配解码器上下文
- `avcodec_find_decoder()`：根据ID查找解码器
- `avcodec_find_decoder_by_name()`:根据解码器名字查找解码器
- `avcodec_open2()`： 打开编解码器
- `avcodec_decode_video2()`：解码一帧视频数据，新版本已经不建议使用
- `avcodec_decode_audio4()`：解码一帧音频数据，新版本已经不建议使用
- `avcodec_send_packet()`: 发送编码数据包，新版本建议
- `avcodec_receive_frame()`: 接收解码后数据，新版本建议
- `avcodec_free_context()`:释放解码器上下文，包含了 `avcodec_close()`
- `avcodec_close()`:关闭解码器

> 这里说明一点，解码器有很多种，比如对于 H264 来说，不同厂家会提供不同的解码器，但是这些解码器的 ID 必须都一样，名字可以不一样。在 ffmpeg 中可以通过 avcodec_find_decoder 来根据解码器的 ID 获取相应的解码器

下面是 ffmpeg 中 h264 解码器的数据结构

```c
AVCodec ff_h264_decoder = {
    .name                  = "h264",   // 解码器的名字
    .long_name             = NULL_IF_CONFIG_SMALL("H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10"),
    .type                  = AVMEDIA_TYPE_VIDEO,
    .id                    = AV_CODEC_ID_H264,   //这是解码器的 ID
    .priv_data_size        = sizeof(H264Context),
    .init                  = h264_decode_init,
    .close                 = h264_decode_end,
    .decode                = h264_decode_frame,
    .capabilities          = /*AV_CODEC_CAP_DRAW_HORIZ_BAND |*/ AV_CODEC_CAP_DR1 |
                             AV_CODEC_CAP_DELAY | AV_CODEC_CAP_SLICE_THREADS |
                             AV_CODEC_CAP_FRAME_THREADS,
    .hw_configs            = (const AVCodecHWConfigInternal*[]) {
```

#### ffmpeg解码流程

- 1.`avcodec_alloc_context3()`: 分配编解码器上下文

函数原型：

```c
AVCodecContext *avcodec_alloc_context3(const AVCodec *codec)
```

- 2.`avcodec_parameters_to_context()`: 将码流中的编解码器信息拷贝到 AVCodecContex

函数原型：

```c
int avcodec_parameters_to_context(AVCodecContext *codec,
                                  const AVCodecParameters *par)
```

- 3.根据编解码器信息查找相应的解码器 通过 avcodec_find_decoder 或指定解码器名 avcodec_find_decoder_by_name

```c
AVCodec *avcodec_find_decoder(enum AVCodecID id)

AVCodec *avcodec_find_decoder_by_name(const char *name)
```

- 4.打开编解码器并关联到 AVCodecContex，通过 `avcodec_open2()`

```c
int attribute_align_arg avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options)
```

各个参数的含义：

avctx：需要初始化的 AVCodecContext（当前 Codec 使用的上下文）。		

codec：输入的 AVCodec (存储编解码器信息的结构体)		

options：一些选项。例如使用libx264编码的时候，“preset”，“tune”等都可以通过该参数设置。

- 5.循环发送数据包
  - 5.1 向解码器发送数据包 avcodec_send_packet
  - 5.2 接收解码后的帧 avcodec_receive_frame

- 6.关闭解码器和释放上下文 avcodec_close 和 avcodec_free_context

```c
av_cold int avcodec_close(AVCodecContext *avctx)

void avcodec_free_context(AVCodecContext **pavctx)
```


-----

## ffplay

ffplay 是 ffmpeg源码中提供的一个播放器实现例子。很多成熟播放器是基于 ffplay 修改而来的，如果想基于 ffmpeg 实现自己的播放器，或者维护一个基于 ffmpeg 的播放器，ffplay 都是一个很好的切入点。

在视频文件的播放过程中，一般要涉及到文件读取、解封装、解码、音视频输出、音视频同步等技术。

> 所以接下来看看播放器的基本流程包括哪些？    
> 音视频开发学习过程中会出现很多不熟悉的名词，建议有时间在 csdn 上看看雷神的博客。

### 播放器基本原理 

- **解协议**

在播放视频前，我们一般会拿到一个视频的播放地址，如果是本地视频，就是一个文件路径；如果是一个在线视频，那么可能有多种流媒体协议，常见的如 HTTP、RTMP、HLS、DASH 等。解协议的过程就是通过拿到的播放地址判断出当前视频的流媒体协议，然后用对应的协议去获取媒体文件数据。

FFmpeg 中内置了常见的流媒体格式协议的解析，对于一个视频比如 `url http:www.qq.com/test.mp4`, 常见的解析的过程如下：

- 首先取出 url 中的协议头如 "http"

- 然后和初始化好的协议列表中的协议名进行对比，如果匹配上则使用该协议解析器；这里会匹配上 http 协议，*在 ffplay 源码中，http 协议的 name 就是: http，实现在 http.c*

- 如果是一个本地文件的地址，会解析为 file 协议，*在 ffplay 源码中，file 协议的 name 就是:file，实现在 file.c*

解析完成后就会使用 对应协议获取媒体数据的方式 来读取媒体流。就比如说 File 协议的实现就是读取本地文件；Http 协议的实现就是通过 http 请求的方式向服务器请求数据

- **解封装**

视频有多种格式，如常见的有 MP4、3GP、AVI、FLV、RMVB。一般来说视频的格式名就对应着他的封装协议名称。**封装协议的主要作用就是将已经编码好的视频数据和音频数据按照协议规则放在一个文件中**。

一个完整的视频文件中，除了有已经编码后的音视频信息外，一般还会有描述媒体数据的组织结构的信息。如 MP4 封装格式如下所示，就是一个一个的 box 及其嵌套，不同的 box 里面存储了不同的信息，MP4 的所有信息都以 box 的方式进行组织，box 可以相互嵌套。其中最重要的就是 moov box 和 mdat box，在 moov box 中存储了描述音视频格式如视频宽高、分辨率、码率等相关的格式信息，也有如 moov box 其中嵌套的 stbl 包含了所有音视频 sample 的时间戳 pts 和在文件中的偏移位置 offset 的信息；而 mdat box 则完全是存储压缩后的音视频数据。

[这里关于 MP4 的一些概念简单了解即可，后面会有专门的推文介绍]

解封装的过程就是通过 moov box 中的媒体结构信息，从 mdta box 中分离出 Audio Track 和 Video Track，再把一份一份的视频数据或音频数据取出来的过程。

- **解码**

目前视频常用的压缩格式为 H264 和 H265，音频常用的压缩格式为 AAC。解码的过程就是**将这些按照压缩算法解码为可直接送给播放器播放的原始数据类型**。通常视频是解码 YUV 或 RGB 格式，音频是解码为 PCM 格式。

使用 FFmpeg 自带的软解解码器大致的解码流程如下：

  - 通过 `find_probe_decoder` 找到合适的解码器，详细寻找过程和寻找 demuxer 类似：

  - 解码器也会在初始化的时候初始化好放在 codec_list 中；寻找的过程就是找到解码器的AVCodecID相等的即可；AVCodecID 存放在 track box 中，是在解析视频 header 的时候初始化的，如果该视频是 HEVC 编码格式的，找到的就是 hevc 的 decoder；

[关于 AVCodecID 在[ffmpeg编程的第一步是啥](https://mp.weixin.qq.com/s?__biz=MzkwMjIzNjc4NA==&mid=2247483941&idx=1&sn=c343591096369fbafb16628b8fd02597&chksm=c0a9dd3af7de542c374041d03bf36d8bc4197c150e94ec6914475f301ee259f5e699755ff386#rd)中已经介绍过，不了解的同学可以点击查阅]

  - 找到匹配的 decoder 为 hevc，hevc格式的 name 就是: `hevc`,，实现在 `hevcdec.c`；

  - 找到对应的 decoder 后，先通过 `hevc_decode_init` 初始化；

  - 解码的时候会单独在一个解码线程，通过读取解封装数据缓存区的数据来进行解码，然后将解码后的数据放入缓存池中。

- **音视频同步**

在视频数据解码完成后，不会立即渲染到 View 上，还需要通过音视频同步机制，等到合适的渲染时机。

音视频同步主要分为三种：

- 音频时钟为基准：以当前正在播放的音频时钟基准，比较视频和音频的 pts 差值，如果视频过慢，则通过丢帧的方式进行追赶；如果视频播放过快，则一直渲染当前帧，直到音频跟上；

- 视频时钟为基准：以当前正在播放的视频时钟为基准，比较视频和音频的pts差值，这里和音频时钟为基准不同的是，这里音频是通过重采样的方式适当缩减或添加 audio sample 来达到同步的目的。

- 以外部时钟为基准：音频和视频在输出时，都需要和外部时钟进行对比，然后音视频按照各自同步的方法进行同步(视频丢帧或等待、音频重采样)，外部时钟的更新依赖于最近同步过的音频时钟或视频时钟。

在这个流程中，主要有几个线程：

- 读线程。读取文件、解封装
- 音频解码线程。解码音频压缩数据为 PCM 数据。
- 视频解码线程。解码视频压缩数据为图像数据。
- 音频输出线程。基于 SDL 播放，该线程实际上是 SDL 的内部线程。
- 视频输出线程。基于 SDL 播放，该线程为程序主线程。

由于存在多个线程，所以线程间的数据传递用到了多线程安全的队列，有 `FrameQueue` 和 `PacketQueue`。

音频和视频各自独立输出的过程不可避免地会出现音视频的不同步现象，所以在输出前会有一些控制策略保证音视频的同步输出。

> **所以后面将用两篇文章从源码层面对 `PacketQueue` 和 `FrameQueue` 进行深入分析。**


> https://zhuanlan.zhihu.com/p/44694286